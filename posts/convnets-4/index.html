<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Обучение нейронных сетей 2 | Заметки по ML, DL</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="ru" href="../../rss.xml">
<link rel="canonical" href="https://mldl.ru/posts/convnets-4/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Андрей Лабинцев">
<link rel="prev" href="../convnets-3/" title="Обучение нейронных сетей" type="text/html">
<link rel="next" href="../architecture/" title="Арзитектура нейросетей " type="text/html">
<meta property="og:site_name" content="Заметки по ML, DL">
<meta property="og:title" content="Обучение нейронных сетей 2">
<meta property="og:url" content="https://mldl.ru/posts/convnets-4/">
<meta property="og:description" content="Обучение нейронных сетей
Содержание:
- Генерация некоторых данных
- Обучение линейного классификатора Softmax
    - Инициализируйте параметры
    - Подсчитайте баллы за класс
    - Вычислите потери
  ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-03-11T19:42:16+03:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">Заметки по ML, DL</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Источник</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Обучение нейронных сетей 2</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Андрей Лабинцев
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2025-03-11T19:42:16+03:00" itemprop="datePublished" title="2025-03-11 19:42">2025-03-11 19:42</time></a>
            </p>
            
        <p class="sourceline"><a href="index.md" class="sourcelink">Источник</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2>Обучение нейронных сетей</h2>
<p>Содержание:
- <a href=".">Генерация некоторых данных</a>
- <a href=".">Обучение линейного классификатора Softmax</a>
    - <a href=".">Инициализируйте параметры</a>
    - <a href=".">Подсчитайте баллы за класс</a>
    - <a href=".">Вычислите потери</a>
    - <a href=".">Вычисление аналитического градиента с обратным распространением</a>
    - <a href=".">Выполнение обновления параметров</a>
    - <a href=".">Сведение всего этого воедино: обучение классификатора Softmax</a>
- <a href=".">Обучение нейронной сети</a>
- <a href=".">Краткая сводка</a>
- <a href=".">Дополнительные материалы</a></p>
<p>В этом разделе мы рассмотрим полную реализацию игрушечной нейронной сети в двух измерениях. Сначала мы реализуем простой линейный классификатор, а затем расширим код до двухслойной нейронной сети. Как мы увидим, это расширение на удивление простое, и требуется внести совсем немного изменений.  </p>
<h2>Генерация некоторых данных</h2>
<p>Давайте создадим набор данных для классификации, который нелегко разделить на линейные классы. Наш любимый пример — набор данных «спираль», который можно создать следующим образом:  </p>
<div class="code"><pre class="code literal-block"><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="k">class</span>
<span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">dimensionality</span>
<span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">classes</span>
<span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="o">*</span><span class="n">K</span><span class="p">,</span><span class="n">D</span><span class="p">))</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="p">(</span><span class="k">each</span><span class="w"> </span><span class="k">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="n">example</span><span class="p">)</span>
<span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="s1">'uint8'</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="n">labels</span>
<span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="err">:</span>
<span class="w">  </span><span class="n">ix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="n">j</span><span class="p">,</span><span class="n">N</span><span class="o">*</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="w">  </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">radius</span>
<span class="w">  </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">j</span><span class="o">*</span><span class="mi">4</span><span class="p">,(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="mf">0.2</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">theta</span>
<span class="w">  </span><span class="n">X</span><span class="o">[</span><span class="n">ix</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="o">[</span><span class="n">r*np.sin(t), r*np.cos(t)</span><span class="o">]</span>
<span class="w">  </span><span class="n">y</span><span class="o">[</span><span class="n">ix</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">j</span>
<span class="err">#</span><span class="w"> </span><span class="n">lets</span><span class="w"> </span><span class="n">visualize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">data</span><span class="err">:</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">[</span><span class="n">:, 0</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="o">[</span><span class="n">:, 1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="w"> </span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">Spectral</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

<hr>
<p><img alt="" src="https://cs231n.github.io/assets/eg/spiral_raw.png"></p>
<p>Данные игрушечной спирали состоят из трёх классов (синий, красный, жёлтый), которые нельзя разделить линейно.  </p>
<hr>
<p>Обычно мы хотим предварительно обработать набор данных, чтобы среднее значение каждого признака было равно нулю, а стандартное отклонение — единице, но в данном случае признаки уже находятся в диапазоне от <strong>-1 до 1</strong>, поэтому мы пропускаем этот шаг.  </p>
<h2>Обучение линейного классификатора Softmax</h2>
<h3>Инициализируйте параметры</h3>
<p>Давайте сначала обучим классификатор <strong>Softmax</strong> на этом наборе данных для классификации. Как мы видели в предыдущих разделах, классификатор <strong>Softmax</strong> имеет линейную функцию оценки и использует функцию потерь кросс-энтропии. Параметры линейного классификатора состоят из весовой матрицы <code>W</code> и вектора смещения <code>b</code> для каждого класса. Давайте сначала инициализируем эти параметры случайными числами:  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> initialize parameters randomly
W = 0.01 * np.random.randn(D,K)
b = np.zeros((1,K))
</pre></div>

<p>Напомним, что <code>D = 2</code> — это размерность, а <code>K = 3</code> — количество классов.  </p>
<h3>Подсчитайте баллы за класс</h3>
<p>Поскольку это линейный классификатор, мы можем очень просто вычислить оценки для всех классов параллельно с помощью одного умножения матриц:  </p>
<div class="code"><pre class="code literal-block">#<span class="w"> </span><span class="nv">compute</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">scores</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">linear</span><span class="w"> </span><span class="nv">classifier</span>
<span class="nv">scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">dot</span><span class="ss">(</span><span class="nv">X</span>,<span class="w"> </span><span class="nv">W</span><span class="ss">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">b</span>
</pre></div>

<p>В этом примере у нас есть <strong>300 двумерных точек</strong>, поэтому после этого умножения массив <code>scores</code> будет иметь размер <strong>[300 x 3]</strong>, где каждая строка содержит баллы за классы, соответствующие трём классам (синий, красный, жёлтый).</p>
<h3>Вычислите потери</h3>
<p>Второй ключевой компонент, который нам нужен, — это функция потерь, представляющая собой дифференцируемую целевую функцию, которая количественно оценивает наше недовольство вычисленными оценками классов. Интуитивно понятно, что мы хотим, чтобы правильный класс имел более высокую оценку, чем другие классы. В этом случае потери должны быть низкими, а в противном случае — высокими. Существует множество способов количественно оценить эту интуитивную догадку, но в этом примере мы будем использовать потери перекрёстной энтропии, которые связаны с классификатором Softmax. Напомним, что если <strong>f</strong> — это массив оценок классов для одного примера (например, массив из трёх чисел), тогда классификатор <strong>Softmax</strong> вычисляет потерю для этого примера следующим образом:  </p>
<p>$$
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)
$$  </p>
<p>Мы можем видеть, что классификатор <strong>Softmax</strong> интерпретирует каждый элемент <strong>f</strong>. В качестве входных данных используются (ненормализованные) логарифмические вероятности трёх классов. Мы возводим их в степень, чтобы получить (<em>ненормализованные</em>) вероятности, а затем нормализуем их, чтобы получить вероятности. Таким образом, выражение внутри логарифма — это нормализованная вероятность правильного класса. Обратите внимание на то, как работает это выражение: эта величина всегда находится в диапазоне от <strong>0 до 1</strong>. Когда вероятность правильного класса очень мала (<strong>близка к 0</strong>), потери будут стремиться к (положительной) бесконечности. И наоборот, когда вероятность правильного класса приближается к <strong>1</strong>, потери приближаются к нулю, потому что <strong>log(1)=0</strong>. Следовательно, выражение для <strong>\(L_i\)</strong>. Вероятность правильного класса низкая, когда она высока, и очень высокая, когда она низка.   </p>
<p>Напомним также, что полная потеря классификатора <strong>Softmax</strong> определяется как средняя потеря кросс-энтропии по обучающим примерам и регуляризация:  </p>
<p>$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }<em k_l="k,l">\text{потеря данных} + \underbrace{ \frac{1}{2} \lambda \sum_k\sum_l W</em> \\
$$  }^2 }_\text{потеря регуляризации</p>
<p>Учитывая массив <code>scores</code> значений, которые мы вычислили выше, мы можем вычислить потери. Во-первых, способ получения вероятностей прост:  </p>
<div class="code"><pre class="code literal-block">num_examples = X.shape[0]
<span class="gh">#</span> get unnormalized probabilities
exp_scores = np.exp(scores)
<span class="gh">#</span> normalize them for each example
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
</pre></div>

<p>Теперь у нас есть массив <code>probs</code> размером [300 x 3], где каждая строка содержит вероятности классов. В частности, поскольку мы их нормализовали, сумма значений в каждой строке равна единице. Теперь мы можем запросить логарифмические вероятности, присвоенные правильным классам в каждом примере:  </p>
<div class="code"><pre class="code literal-block">orrect_logprobs = -np.log(probs[range(num_examples),y])
</pre></div>

<p>Массив <code>correct_logprobs</code> — это одномерный массив, содержащий только вероятности, присвоенные правильным классам для каждого примера. <strong>Полная потеря</strong> — это среднее значение этих логарифмических вероятностей и потери от регуляризации:  </p>
<div class="code"><pre class="code literal-block"><span class="p">#</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nl">loss:</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="n">cross</span><span class="o">-</span><span class="n">entropy</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">regularization</span>
<span class="n">data_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_logprobs</span><span class="p">)</span><span class="o">/</span><span class="n">num_examples</span>
<span class="n">reg_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="o">*</span><span class="kt">reg</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
<span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">reg_loss</span>
</pre></div>

<p>В этом коде сила регуляризации <strong>λ</strong> хранится внутри <code>reg</code>. Коэффициент удобства <code>0.5</code> умножения регуляризации станет ясен через секунду. Оценка этого вначале (со случайными параметрами) может дать нам <code>loss = 1.1</code>, что и есть <code>-np.log(1.0/3)</code>, поскольку при небольших начальных случайных весах все вероятности, присвоенные всем классам, составляют около одной трети. Теперь мы хотим сделать потери как можно более низкими, используя <code>loss = 0</code> в качестве абсолютной нижней границы. Но чем меньше потери, тем выше вероятности, присвоенные правильным классам для всех примеров.  </p>
<h3>Вычисление аналитического градиента с обратным распространением</h3>
<p>У нас есть способ оценки потерь, и теперь нам нужно их минимизировать. Мы сделаем это с помощью градиентного спуска. То есть мы начнём со случайных параметров (как показано выше) и вычислим градиент функции потерь по отношению к параметрам, чтобы знать, как изменить параметры для уменьшения потерь. Давайте введём промежуточную переменную <strong>p</strong>, который представляет собой вектор (<em>нормализованных</em>) вероятностей. Потери для одного примера составляют:  </p>
<p>$$
p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)
$$  </p>
<p>Теперь мы хотим понять, как вычисляются баллы внутри <strong>f</strong> следует изменить, чтобы уменьшить потери <strong>\(L_i\)</strong>, что этот пример соответствует общей цели. Другими словами, мы хотим вычислить градиент <strong>\( \partial L_i / \partial f_k \)</strong>. Потеря __\(L_i\)__вычисляется из <strong>p</strong>, что , в свою очередь , зависит от <strong>f</strong>. Читателю будет интересно использовать правило дифференцирования сложной функции для вычисления градиента, но в итоге всё оказывается очень простым и понятным, после того как многое сокращается:  </p>
<p>$$
\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)
$$  </p>
<p>Обратите внимание, насколько элегантно и просто выглядит это выражение. Предположим, что вычисленные нами вероятности были <code>p = [0.2, 0.3, 0.5]</code> и что правильным классом был средний (<strong>с вероятностью 0,3</strong>). Согласно этому выводу, градиент оценок будет равен <code>df = [0.2, -0.7, 0.5]</code>. Вспомнив, что означает интерпретация градиента, мы видим, что этот результат вполне интуитивен: увеличение первого или последнего элемента вектора оценок <strong>f</strong> (оценок неверных классов) приводит к <em>увеличению</em> потерь (из-за положительных значений <strong>+0,2 и +0,5</strong>) — а увеличение потерь плохо, как и ожидалось. Однако увеличение оценки правильного класса отрицательно влияет на потери. Градиент <strong>-0,7</strong> говорит нам о том, что увеличение оценки правильного класса приведёт к уменьшению потерь <strong>\(L_i\)</strong>, что имеет смысл.  </p>
<p>Всё это сводится к следующему коду. Напомним, что <code>probs</code> хранит вероятности всех классов (в виде строк) для каждого примера. Чтобы получить градиент оценок, который мы называем <code>dscores</code>, мы поступаем следующим образом:  </p>
<div class="code"><pre class="code literal-block">dscores = probs
dscores[range(num_examples),y] -= 1
dscores /= num_examples
</pre></div>

<p>Наконец, у нас есть <code>scores = np.dot(X, W) + b</code> и, вооружившись градиентом <code>scores</code> (хранящимся в <em>dscores</em>), мы можем выполнить обратное распространение ошибки в <code>W</code> и <code>b</code>:  </p>
<div class="code"><pre class="code literal-block"><span class="n">dW</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">dscores</span><span class="p">)</span>
<span class="n">db</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mh">0</span><span class="p">,</span><span class="w"> </span><span class="n">keepdims</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="n">dW</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="kt">reg</span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="p">#</span><span class="w"> </span><span class="n">don</span><span class="p">'</span><span class="n">t</span><span class="w"> </span><span class="n">forget</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">gradient</span>
</pre></div>

<p>Здесь мы видим, что мы выполнили обратное преобразование с помощью операции умножения матриц, а также добавили вклад от регуляризации. Обратите внимание, что градиент регуляризации имеет очень простую форму <code>reg*W</code>, поскольку мы использовали константу <code>0.5</code> для вклада в потери (т. е. <strong>\(\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda w\)</strong> ). Это распространенный удобный прием, который упрощает выражение градиента.  </p>
<h3>Выполнение обновления параметров</h3>
<p>Теперь, когда мы вычислили градиент, мы знаем, как каждый параметр влияет на функцию потерь. Теперь мы обновим параметры в направлении <em>отрицательного</em> градиента, чтобы <em>уменьшить</em> потери:  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> perform a parameter update
W += -step_size <span class="gs">* dW</span>
<span class="gs">b += -step_size *</span> db
</pre></div>

<h3>Сведение всего этого воедино: обучение классификатора Softmax</h3>
<p>Если собрать всё это воедино, получится полный код для обучения классификатора <strong>Softmax</strong> с помощью градиентного спуска:  </p>
<div class="code"><pre class="code literal-block"><span class="p">#</span><span class="n">Train</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Linear</span><span class="w"> </span><span class="n">Classifier</span>

<span class="p">#</span><span class="w"> </span><span class="n">initialize</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">randomly</span>
<span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.01</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">K</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mh">1</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>

<span class="p">#</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">hyperparameters</span>
<span class="n">step_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1e-0</span>
<span class="kt">reg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1e-3</span><span class="w"> </span><span class="p">#</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="k">strength</span>

<span class="p">#</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">descent</span><span class="w"> </span><span class="n">loop</span>
<span class="n">num_examples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mh">0</span><span class="p">]</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">range</span><span class="p">(</span><span class="mh">200</span><span class="p">)</span><span class="o">:</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">scores</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="n">N</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">K</span><span class="p">]</span>
<span class="w">  </span><span class="n">scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">probabilities</span>
<span class="w">  </span><span class="n">exp_scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="w">  </span><span class="n">probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exp_scores</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mh">1</span><span class="p">,</span><span class="w"> </span><span class="n">keepdims</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="w"> </span><span class="p">#</span><span class="w"> </span><span class="p">[</span><span class="n">N</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">K</span><span class="p">]</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nl">loss:</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="n">cross</span><span class="o">-</span><span class="n">entropy</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">regularization</span>
<span class="w">  </span><span class="n">correct_logprobs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">),</span><span class="n">y</span><span class="p">])</span>
<span class="w">  </span><span class="n">data_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_logprobs</span><span class="p">)</span><span class="o">/</span><span class="n">num_examples</span>
<span class="w">  </span><span class="n">reg_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="o">*</span><span class="kt">reg</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
<span class="w">  </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">reg_loss</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mh">10</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mh">0</span><span class="o">:</span>
<span class="w">    </span><span class="n">print</span><span class="w"> </span><span class="s">"iteration %d: loss %f"</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">loss</span><span class="p">)</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">scores</span>
<span class="w">  </span><span class="n">dscores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">probs</span>
<span class="w">  </span><span class="n">dscores</span><span class="p">[</span><span class="n">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">),</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mh">1</span>
<span class="w">  </span><span class="n">dscores</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">num_examples</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">backpropate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="w">  </span><span class="n">dW</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">dscores</span><span class="p">)</span>
<span class="w">  </span><span class="n">db</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mh">0</span><span class="p">,</span><span class="w"> </span><span class="n">keepdims</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="w">  </span><span class="n">dW</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="kt">reg</span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="p">#</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">gradient</span>

<span class="w">  </span><span class="p">#</span><span class="w"> </span><span class="n">perform</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">parameter</span><span class="w"> </span><span class="n">update</span>
<span class="w">  </span><span class="n">W</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="o">-</span><span class="n">step_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dW</span>
<span class="w">  </span><span class="n">b</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="o">-</span><span class="n">step_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">db</span>
<span class="w">  </span><span class="err">```</span>


<span class="w"> </span><span class="err">При</span><span class="w"> </span><span class="err">выполнении</span><span class="w"> </span><span class="err">этой</span><span class="w"> </span><span class="err">операции</span><span class="w"> </span><span class="err">выводятся</span><span class="w"> </span><span class="err">выходные</span><span class="w"> </span><span class="err">данные</span><span class="o">:</span><span class="w">  </span>

<span class="w"> </span><span class="err">```</span>
<span class="w"> </span><span class="n">iteration</span><span class="w"> </span><span class="mh">0</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">1.096956</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">10</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.917265</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">20</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.851503</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">30</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.822336</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">40</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.807586</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">50</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.799448</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">60</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.794681</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">70</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.791764</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">80</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.789920</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">90</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.788726</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">100</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.787938</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">110</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.787409</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">120</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.787049</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">130</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786803</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">140</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786633</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">150</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786514</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">160</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786431</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">170</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786373</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">180</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786331</span>
<span class="n">iteration</span><span class="w"> </span><span class="mh">190</span><span class="o">:</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="mf">0.786302</span>
</pre></div>

<p>Мы видим, что после примерно <strong>190</strong> итераций мы приблизились к чему-то. Мы можем оценить точность обучающего набора данных:  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> evaluate training set accuracy
scores = np.dot(X, W) + b
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))
</pre></div>

<p>Это выводит <strong>49%</strong>. Не очень хорошо, но и неудивительно, учитывая, что набор данных составлен таким образом, что он не является линейно разделимым. Мы также можем построить границы принятых решений:  </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/eg/spiral_linear.png"><br>
Линейный классификатор не может изучить набор данных <em>toy spiral</em>.  </p>
<hr>
<h2>Обучение нейронной сети</h2>
<p>Очевидно, что линейный классификатор не подходит для этого набора данных, и мы хотели бы использовать нейронную сеть. Для этих игрушечных данных будет достаточно одного дополнительного скрытого слоя. Теперь нам понадобятся два набора весовых коэффициентов и смещений (<em>для первого и второго слоев</em>):  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> initialize parameters randomly
h = 100 # size of hidden layer
W = 0.01 <span class="gs">* np.random.randn(D,h)</span>
<span class="gs">b = np.zeros((1,h))</span>
<span class="gs">W2 = 0.01 *</span> np.random.randn(h,K)
b2 = np.zeros((1,K))
</pre></div>

<p>Прямой проход для подсчета очков теперь меняет форму:  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> evaluate class scores with a 2-layer Neural Network
hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
scores = np.dot(hidden_layer, W2) + b2
</pre></div>

<p>Обратите внимание, что единственное отличие от предыдущего варианта — это одна дополнительная строка кода, в которой мы сначала вычисляем представление скрытого слоя, а затем баллы на основе этого скрытого слоя. Важно отметить, что мы также добавили нелинейность, которая в данном случае представляет собой простую функцию <strong>ReLU</strong>, устанавливающую пороговое значение активации скрытого слоя на нуле.  </p>
<p>Всё остальное остаётся прежним. Мы вычисляем потери на основе оценок точно так же, как и раньше, и получаем градиент для оценок <code>dscores</code> точно так же, как и раньше. Однако способ обратного распространения этого градиента на параметры модели, конечно, меняется. Сначала давайте выполним обратное распространение для второго слоя нейронной сети. Это выглядит так же, как и код для классификатора <strong>Softmax</strong>, за исключением того, что мы заменяем <code>X</code> (исходные данные) на переменную <code>hidden_layer</code>):  </p>
<p>```# backpropate the gradient to the parameters</p>
<h2>first backprop into parameters W2 and b2</h2>
<p>dW2 = np.dot(hidden_layer.T, dscores)
db2 = np.sum(dscores, axis=0, keepdims=True)</p>
<div class="code"><pre class="code literal-block"><span class="n">Однако</span><span class="p">,</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">отличие</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">предыдущего</span><span class="w"> </span><span class="n">случая</span><span class="p">,</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">ещё</span><span class="w"> </span><span class="n">не</span><span class="w"> </span><span class="n">закончили</span><span class="p">,</span><span class="w"> </span><span class="n">потому</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n n-Quoted">`hidden_layer`</span><span class="w"> </span><span class="n">сама</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">функцией</span><span class="w"> </span><span class="n">других</span><span class="w"> </span><span class="n">параметров</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">данных</span><span class="o">!</span><span class="w"> </span><span class="n">Нам</span><span class="w"> </span><span class="n">нужно</span><span class="w"> </span><span class="n">продолжить</span><span class="w"> </span><span class="n">обратное</span><span class="w"> </span><span class="n">распространение</span><span class="w"> </span><span class="n">ошибки</span><span class="w"> </span><span class="n">через</span><span class="w"> </span><span class="n">эту</span><span class="w"> </span><span class="n">переменную</span><span class="p">.</span><span class="w"> </span><span class="n">Её</span><span class="w"> </span><span class="n">градиент</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">вычислить</span><span class="w"> </span><span class="n">следующим</span><span class="w"> </span><span class="n">образом</span><span class="o">:</span><span class="w">  </span>
</pre></div>

<p>dhidden = np.dot(dscores, W2.T)</p>
<div class="code"><pre class="code literal-block">Теперь у нас есть градиент на выходе скрытого слоя. Далее нам нужно выполнить обратное распространение ошибки для нелинейности **ReLU**. Это оказывается простым, потому что **ReLU** при обратном распространении ошибки фактически является переключателем. Поскольку **r=max(0,x)**, у нас есть это **dr/dx=1(x&gt;0)**. В сочетании с правилом дифференцирования по частям мы видим, что блок **ReLU** пропускает градиент без изменений, если его входные данные больше 0, но <span class="ge">_отменяет_</span> его, если входные данные меньше нуля во время прямого прохода. Следовательно, мы можем выполнить обратное распространение ошибки для **ReLU** следующим образом:  
</pre></div>

<h2>backprop the ReLU non-linearity</h2>
<p>dhidden[hidden_layer &lt;= 0] = 0</p>
<div class="code"><pre class="code literal-block">И теперь мы, наконец, переходим к первому слою весов и смещений:  
</pre></div>

<h2>finally into W,b</h2>
<p>dW = np.dot(X.T, dhidden)
db = np.sum(dhidden, axis=0, keepdims=True)</p>
<div class="code"><pre class="code literal-block">**Готово!** У нас есть градиенты <span class="sb">`dW,db,dW2,db2`</span> и мы можем выполнить обновление параметров. Всё остальное остаётся без изменений. Полный код выглядит очень похоже:  
</pre></div>

<h2>initialize parameters randomly</h2>
<p>h = 100 # size of hidden layer
W = 0.01 * np.random.randn(D,h)
b = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,K)
b2 = np.zeros((1,K))</p>
<h2>some hyperparameters</h2>
<p>step_size = 1e-0
reg = 1e-3 # regularization strength</p>
<h2>gradient descent loop</h2>
<p>num_examples = X.shape[0]
for i in range(10000):</p>
<p># evaluate class scores, [N x K]
  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
  scores = np.dot(hidden_layer, W2) + b2</p>
<p># compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]</p>
<p># compute the loss: average cross-entropy loss and regularization
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5<em>reg</em>np.sum(W<em>W) + 0.5</em>reg<em>np.sum(W2</em>W2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)</p>
<p># compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples</p>
<p># backpropate the gradient to the parameters
  # first backprop into parameters W2 and b2
  dW2 = np.dot(hidden_layer.T, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=True)
  # next backprop into hidden layer
  dhidden = np.dot(dscores, W2.T)
  # backprop the ReLU non-linearity
  dhidden[hidden_layer &lt;= 0] = 0
  # finally into W,b
  dW = np.dot(X.T, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=True)</p>
<p># add regularization gradient contribution
  dW2 += reg * W2
  dW += reg * W</p>
<p># perform a parameter update
  W += -step_size * dW
  b += -step_size * db
  W2 += -step_size * dW2
  b2 += -step_size * db2
  ```</p>
<p>Это печатает:  </p>
<div class="code"><pre class="code literal-block">iteration 0: loss 1.098744
iteration 1000: loss 0.294946
iteration 2000: loss 0.259301
iteration 3000: loss 0.248310
iteration 4000: loss 0.246170
iteration 5000: loss 0.245649
iteration 6000: loss 0.245491
iteration 7000: loss 0.245400
iteration 8000: loss 0.245335
iteration 9000: loss 0.245292
</pre></div>

<p>Точность обучения теперь равна:  </p>
<div class="code"><pre class="code literal-block"><span class="gh">#</span> evaluate training set accuracy
hidden_layer = np.maximum(0, np.dot(X, W) + b)
scores = np.dot(hidden_layer, W2) + b2
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))
</pre></div>

<p>Что выводит <strong>98%</strong>!. Мы также можем визуализировать границы решений:  </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/eg/spiral_net.png"><br>
Классификатор нейронной сети сжимает набор данных <em>spiral</em>.  </p>
<hr>
<h2>Краткие сведения</h2>
<p>Мы работали с игрушечным 2D-набором данных и обучали как линейную сеть, так и двухслойную нейронную сеть. Мы увидели, что переход от линейного классификатора к нейронной сети требует очень мало изменений в коде. Функция оценки меняет свою форму (разница в 1 строке кода), а обратное распространение ошибки меняет свою форму (нам нужно выполнить ещё один цикл обратного распространения ошибки через скрытый слой к первому слою сети).  </p>
<h2>Дополнительные материалы</h2>
<ul>
<li>Возможно, вам захочется взглянуть на этот код <strong>IPython Notebook</strong> <a href="http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html">отображаемый в формате HTML</a>.</li>
<li>Или загрузите <a href="http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.ipynb">файл ipynb</a>
</li>
</ul>
</div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../convnets-3/" rel="prev" title="Обучение нейронных сетей">Предыдущая запись</a>
            </li>
            <li class="next">
                <a href="../architecture/" rel="next" title="Арзитектура нейросетей ">Следующая запись</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:andrej.labintsev@yandex.ru">Андрей Лабинцев</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
