<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="ru">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Линейный классификатор  | Заметки по ML, DL</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="ru" href="../../rss.xml">
<link rel="canonical" href="https://mldl.ru/posts/linear-classifier/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Андрей Лабинцев">
<link rel="prev" href="../transfer-learning/" title="Трансфер обучения" type="text/html">
<link rel="next" href="../optimization/" title="Оптимизация" type="text/html">
<meta property="og:site_name" content="Заметки по ML, DL">
<meta property="og:title" content="Линейный классификатор ">
<meta property="og:url" content="https://mldl.ru/posts/linear-classifier/">
<meta property="og:description" content="Линейный классификатор
Содержание: 
- Линейная классификация
- Параметризованное сопоставление изображений с оценками меток
- Интерпретация линейного классификатора
- Функция потерь
- Потеря машины му">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-03-05T19:42:16+03:00">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">Заметки по ML, DL</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Источник</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Линейный классификатор </a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Андрей Лабинцев
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2025-03-05T19:42:16+03:00" itemprop="datePublished" title="2025-03-05 19:42">2025-03-05 19:42</time></a>
            </p>
            
        <p class="sourceline"><a href="index.md" class="sourcelink">Источник</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2>Линейный классификатор</h2>
<p>Содержание: 
- <a href="#%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">Линейная классификация</a>
- <a href="#%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D0%B5-%D1%81%D0%BE%D0%BF%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9-%D1%81-%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0%D0%BC%D0%B8-%D0%BC%D0%B5%D1%82%D0%BE%D0%BA">Параметризованное сопоставление изображений с оценками меток</a>
- <a href="#%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BF%D1%80%D0%B5%D1%82%D0%B0%D1%86%D0%B8%D1%8F-%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B3%D0%BE-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80%D0%B0">Интерпретация линейного классификатора</a>
- <a href="#%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C">Функция потерь</a>
- <a href="#%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8F-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D1%8B-%D0%BC%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%BE%D0%B2%D0%BE%D0%B3%D0%BE-%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D0%BE%D0%B3%D0%BE-%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B0">Потеря машины мультиклассового опорного вектора</a>
- <a href="#%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D1%81%D0%BE%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F">Практические соображения</a>
- <a href="#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-softmax">Классификатор Softmax</a>
- <a href="#svm-%D0%BF%D1%80%D0%BE%D1%82%D0%B8%D0%B2-softmax">SVM против Softmax</a>
- <a href="#%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F-%D0%B2%D0%B5%D0%B1-%D0%B4%D0%B5%D0%BC%D0%BE%D0%BD%D1%81%D1%82%D1%80%D0%B0%D1%86%D0%B8%D1%8F">Интерактивная веб-демонстрация</a>
- <a href="#%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%B0%D1%8F-%D1%81%D0%B2%D0%BE%D0%B4%D0%BA%D0%B0">Краткая сводка</a>
- <a href="#%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%82%D0%B5%D1%80%D0%B8%D0%B0%D0%BB%D1%8B">Дополнительные материалы</a></p>
<h3>Линейная классификация</h3>
<p>В предыдущем разделе мы рассмотрели задачу классификации изображений, которая заключается в присвоении изображению одного из фиксированного набора категорий. Кроме того, мы описали классификатор k-ближайших соседей (kNN), который присваивает изображениям метки, сравнивая их с (помеченными) изображениями из обучающей выборки. Как мы увидели, у kNN есть ряд недостатков:
- Классификатор должен запоминать все обучающие данные и сохранять их для последующего сравнения с тестовыми данными. Это неэффективно с точки зрения использования памяти, поскольку размер наборов данных может легко достигать гигабайтов.
- Классификация тестового изображения обходится дорого, поскольку требует сравнения со всеми обучающими изображениями.  </p>
<p><strong>Обзор</strong>. Теперь мы собираемся разработать более эффективный подход к классификации изображений, который в конечном итоге естественным образом распространится на нейронные сети и свёрточные нейронные сети. Этот подход будет состоять из двух основных компонентов: <strong>функции оценки</strong>, которая преобразует исходные данные в оценки классов, и <strong>функции потерь</strong>, которая количественно оценивает соответствие между прогнозируемыми оценками и истинными метками. Затем мы сформулируем это, как задачу оптимизации, в которой мы минимизируем функцию потерь по отношению к параметрам функции оценки.   </p>
<h3>Параметризованное сопоставление изображений с оценками меток</h3>
<p>Первым компонентом этого подхода является определение функции оценки, которая сопоставляет значения пикселей изображения со значениями уверенности для каждого класса. Мы рассмотрим этот подход на конкретном примере. Как и прежде, предположим, что у нас есть набор обучающих изображений $x_i \ in R^D $ каждый из которых связан с меткой  $ y_i $. Здесь $i=1 ... N $ и $y_i \in { 1 ... K } $. </p>
<p>То есть у нас есть <strong>N</strong> примеров (каждый из которых имеет размерность <strong>D</strong>) и <strong>K</strong> различных категорий. Например, в CIFAR-10 у нас есть обучающий набор из <strong>N</strong> = 50 000 изображений, каждое из которых имеет <strong>D</strong> = 32 x 32 x 3 = 3072 пикселя, и <strong>K</strong> = 10, так как существует 10 различных классов (собака, кошка, автомобиль и т. д.). Теперь мы определим функцию оценки $f: R^D \mapsto R^K$, которое сопоставляет пиксели необработанного изображения с оценками класса.   </p>
<p><strong>Линейный классификатор</strong>. В этом модуле мы начнём с, пожалуй, самой простой из возможных функций — линейного отображения:  </p>
<p>$$
f(x_i, W, b) = W x_i + b
$$   </p>
<p>В приведенном выше уравнении мы предполагаем, что изображение $x_i$ все его пиксели сглаживаются до одного вектора-столбца размером [D x 1] . Матрица <strong>W</strong> (размером [K x D]) и вектор <strong>b</strong> (размером [K x 1]) являются <strong>параметрами</strong> функции. В CIFAR-10 $x_i$ содержит все пиксели в i-м изображении, объединённые в один столбец [3072 x 1], <strong>W</strong> имеет размер [10 x 3072], а <strong>b</strong> имеет размер [10 x 1], то есть в функцию поступает 3072 числа (исходные значения пикселей), а выходит 10 чисел (оценки классов). Параметры в <strong>W</strong> часто называют весами, а <strong>b</strong> называют <strong>вектором смещения</strong>, потому что он влияет на выходные оценки, но не взаимодействует с фактическими данными $x_i$. Однако вы часто будете слышать, как люди используют термины веса и параметры как взаимозаменяемые. 
Есть несколько вещей, на которые следует обратить внимание:
- Во-первых, обратите внимание, что умножение одной матрицы $W x_1$. Фактически выполняется параллельная оценка 10 отдельных классификаторов (по одному для каждого класса), где каждый классификатор представляет собой строку <strong>W</strong>.
- Обратите также внимание, что мы думаем о входных данных $ (x_i, y_i) $
Мы считаем, что они заданы и неизменны, но мы можем управлять параметрами <strong>W</strong>, <strong>b</strong>. Наша цель — настроить их таким образом, чтобы вычисленные оценки соответствовали истинным меткам во всём обучающем наборе данных. Мы подробно рассмотрим, как это сделать, но интуитивно понятно, что мы хотим, чтобы оценка правильного класса была выше, чем оценка неправильных классов.
- Преимущество этого подхода заключается в том, что обучающие данные используются для определения параметров <strong>W</strong>, <strong>b</strong>, но после завершения обучения мы можем отбросить весь обучающий набор данных и оставить только полученные параметры. Это связано с тем, что новое тестовое изображение можно просто передать в функцию и классифицировать на основе вычисленных показателей.
- Наконец, обратите внимание, что классификация тестового изображения включает в себя одно матричное умножение и сложение, что значительно быстрее, чем сравнение тестового изображения со всеми обучающими изображениями. </p>
<blockquote>
<p>Предвосхищая вопрос: свёрточные нейронные сети будут сопоставлять пиксели изображения со значениями точно так же, как показано выше, но сопоставление ( f ) будет более сложным и будет содержать больше параметров.</p>
</blockquote>
<h3>Интерпретация линейного классификатора</h3>
<p>Обратите внимание, что линейный классификатор вычисляет оценку класса как взвешенную сумму всех значений пикселей по всем трём цветовым каналам. В зависимости от того, какие именно значения мы задаём для этих весов, функция может любить или не любить (в зависимости от знака каждого веса) определённые цвета в определённых местах изображения. Например, можно представить, что класс «корабль» может быть более вероятным, если по краям изображения много синего (что, скорее всего, соответствует воде). Можно было бы ожидать, что классификатор «корабль» будет иметь множество положительных весовых коэффициентов для синего канала (присутствие синего повышает оценку корабля) и отрицательные весовые коэффициенты для красного/зелёного каналов (присутствие красного/зелёного понижает оценку корабля). </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/imagemap.jpg"><br>
Пример сопоставления изображения с баллами по классам. Для наглядности предположим, что изображение состоит всего из 4 пикселей (4 монохромных пикселя, в этом примере мы не рассматриваем цветовые каналы для краткости) и что у нас есть 3 класса (красный (кошка), зелёный (собака), синий (корабль)). (Уточнение: в частности, цвета здесь просто обозначают 3 класса и не связаны с каналами RGB.) Мы растягиваем пиксели изображения в столбец и выполняем умножение матриц, чтобы получить баллы по каждому классу. Обратите внимание, что этот конкретный набор весовых коэффициентов W совсем не хорош: весовые коэффициенты присваивают нашему изображению кошки очень низкий балл. В частности, этот набор весовых коэффициентов, похоже, убеждён, что видит собаку. </p>
<hr>
<p><strong>Аналогия изображений с многомерными точками</strong>. Поскольку изображения растягиваются в многомерные векторы-столбцы, мы можем интерпретировать каждое изображение как отдельную точку в этом пространстве (например, каждое изображение в CIFAR-10 — это точка в 3072-мерном пространстве размером 32x32x3 пикселя). Аналогично, весь набор данных — это (помеченный) набор точек.   </p>
<p>Поскольку мы определили оценку каждого класса как взвешенную сумму всех пикселей изображения, оценка каждого класса является линейной функцией в этом пространстве. Мы не можем визуализировать 3072-мерное пространство, но если мы представим, что все эти измерения сведены к двум, то сможем попытаться визуализировать, что может делать классификатор: </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/pixelspace.jpeg"><br>
Мультяшное представление пространства изображений, где каждое изображение представляет собой одну точку, а три классификатора визуализированы. На примере классификатора автомобилей (красным цветом) красная линия показывает все точки в пространстве, которые получают нулевой балл за класс автомобилей. Красная стрелка показывает направление увеличения, поэтому все точки справа от красной линии имеют положительные (и линейно возрастающие) баллы, а все точки слева — отрицательные (и линейно убывающие) баллы. </p>
<hr>
<p>Как мы видели выше, каждый ряд <strong>W</strong> является классификатором для одного из классов. Геометрическая интерпретация этих чисел заключается в том, что при изменении одного из столбцов <strong>W</strong> соответствующая линия в пиксельном пространстве будет поворачиваться в разных направлениях. Смещение <strong>b</strong>. С другой стороны, наши классификаторы позволяют переводить строки. В частности, обратите внимание, что без коэффициентов смещения подстановка <strong>$ x_i = 0 $</strong> всегда будет давать нулевой результат независимо от весов, поэтому все линии будут вынуждены пересекать начало координат.   </p>
<p><strong>Интерпретация линейных классификаторов как сопоставление шаблонов</strong>. Другая интерпретация весовых коэффициентов <strong>W</strong> заключается в том, что каждая строка <strong>W</strong> соответствует <em>шаблону</em> (или, как его иногда называют, <em>прототипу</em>) для одного из классов. Оценка каждого класса для изображения затем получается путём сравнения каждого шаблона с изображением с помощью <em>скалярного произведения</em> (или <em>точечного произведения</em>) по очереди, чтобы найти наиболее подходящий. В этой терминологии линейный классификатор выполняет сопоставление шаблонов, которые он изучает. Другой способ взглянуть на это — представить, что мы по-прежнему используем метод ближайшего соседа, но вместо тысяч обучающих изображений мы используем только одно изображение для каждого класса (хотя мы его изучим, и оно не обязательно должно быть одним из изображений в обучающем наборе), и в качестве расстояния мы используем (отрицательное) скалярное произведение вместо расстояния L1 или L2. </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/templates.jpg"><br>
Немного забегая вперёд: пример изученных весовых коэффициентов в конце обучения для CIFAR-10. Обратите внимание, что, например, шаблон корабля содержит много синих пикселей, как и ожидалось. Таким образом, этот шаблон будет давать высокий балл при сопоставлении с изображениями кораблей в океане с помощью скалярного произведения. </p>
<hr>
<p>Кроме того, обратите внимание, что шаблон лошади, по-видимому, содержит двухголовую лошадь, что связано с тем, что в наборе данных есть лошади, смотрящие как влево, так и вправо. Линейный классификатор <em>объединяет</em> эти два вида лошадей в данных в один шаблон. Аналогичным образом, классификатор автомобилей, по-видимому, объединил несколько видов в один шаблон, который должен распознавать автомобили со всех сторон и всех цветов. В частности, этот шаблон оказался красным, что указывает на то, что в наборе данных CIFAR-10 больше красных автомобилей, чем автомобилей любого другого цвета. Линейный классификатор слишком слаб, чтобы правильно распознавать автомобили разных цветов, но, как мы увидим позже, нейронные сети позволят нам выполнить эту задачу. Забегая немного вперёд, скажу, что нейронная сеть сможет создавать промежуточные нейроны в своих скрытых слоях, которые смогут распознавать определённые типы автомобилей (например, зелёный автомобиль, поворачивающий налево, синий автомобиль, поворачивающий вперёд, и т. д.), а нейроны на следующем слое смогут объединять их в более точную оценку автомобиля с помощью взвешенной суммы отдельных детекторов автомобилей.   </p>
<p><strong>Уловка с предвзятостью</strong>. Прежде чем двигаться дальше, мы хотим упомянуть распространённую упрощающую уловку для представления двух параметров <strong>W</strong>,<strong>b</strong> как один. Напомним, что мы определили функцию оценки как:   </p>
<p>$$
f(x_i, W, b) = W x_i + b
$$</p>
<p>По мере изучения материала становится немного сложнее отслеживать два набора параметров (смещения <strong>b</strong> и веса <strong>W</strong>) по отдельности. Часто используемый приём заключается в объединении двух наборов параметров в одну матрицу, которая содержит их оба, путём расширения вектора $ x_i $ с одним дополнительным измерением, которое всегда сохраняет константу 1 - <em>измерение смещения</em> по умолчанию. С дополнительным измерением новая функция оценки упростится до простого умножения матриц: 
$$
f(x_i, W) = W x_i
$$
С нашим примером CIFAR-10, $ x_i $ теперь [3073 x 1] вместо [3072 x 1] — (с дополнительным измерением, содержащим константу 1), и <strong>W</strong> теперь имеет значение [10 x 3073] вместо [10 x 3072]. Дополнительный столбец, который <strong>W</strong> теперь соответствует смещению <strong>b</strong>. Иллюстрация могла бы помочь прояснить ситуацию: </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/wb.jpeg"><br>
Иллюстрация трюка с предвзятостью. Выполнение матричного умножения с последующим добавлением вектора смещения (слева) эквивалентно добавлению размера смещения с константой 1 ко всем входным векторам и расширению весовой матрицы на 1 столбец - столбец смещения (справа). Таким образом, если мы предварительно обработаем наши данные, добавив единицы ко всем векторам, нам нужно будет выучить только одну матрицу весов вместо двух матриц, которые содержат веса и отклонения. </p>
<hr>
<p><strong>Предварительная обработка данных изображения</strong>. В качестве примечания: в приведённых выше примерах мы использовали необработанные значения пикселей (которые находятся в диапазоне [0…255]). В машинном обучении очень распространена практика нормализации входных признаков (в случае изображений каждый пиксель считается признаком). В частности, важно <strong>центрировать данные</strong>, вычитая среднее значение из каждого признака. В случае с изображениями это соответствует вычислению <em>среднего значения изображения</em> по обучающим изображениям и вычитанию его из каждого изображения, чтобы получить изображения, в которых пиксели находятся в диапазоне примерно от [-127 до 127]. Далее обычно выполняется предварительная обработка, при которой каждый входной признак масштабируется так, чтобы его значения находились в диапазоне от [-1 до 1]. Из них, пожалуй, более важным является центрирование относительно нуля, но нам придётся подождать с его обоснованием, пока мы не поймём динамику градиентного спуска.   </p>
<h3>Функция потерь</h3>
<p>В предыдущем разделе мы определили функцию преобразования значений пикселей в оценки классов, которая параметризуется набором весовых коэффициентов <strong>W</strong>. Более того, мы увидели, что у нас нет контроля над данными $(x_i, y_i)$ (это фиксировано и задано), но мы можем управлять этими весами и хотим установить их так, чтобы прогнозируемые оценки классов соответствовали исходным меткам в обучающих данных. 
Например, если вернуться к примеру с изображением кошки и её оценками для классов «кошка», «собака» и «корабль», то мы увидим, что конкретный набор весов в этом примере был не очень хорошим: мы ввели пиксели, изображающие кошку, но оценка кошки получилась очень низкой (-96,8) по сравнению с другими классами (оценка собаки 437,9, а оценка корабля 61,95). Мы будем измерять степень нашего недовольства такими результатами, как этот, с помощью <strong>функции потерь</strong> (иногда также называемой <strong>функцией затрат</strong> или <strong>целевой функцией</strong>). Интуитивно понятно, что потери будут высокими, если мы плохо справляемся с классификацией обучающих данных, и низкими, если мы хорошо справляемся.  </p>
<h4>Потеря машины мультиклассового опорного вектора</h4>
<p>Существует несколько способов определения параметров функции потерь. В качестве первого примера мы рассмотрим часто используемую функцию потерь, называемую <strong>многоклассовой функцией потерь машины опорных векторов</strong> (SVM). Функция потерь SVM настроена таким образом, что SVM «хочет», чтобы правильный класс для каждого изображения имел оценку выше, чем у неправильных классов, на некоторую фиксированную величину <strong>Δ</strong>. Обратите внимание, что иногда полезно очеловечить функции потерь, как мы сделали выше: SVM «хочет» определённого результата в том смысле, что этот результат приведёт к меньшим потерям (что хорошо).   </p>
<p>Теперь давайте уточним. Напомним, что для i-го примера нам даны пиксели изображения $ x_i $ и этикетка $ y_i $ которая определяет индекс правильного класса. Функция оценки принимает пиксели и вычисляет вектор $ f(x_i, W) $ оценок за класс, которые мы будем сокращать до <strong>s</strong> (сокращение от «баллы»). Например, балл за j-й класс — это j-й элемент: $ s_j = f(x_i, W)_j $. Потери многоклассового SVM для i-го примера формализуются следующим образом:   </p>
<p>$$
L_i = Σ_{j\neq y_i} max(0, s_j - s_{y_i} + \Delta)
$$ </p>
<p><strong>Пример</strong>. Давайте разберём это на примере, чтобы понять, как это работает. Предположим, что у нас есть три класса, которые получают оценки <strong>s=[13,−7,11]</strong>, и что первый класс является истинным классом (т.е. $ y_i = 0 $ ). Также предположим, что <strong>Δ</strong> (гиперпараметр, о котором мы вскоре поговорим подробнее) равен 10. Приведенное выше выражение суммирует все неправильные классы ($ j \neq y_i $), таким образом, мы получаем два термина:   </p>
<p>$$
L_i = max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)
$$ </p>
<p>Вы видите, что первый член даёт ноль, так как [-7 - 13 + 10] даёт отрицательное число, которое затем округляется до ннля с помощью <strong>max(0,−)</strong>
функция. Мы получаем нулевые потери для этой пары, потому что оценка правильного класса (13) была больше, чем оценка неправильного класса (-7), как минимум на 10. На самом деле разница составляла 20, что намного больше 10, но SVM интересует только то, что разница составляет не менее 10; любая дополнительная разница, превышающая 10, ограничивается нулём с помощью операции <em>max</em>. Второй член вычисляет [11 - 13 + 10], что даёт 8. То есть, даже если правильный класс имел более высокий балл, чем неправильный (13 &gt; 11), он не был выше желаемого значения в 10 баллов. Разница составила всего 2, поэтому проигрыш равен 8 (т. Е. Насколько выше должна быть разница, чтобы соответствовать марже). Таким образом, функция SVM loss запрашивает оценку правильного класса $ y_i = 0 $ быть больше, чем неправильные оценки класса, по крайней мере, на <strong>Δ</strong> (дельта). Если это не так, мы понесём убытки.   </p>
<p>Обратите внимание, что в этом конкретном модуле мы работаем с линейными функциями оценки ( $ f(x_i; W) = W x_i $ ), поэтому мы также можем переписать функцию потерь в этой эквивалентной форме:   </p>
<p>$$
L_i = Σ_{j\neq y_i} max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$  </p>
<p>где $ w_j$ является j-й строкой <strong>W</strong> преобразован в столбец. Однако это не обязательно будет так, если мы начнём рассматривать более сложные формы функции оценки <strong>f</strong>.   </p>
<p>Последний термин, который мы упомянем, прежде чем закончить этот раздел, — это нулевой порог <strong>max(0,−)</strong>. Эта функция часто называется <strong>потерей от перегиба</strong>. Иногда можно услышать, что вместо этого люди используют SVM с квадратичной потерей от перегиба (или L2-SVM), которая имеет вид <strong>$max(0,−) ^ 2$</strong>
это сильнее6 сказывается на нарушении границ (квадратично, а не линейно). Неквадратичная версия является более стандартной, но в некоторых наборах данных квадратичная функция потерь может работать лучше. Это можно определить во время перекрестной проверки.   </p>
<blockquote>
<p>Функция потерь количественно определяет наше недовольство прогнозами на обучающем наборе</p>
</blockquote>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/margin.jpg"></p>
<p>Многоклассовая машина опорных векторов «хочет», чтобы оценка правильного класса была выше, чем у всех остальных классов, как минимум на величину дельты. Если оценка какого-либо класса находится в красной области (или выше), то будет накоплен убыток. В противном случае убыток будет равен нулю. Наша цель — найти веса, которые одновременно удовлетворят этому ограничению для всех примеров в обучающих данных и обеспечат минимально возможный общий убыток.  </p>
<hr>
<p><strong>Регуляризация</strong>. Есть одна ошибка с функцией потерь, которую мы представили выше. Предположим, что у нас есть набор данных и набор параметров <strong>W</strong>, которые правильно классифицируют каждый пример (т.е. все оценки таковы, что все поля соблюдены, и <strong>$L_i = 0)\</strong> для всех i). Проблема в том, что этот набор <strong>W</strong> не обязательно уникален: может быть много похожих <strong>W</strong>, которые правильно классифицируют примеры. Один из простых способов увидеть это заключается в том, что если некоторые параметры <strong>W</strong> правильно классифицируют все примеры (так что потери равны нулю для каждого примера), то любое кратное этим параметрам <strong>λW</strong>, где <strong>λ&gt;1</strong> также даст нулевой убыток, потому что это преобразование равномерно растягивает все величины счета и, следовательно, их абсолютные разности. Например, если разница в оценках между правильным классом и ближайшим неправильным классом равна 15, то умножение всех элементов <strong>W</strong> на 2 даст новую разницу 30.  </p>
<p>Другими словами, мы хотим закодировать некоторое предпочтение для определенного набора весов <strong>W</strong> по сравнению с другими, чтобы устранить эту двусмысленность. Мы можем сделать это, расширив функцию потерь штрафом за регуляризацию <strong>R(W)</strong>. Наиболее распространенным штрафом за регуляризацию является квадрат нормы <strong>L2</strong>, который препятствует использованию больших весов с помощью квадратичного штрафа по всем параметрам:  </p>
<p>$$
R(W) = \sum_k\sum_l W_{k,l}^2
$$  </p>
<p>В приведенном выше выражении мы суммируем все возведенные в квадрат элементы <strong>W</strong>
Обратите внимание, что функция регуляризации не зависит от данных, она зависит только от весовых коэффициентов. Включение штрафа за регуляризацию завершает формирование полной функции потерь многоклассовой машины опорных векторов, которая состоит из двух компонентов: <strong>потерь данных</strong> (которые представляют собой средние потери <strong>$Li$</strong> по всем примерам) и <strong>потери от регуляризации</strong>. То есть полная потеря многоклассового SVM становится:  </p>
<p>$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }<em>\text{потеря данных} + \underbrace{ \lambda R(W) }</em>\text{потеря регуляризации} \\
$$  </p>
<p>Или расширить это в его полной форме:  </p>
<p>$$
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)<em y_i>j - f(x_i; W)</em>^2
$$  } + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l</p>
<p>Где <strong>N</strong>— это количество обучающих примеров. Как видите, мы добавляем штраф за регуляризацию к функции потерь, взвешенной с помощью гиперпараметра <strong>λ</strong>.
Не существует простого способа задать этот гиперпараметр, и обычно он определяется методом перекрёстной проверки.   </p>
<p>Помимо мотивации, которую мы привели выше, существует множество желательных свойств, связанных с включением штрафа за регуляризацию, к которым мы вернёмся в следующих разделах. Например, оказывается, что включение штрафа L2 приводит к привлекательному свойству <strong>максимального запаса прочности</strong> в SVM (если вам интересно, см. <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">CS229</a> для получения полной информации).  </p>
<p>Наиболее привлекательным свойством является то, что штрафные санкции за большие веса, как правило, улучшают обобщение, поскольку это означает, что ни один входной параметр сам по себе не может оказывать очень сильное влияние на оценки. Например, предположим, что у нас есть некоторый входной вектор <strong>$x=[1,1,1,1] ))</strong> и два весовых вектора__$w1=[1,0,0,0] $<strong>, </strong>$w2=[0,25,0,25,0,25,0,25]  $__. Затем $w_1^Tx = w_2^Tx = 1$. Таким образом, оба вектора весов приводят к одному и тому же скалярному произведению, но штраф L2 $w_1$ равно 1.0, в то время как штраф L2 равен $w_2$ составляет всего 0,5. Следовательно, согласно штрафу L2, вектор весов $w_2$ это предпочтительнее, так как достигается меньшая потеря при регуляризации. Интуитивно понятно, что это происходит потому, что веса в $w_2$ являются более компактными и менее размытыми. Поскольку штраф L2 предпочитает более компактные и менее размытые векторы весов, итоговому классификатору рекомендуется учитывать все входные параметры в небольших количествах, а не несколько входных параметров в очень больших количествах. Как мы увидим далее в этом курсе, этот эффект может улучшить обобщающую способность классификаторов на тестовых изображениях и привести к меньшему <em>переобучению</em>.  </p>
<p>Обратите внимание, что смещения не оказывают такого же эффекта, поскольку, в отличие от весовых коэффициентов, они не контролируют силу влияния входного параметра. Поэтому обычно нормализуют только весовые коэффициенты <strong>W</strong> но не из - за предубеждений <strong>b</strong>. Однако на практике это часто оказывается несущественным. Наконец, обратите внимание, что из-за штрафа за регуляризацию мы никогда не сможем добиться потери точности, равной 0,0, во всех примерах, потому что это возможно только в патологических условиях <strong>W=0</strong>.  </p>
<p><strong>Код</strong>. Вот функция потерь (без регуляризации), реализованная на Python как в не векторизованной, так и в полувекторной форме:  </p>
<div class="code"><pre class="code literal-block"><span class="n">def</span><span class="w"> </span><span class="n">L_i</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="err">:</span>
<span class="w">  </span><span class="ss">"""</span>
<span class="ss">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span>
<span class="ss">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span>
<span class="ss">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span>
<span class="ss">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span>
<span class="ss">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span>
<span class="ss">  """</span>
<span class="w">  </span><span class="n">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">notes</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="n">later</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="k">section</span>
<span class="w">  </span><span class="n">scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="n">becomes</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">class</span>
<span class="w">  </span><span class="n">correct_class_score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scores</span><span class="o">[</span><span class="n">y</span><span class="o">]</span>
<span class="w">  </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">classes</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="mi">10</span>
<span class="w">  </span><span class="n">loss_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">D</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">iterate</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="n">classes</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">y</span><span class="p">:</span>
<span class="w">      </span><span class="err">#</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">true</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">incorrect</span><span class="w"> </span><span class="n">classes</span>
<span class="w">      </span><span class="k">continue</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">accumulate</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">i</span><span class="o">-</span><span class="n">th</span><span class="w"> </span><span class="n">example</span>
<span class="w">    </span><span class="n">loss_i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">scores</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">correct_class_score</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="p">)</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">loss_i</span>

<span class="n">def</span><span class="w"> </span><span class="n">L_i_vectorized</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="err">:</span>
<span class="w">  </span><span class="ss">"""</span>
<span class="ss">  A faster half-vectorized implementation. half-vectorized</span>
<span class="ss">  refers to the fact that for a single example the implementation contains</span>
<span class="ss">  no for loops, but there is still one loop over the examples (outside this function)</span>
<span class="ss">  """</span>
<span class="w">  </span><span class="n">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span>
<span class="w">  </span><span class="n">scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="k">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">margins</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">classes</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">vector</span><span class="w"> </span><span class="k">operation</span>
<span class="w">  </span><span class="n">margins</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">scores</span><span class="o">[</span><span class="n">y</span><span class="o">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="p">)</span>
<span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">y</span><span class="o">-</span><span class="n">th</span><span class="w"> </span><span class="k">position</span><span class="w"> </span><span class="n">scores</span><span class="o">[</span><span class="n">y</span><span class="o">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">scores</span><span class="o">[</span><span class="n">y</span><span class="o">]</span><span class="w"> </span><span class="n">canceled</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">gave</span><span class="w"> </span><span class="n">delta</span><span class="p">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">want</span>
<span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">ignore</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">y</span><span class="o">-</span><span class="n">th</span><span class="w"> </span><span class="k">position</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">consider</span><span class="w"> </span><span class="n">margin</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="nf">max</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="k">class</span>
<span class="w">  </span><span class="n">margins</span><span class="o">[</span><span class="n">y</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">  </span><span class="n">loss_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">margins</span><span class="p">)</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">loss_i</span>

<span class="n">def</span><span class="w"> </span><span class="n">L</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="err">:</span>
<span class="w">  </span><span class="ss">"""</span>
<span class="ss">  fully-vectorized implementation :</span>
<span class="ss">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span>
<span class="ss">  - y is array of integers specifying correct class (e.g. 50,000-D array)</span>
<span class="ss">  - W are weights (e.g. 10 x 3073)</span>
<span class="ss">  """</span>
<span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="k">without</span><span class="w"> </span><span class="k">using</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">loops</span>
<span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nf">left</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">exercise</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">assignment</span>
<span class="w">  </span><span class="err">```</span>

<span class="n">Из</span><span class="w"> </span><span class="n">этого</span><span class="w"> </span><span class="n">раздела</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">сделать</span><span class="w"> </span><span class="n">вывод</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">функция</span><span class="w"> </span><span class="n">потерь</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="n">использует</span><span class="w"> </span><span class="n">особый</span><span class="w"> </span><span class="n">подход</span><span class="w"> </span><span class="n">к</span><span class="w"> </span><span class="n">измерению</span><span class="w"> </span><span class="n">того</span><span class="p">,</span><span class="w"> </span><span class="n">насколько</span><span class="w"> </span><span class="n">прогнозы</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">основе</span><span class="w"> </span><span class="n">обучающих</span><span class="w"> </span><span class="n">данных</span><span class="w"> </span><span class="n">соответствуют</span><span class="w"> </span><span class="n">истинным</span><span class="w"> </span><span class="n">значениям</span><span class="p">.</span><span class="w"> </span><span class="n">Кроме</span><span class="w"> </span><span class="n">того</span><span class="p">,</span><span class="w"> </span><span class="n">делать</span><span class="w"> </span><span class="n">хорошие</span><span class="w"> </span><span class="n">прогнозы</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">основе</span><span class="w"> </span><span class="n">обучающего</span><span class="w"> </span><span class="n">набора</span><span class="w"> </span><span class="n">данных</span><span class="w"> </span><span class="n">равносильно</span><span class="w"> </span><span class="n">минимизации</span><span class="w"> </span><span class="n">функции</span><span class="w"> </span><span class="n">потерь</span><span class="p">.</span><span class="w">  </span>

<span class="o">&gt;</span><span class="n">Теперь</span><span class="w"> </span><span class="n">нам</span><span class="w"> </span><span class="n">нужно</span><span class="w"> </span><span class="n">придумать</span><span class="w"> </span><span class="n">способ</span><span class="w"> </span><span class="n">найти</span><span class="w"> </span><span class="n">веса</span><span class="p">,</span><span class="w"> </span><span class="n">которые</span><span class="w"> </span><span class="n">минимизируют</span><span class="w"> </span><span class="n">потери</span><span class="p">.</span><span class="w">  </span>

<span class="err">##</span><span class="w"> </span><span class="n">Практические</span><span class="w"> </span><span class="n">соображения</span><span class="w">  </span>

<span class="n">__Устанавливаем</span><span class="w"> </span><span class="n">дельту__</span><span class="p">.</span><span class="w"> </span><span class="n">Обратите</span><span class="w"> </span><span class="n">внимание</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">затронули</span><span class="w"> </span><span class="n">гиперпараметр</span><span class="w"> </span><span class="n">__Δ__</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">его</span><span class="w"> </span><span class="n">настройка</span><span class="p">.</span><span class="w"> </span><span class="n">Какое</span><span class="w"> </span><span class="n">значение</span><span class="w"> </span><span class="n">следует</span><span class="w"> </span><span class="n">установить</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">нужно</span><span class="w"> </span><span class="n">ли</span><span class="w"> </span><span class="n">проводить</span><span class="w"> </span><span class="n">перекрестную</span><span class="w"> </span><span class="n">проверку</span><span class="vm">?</span><span class="w"> </span><span class="n">Оказывается</span><span class="p">,</span><span class="w"> </span><span class="n">этот</span><span class="w"> </span><span class="n">гиперпараметр</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">смело</span><span class="w"> </span><span class="n">устанавливать</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">__Δ</span><span class="o">=</span><span class="mf">1.0</span><span class="n">_</span><span class="w"> </span><span class="n">во</span><span class="w"> </span><span class="n">всех</span><span class="w"> </span><span class="n">случаях</span><span class="p">.</span><span class="w"> </span><span class="n">Гиперпараметры</span><span class="w"> </span><span class="n">__Δ__</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="nl">__λ__</span><span class="p">:</span><span class="w"> </span><span class="n">кажется</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">это</span><span class="w"> </span><span class="n">два</span><span class="w"> </span><span class="n">разных</span><span class="w"> </span><span class="n">гиперпараметра</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">самом</span><span class="w"> </span><span class="n">деле</span><span class="w"> </span><span class="n">они</span><span class="w"> </span><span class="n">оба</span><span class="w"> </span><span class="n">управляют</span><span class="w"> </span><span class="n">одним</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">тем</span><span class="w"> </span><span class="n">же</span><span class="w"> </span><span class="nl">компромиссом</span><span class="p">:</span><span class="w"> </span><span class="n">компромиссом</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">потерей</span><span class="w"> </span><span class="n">данных</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">потерей</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">регуляризации</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">целевой</span><span class="w"> </span><span class="n">функции</span><span class="p">.</span><span class="w"> </span><span class="n">Ключ</span><span class="w"> </span><span class="n">к</span><span class="w"> </span><span class="n">пониманию</span><span class="w"> </span><span class="n">этого</span><span class="w"> </span><span class="n">заключается</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">том</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">величина</span><span class="w"> </span><span class="n">весовых</span><span class="w"> </span><span class="n">коэффициентов</span><span class="w"> </span><span class="n">__W__</span><span class="w"> </span><span class="n">оказывает</span><span class="w"> </span><span class="n">непосредственное</span><span class="w"> </span><span class="n">влияние</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">баллы</span><span class="w"> </span><span class="p">(</span><span class="n">и</span><span class="p">,</span><span class="w"> </span><span class="n">следовательно</span><span class="p">,</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">их</span><span class="w"> </span><span class="n">разницу</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="n">по</span><span class="w"> </span><span class="n">мере</span><span class="w"> </span><span class="n">уменьшения</span><span class="w"> </span><span class="n">всех</span><span class="w"> </span><span class="n">значений</span><span class="w"> </span><span class="n">внутри</span><span class="w"> </span><span class="n">__W__</span><span class="w"> </span><span class="n">разница</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">баллах</span><span class="w"> </span><span class="n">будет</span><span class="w"> </span><span class="n">уменьшаться</span><span class="p">,</span><span class="w"> </span><span class="n">а</span><span class="w"> </span><span class="n">по</span><span class="w"> </span><span class="n">мере</span><span class="w"> </span><span class="n">увеличения</span><span class="w"> </span><span class="n">весов</span><span class="w"> </span><span class="n">разница</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">баллах</span><span class="w"> </span><span class="n">будет</span><span class="w"> </span><span class="n">увеличиваться</span><span class="p">.</span><span class="w"> </span><span class="n">Таким</span><span class="w"> </span><span class="n">образом</span><span class="p">,</span><span class="w"> </span><span class="n">точное</span><span class="w"> </span><span class="n">значение</span><span class="w"> </span><span class="n">разницы</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">баллами</span><span class="w"> </span><span class="p">(</span><span class="n">например</span><span class="p">,</span><span class="w"> </span><span class="n">__Δ</span><span class="o">=</span><span class="mi">1</span><span class="n">__</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">или</span><span class="w"> </span><span class="n">__Δ</span><span class="o">=</span><span class="mi">100</span><span class="n">__</span><span class="p">)</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">некотором</span><span class="w"> </span><span class="n">смысле</span><span class="w"> </span><span class="n">бессмысленно</span><span class="p">,</span><span class="w"> </span><span class="n">потому</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">веса</span><span class="w"> </span><span class="n">могут</span><span class="w"> </span><span class="n">произвольно</span><span class="w"> </span><span class="n">уменьшать</span><span class="w"> </span><span class="n">или</span><span class="w"> </span><span class="n">увеличивать</span><span class="w"> </span><span class="n">разницу</span><span class="p">.</span><span class="w"> </span><span class="n">Следовательно</span><span class="p">,</span><span class="w"> </span><span class="n">единственный</span><span class="w"> </span><span class="n">реальный</span><span class="w"> </span><span class="n">компромисс</span><span class="w"> </span><span class="n">заключается</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">том</span><span class="p">,</span><span class="w"> </span><span class="n">насколько</span><span class="w"> </span><span class="n">сильно</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">позволяем</span><span class="w"> </span><span class="n">весам</span><span class="w"> </span><span class="n">увеличиваться</span><span class="w"> </span><span class="p">(</span><span class="n">с</span><span class="w"> </span><span class="n">помощью</span><span class="w"> </span><span class="n">силы</span><span class="w"> </span><span class="n">регуляризации</span><span class="w"> </span><span class="n">__λ__</span><span class="p">).</span><span class="w">  </span>

<span class="n">__Связь</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">бинарной</span><span class="w"> </span><span class="n">машиной</span><span class="w"> </span><span class="n">опорных</span><span class="w"> </span><span class="n">векторов__</span><span class="p">.</span><span class="w"> </span><span class="n">Возможно</span><span class="p">,</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">пришли</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">этот</span><span class="w"> </span><span class="n">курс</span><span class="p">,</span><span class="w"> </span><span class="n">уже</span><span class="w"> </span><span class="n">имея</span><span class="w"> </span><span class="n">опыт</span><span class="w"> </span><span class="n">работы</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">бинарными</span><span class="w"> </span><span class="n">машинами</span><span class="w"> </span><span class="n">опорных</span><span class="w"> </span><span class="n">векторов</span><span class="p">,</span><span class="w"> </span><span class="n">где</span><span class="w"> </span><span class="n">потери</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">i</span><span class="o">-</span><span class="n">го</span><span class="w"> </span><span class="n">примера</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">записать</span><span class="w"> </span><span class="nl">как</span><span class="p">:</span><span class="w">  </span>

<span class="err">$$</span>
<span class="n">L_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="err">\</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y_i</span><span class="w"> </span><span class="n">w</span><span class="o">^</span><span class="n">Tx_i</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">R</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="err">$$</span><span class="w">  </span>

<span class="n">где</span><span class="w"> </span><span class="n">__C__</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">гиперпараметром</span><span class="p">,</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="err">$</span><span class="n">y_i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\\{</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="err">\\}</span><span class="w"> </span><span class="err">$</span><span class="p">.</span><span class="w"> </span><span class="n">Вы</span><span class="w"> </span><span class="n">можете</span><span class="w"> </span><span class="n">убедиться</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">представленная</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">этом</span><span class="w"> </span><span class="n">разделе</span><span class="w"> </span><span class="n">формулировка</span><span class="w"> </span><span class="n">содержит</span><span class="w"> </span><span class="n">бинарный</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">качестве</span><span class="w"> </span><span class="n">частного</span><span class="w"> </span><span class="n">случая</span><span class="p">,</span><span class="w"> </span><span class="n">когда</span><span class="w"> </span><span class="n">есть</span><span class="w"> </span><span class="n">только</span><span class="w"> </span><span class="n">два</span><span class="w"> </span><span class="n">класса</span><span class="p">.</span><span class="w"> </span><span class="n">То</span><span class="w"> </span><span class="n">есть</span><span class="p">,</span><span class="w"> </span><span class="n">если</span><span class="w"> </span><span class="n">бы</span><span class="w"> </span><span class="n">у</span><span class="w"> </span><span class="n">нас</span><span class="w"> </span><span class="n">было</span><span class="w"> </span><span class="n">только</span><span class="w"> </span><span class="n">два</span><span class="w"> </span><span class="n">класса</span><span class="p">,</span><span class="w"> </span><span class="n">то</span><span class="w"> </span><span class="n">потери</span><span class="w"> </span><span class="n">сводились</span><span class="w"> </span><span class="n">бы</span><span class="w"> </span><span class="n">к</span><span class="w"> </span><span class="n">бинарному</span><span class="w"> </span><span class="n">SVM</span><span class="p">,</span><span class="w"> </span><span class="n">показанному</span><span class="w"> </span><span class="n">выше</span><span class="p">.</span><span class="w"> </span><span class="n">Кроме</span><span class="w"> </span><span class="n">того</span><span class="p">,</span><span class="w"> </span><span class="n">__C__</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">этой</span><span class="w"> </span><span class="n">формулировке</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">__λ__</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">нашей</span><span class="w"> </span><span class="n">формулировке</span><span class="w"> </span><span class="n">контролируется</span><span class="w"> </span><span class="n">один</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">тот</span><span class="w"> </span><span class="n">же</span><span class="w"> </span><span class="n">компромисс</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">они</span><span class="w"> </span><span class="n">связаны</span><span class="w"> </span><span class="n">через</span><span class="w"> </span><span class="n">взаимное</span><span class="w"> </span><span class="n">отношение</span><span class="w"> </span><span class="err">$</span><span class="n">C</span><span class="w"> </span><span class="err">\</span><span class="n">propto</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="mi">1</span><span class="err">}{\</span><span class="n">lambda</span><span class="err">}$</span><span class="p">.</span><span class="w">  </span>

<span class="nl">__Примечание</span><span class="p">:</span><span class="w"> </span><span class="n">оптимизация</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">прямой</span><span class="w"> </span><span class="n">форме__</span><span class="p">.</span><span class="w"> </span><span class="n">Если</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">пришли</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">этот</span><span class="w"> </span><span class="n">курс</span><span class="p">,</span><span class="w"> </span><span class="n">уже</span><span class="w"> </span><span class="n">имея</span><span class="w"> </span><span class="n">представление</span><span class="w"> </span><span class="n">о</span><span class="w"> </span><span class="n">SVM</span><span class="p">,</span><span class="w"> </span><span class="n">то</span><span class="p">,</span><span class="w"> </span><span class="n">возможно</span><span class="p">,</span><span class="w"> </span><span class="n">слышали</span><span class="w"> </span><span class="n">о</span><span class="w"> </span><span class="n">ядрах</span><span class="p">,</span><span class="w"> </span><span class="n">двойственных</span><span class="w"> </span><span class="n">задачах</span><span class="p">,</span><span class="w"> </span><span class="n">алгоритме</span><span class="w"> </span><span class="n">SMO</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">т</span><span class="p">.</span><span class="w"> </span><span class="n">д</span><span class="p">.</span><span class="w"> </span><span class="n">В</span><span class="w"> </span><span class="n">этом</span><span class="w"> </span><span class="n">курсе</span><span class="w"> </span><span class="p">(</span><span class="n">как</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">случае</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">нейронными</span><span class="w"> </span><span class="n">сетями</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">целом</span><span class="p">)</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">всегда</span><span class="w"> </span><span class="n">будем</span><span class="w"> </span><span class="n">работать</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">задачами</span><span class="w"> </span><span class="n">оптимизации</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">их</span><span class="w"> </span><span class="n">прямой</span><span class="w"> </span><span class="n">форме</span><span class="w"> </span><span class="n">без</span><span class="w"> </span><span class="n">ограничений</span><span class="p">.</span><span class="w"> </span><span class="n">Многие</span><span class="w"> </span><span class="n">из</span><span class="w"> </span><span class="n">этих</span><span class="w"> </span><span class="n">задач</span><span class="w"> </span><span class="n">технически</span><span class="w"> </span><span class="n">не</span><span class="w"> </span><span class="n">являются</span><span class="w"> </span><span class="n">дифференцируемыми</span><span class="w"> </span><span class="p">(</span><span class="n">например</span><span class="p">,</span><span class="w"> </span><span class="n">функция</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="n">не</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">дифференцируемой</span><span class="p">,</span><span class="w"> </span><span class="n">потому</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">имеет</span><span class="w"> </span><span class="n">излом</span><span class="w"> </span><span class="n">при</span><span class="w"> </span><span class="n">x</span><span class="o">=</span><span class="n">y</span><span class="p">),</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">практике</span><span class="w"> </span><span class="n">это</span><span class="w"> </span><span class="n">не</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">проблемой</span><span class="p">,</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">обычно</span><span class="w"> </span><span class="n">используется</span><span class="w"> </span><span class="n">субградиент</span><span class="p">.</span><span class="w">  </span>

<span class="nl">Примечание</span><span class="p">:</span><span class="w"> </span><span class="n">другие</span><span class="w"> </span><span class="n">многоклассовые</span><span class="w"> </span><span class="n">формулировки</span><span class="w"> </span><span class="n">SVM</span><span class="p">.</span><span class="w"> </span><span class="n">Стоит</span><span class="w"> </span><span class="n">отметить</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">многоклассовая</span><span class="w"> </span><span class="n">формулировка</span><span class="w"> </span><span class="n">SVM</span><span class="p">,</span><span class="w"> </span><span class="n">представленная</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">этом</span><span class="w"> </span><span class="n">разделе</span><span class="p">,</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">одним</span><span class="w"> </span><span class="n">из</span><span class="w"> </span><span class="n">немногих</span><span class="w"> </span><span class="n">способов</span><span class="w"> </span><span class="n">формулировки</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">нескольких</span><span class="w"> </span><span class="n">классов</span><span class="p">.</span><span class="w"> </span><span class="n">Другой</span><span class="w"> </span><span class="n">часто</span><span class="w"> </span><span class="n">используемой</span><span class="w"> </span><span class="n">формой</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="n">_</span><span class="err">«</span><span class="n">один</span><span class="w"> </span><span class="n">против</span><span class="w"> </span><span class="n">всех</span><span class="err">»</span><span class="n">_</span><span class="w"> </span><span class="p">(</span><span class="n">OVA</span><span class="p">),</span><span class="w"> </span><span class="n">которая</span><span class="w"> </span><span class="n">обучает</span><span class="w"> </span><span class="n">независимый</span><span class="w"> </span><span class="n">бинарный</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">каждого</span><span class="w"> </span><span class="n">класса</span><span class="w"> </span><span class="n">по</span><span class="w"> </span><span class="n">сравнению</span><span class="w"> </span><span class="n">со</span><span class="w"> </span><span class="n">всеми</span><span class="w"> </span><span class="n">остальными</span><span class="w"> </span><span class="n">классами</span><span class="p">.</span><span class="w"> </span><span class="n">С</span><span class="w"> </span><span class="n">ней</span><span class="w"> </span><span class="n">связана</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">реже</span><span class="w"> </span><span class="n">встречается</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">практике</span><span class="w"> </span><span class="n">стратегия</span><span class="w"> </span><span class="n">_</span><span class="err">«</span><span class="n">все</span><span class="w"> </span><span class="n">против</span><span class="w"> </span><span class="n">всех</span><span class="err">»</span><span class="n">_</span><span class="w"> </span><span class="p">(</span><span class="n">AVA</span><span class="p">).</span><span class="w"> </span><span class="n">Наша</span><span class="w"> </span><span class="n">формулировка</span><span class="w"> </span><span class="n">основана</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">версии</span><span class="w"> </span><span class="o">[</span><span class="n">Weston and Watkins 1999</span><span class="o">]</span><span class="p">(</span><span class="nl">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="p">.</span><span class="n">elen</span><span class="p">.</span><span class="n">ucl</span><span class="p">.</span><span class="n">ac</span><span class="p">.</span><span class="n">be</span><span class="o">/</span><span class="n">Proceedings</span><span class="o">/</span><span class="n">esann</span><span class="o">/</span><span class="n">esannpdf</span><span class="o">/</span><span class="n">es1999</span><span class="o">-</span><span class="mf">461.</span><span class="n">pdf</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">pdf</span><span class="p">),</span><span class="w"> </span><span class="n">которая</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">более</span><span class="w"> </span><span class="n">мощной</span><span class="w"> </span><span class="n">версией</span><span class="p">,</span><span class="w"> </span><span class="n">чем</span><span class="w"> </span><span class="n">OVA</span><span class="w"> </span><span class="p">(</span><span class="n">в</span><span class="w"> </span><span class="n">том</span><span class="w"> </span><span class="n">смысле</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">можете</span><span class="w"> </span><span class="n">создавать</span><span class="w"> </span><span class="n">многоклассовые</span><span class="w"> </span><span class="n">наборы</span><span class="w"> </span><span class="n">данных</span><span class="p">,</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">которых</span><span class="w"> </span><span class="n">эта</span><span class="w"> </span><span class="n">версия</span><span class="w"> </span><span class="n">может</span><span class="w"> </span><span class="n">обеспечить</span><span class="w"> </span><span class="n">нулевую</span><span class="w"> </span><span class="n">потерю</span><span class="w"> </span><span class="n">данных</span><span class="p">,</span><span class="w"> </span><span class="n">а</span><span class="w"> </span><span class="n">OVA</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">нет</span><span class="p">.</span><span class="w"> </span><span class="n">Если</span><span class="w"> </span><span class="n">интересно</span><span class="p">,</span><span class="w"> </span><span class="n">подробности</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">найти</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">статье</span><span class="p">).</span><span class="w"> </span><span class="n">Последняя</span><span class="w"> </span><span class="n">формулировка</span><span class="p">,</span><span class="w"> </span><span class="n">которую</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">можете</span><span class="w"> </span><span class="n">увидеть</span><span class="p">,</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">это</span><span class="w"> </span><span class="n">_структурированный_</span><span class="w"> </span><span class="n">SVM</span><span class="p">,</span><span class="w"> </span><span class="n">который</span><span class="w"> </span><span class="n">максимизирует</span><span class="w"> </span><span class="n">разницу</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">оценкой</span><span class="w"> </span><span class="n">правильного</span><span class="w"> </span><span class="n">класса</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">оценкой</span><span class="w"> </span><span class="n">наиболее</span><span class="w"> </span><span class="n">близкого</span><span class="w"> </span><span class="n">к</span><span class="w"> </span><span class="n">нему</span><span class="w"> </span><span class="n">неправильного</span><span class="w"> </span><span class="n">класса</span><span class="p">.</span><span class="w"> </span><span class="n">Понимание</span><span class="w"> </span><span class="n">различий</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">этими</span><span class="w"> </span><span class="n">формулировками</span><span class="w"> </span><span class="n">выходит</span><span class="w"> </span><span class="n">за</span><span class="w"> </span><span class="n">рамки</span><span class="w"> </span><span class="n">данного</span><span class="w"> </span><span class="n">курса</span><span class="p">.</span><span class="w"> </span><span class="n">Версия</span><span class="p">,</span><span class="w"> </span><span class="n">представленная</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">этих</span><span class="w"> </span><span class="n">заметках</span><span class="p">,</span><span class="w"> </span><span class="n">безопасна</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">использования</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">практике</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="p">,</span><span class="w"> </span><span class="n">возможно</span><span class="p">,</span><span class="w"> </span><span class="n">самая</span><span class="w"> </span><span class="n">простая</span><span class="w"> </span><span class="n">стратегия</span><span class="w"> </span><span class="n">OVA</span><span class="w"> </span><span class="n">тоже</span><span class="w"> </span><span class="n">будет</span><span class="w"> </span><span class="n">работать</span><span class="w"> </span><span class="p">(</span><span class="n">как</span><span class="w"> </span><span class="n">утверждают</span><span class="w"> </span><span class="n">Рикин</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">др</span><span class="p">.</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="mi">2004</span><span class="w"> </span><span class="n">году</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="o">[</span><span class="n">«В защиту классификации «один против всех»</span><span class="o">]</span><span class="p">(</span><span class="nl">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="p">.</span><span class="n">jmlr</span><span class="p">.</span><span class="n">org</span><span class="o">/</span><span class="n">papers</span><span class="o">/</span><span class="n">volume5</span><span class="o">/</span><span class="n">rifkin04a</span><span class="o">/</span><span class="n">rifkin04a</span><span class="p">.</span><span class="n">pdf</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">pdf</span><span class="p">)).</span><span class="w">  </span>

<span class="err">##</span><span class="w"> </span><span class="n">Классификатор</span><span class="w"> </span><span class="n">Softmax</span><span class="w">  </span>

<span class="n">Оказывается</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">SVM</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">один</span><span class="w"> </span><span class="n">из</span><span class="w"> </span><span class="n">двух</span><span class="w"> </span><span class="n">наиболее</span><span class="w"> </span><span class="n">распространённых</span><span class="w"> </span><span class="n">классификаторов</span><span class="p">.</span><span class="w"> </span><span class="n">Другой</span><span class="w"> </span><span class="n">популярный</span><span class="w"> </span><span class="n">вариант</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">__классификатор</span><span class="w"> </span><span class="n">Softmax__</span><span class="p">,</span><span class="w"> </span><span class="n">у</span><span class="w"> </span><span class="n">которого</span><span class="w"> </span><span class="n">другая</span><span class="w"> </span><span class="n">функция</span><span class="w"> </span><span class="n">потерь</span><span class="p">.</span><span class="w"> </span><span class="n">Если</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">раньше</span><span class="w"> </span><span class="n">слышали</span><span class="w"> </span><span class="n">о</span><span class="w"> </span><span class="n">классификаторе</span><span class="w"> </span><span class="n">бинарной</span><span class="w"> </span><span class="n">логистической</span><span class="w"> </span><span class="n">регрессии</span><span class="p">,</span><span class="w"> </span><span class="n">то</span><span class="w"> </span><span class="n">классификатор</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">это</span><span class="w"> </span><span class="n">его</span><span class="w"> </span><span class="n">обобщение</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">нескольких</span><span class="w"> </span><span class="n">классов</span><span class="p">.</span><span class="w"> </span><span class="n">В</span><span class="w"> </span><span class="n">отличие</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">SVM</span><span class="p">,</span><span class="w"> </span><span class="n">который</span><span class="w"> </span><span class="n">обрабатывает</span><span class="w"> </span><span class="n">выходные</span><span class="w"> </span><span class="n">данные</span><span class="w"> </span><span class="err">$</span><span class="n">f</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="err">$</span><span class="p">.</span><span class="w"> </span><span class="n">В</span><span class="w"> </span><span class="n">качестве</span><span class="w"> </span><span class="p">(</span><span class="n">некалиброванных</span><span class="w"> </span><span class="n">и</span><span class="p">,</span><span class="w"> </span><span class="n">возможно</span><span class="p">,</span><span class="w"> </span><span class="n">трудно</span><span class="w"> </span><span class="n">интерпретируемых</span><span class="p">)</span><span class="w"> </span><span class="n">оценок</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">каждого</span><span class="w"> </span><span class="n">класса</span><span class="w"> </span><span class="n">классификатор</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="n">даёт</span><span class="w"> </span><span class="n">чуть</span><span class="w"> </span><span class="n">более</span><span class="w"> </span><span class="n">понятный</span><span class="w"> </span><span class="n">результат</span><span class="w"> </span><span class="p">(</span><span class="n">нормализованные</span><span class="w"> </span><span class="n">вероятности</span><span class="w"> </span><span class="n">классов</span><span class="p">),</span><span class="w"> </span><span class="n">а</span><span class="w"> </span><span class="n">также</span><span class="w"> </span><span class="n">имеет</span><span class="w"> </span><span class="n">вероятностную</span><span class="w"> </span><span class="n">интерпретацию</span><span class="p">,</span><span class="w"> </span><span class="n">которую</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">вскоре</span><span class="w"> </span><span class="n">опишем</span><span class="p">.</span><span class="w"> </span><span class="n">В</span><span class="w"> </span><span class="n">классификаторе</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="n">функция</span><span class="p">,</span><span class="w"> </span><span class="n">отображающая</span><span class="w"> </span><span class="err">$</span><span class="n">f</span><span class="p">(</span><span class="n">x_i</span><span class="p">;</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="n">W</span><span class="w"> </span><span class="n">x_i</span><span class="err">$</span><span class="w"> </span><span class="n">остаётся</span><span class="w"> </span><span class="n">неизменным</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">теперь</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">интерпретируем</span><span class="w"> </span><span class="n">эти</span><span class="w"> </span><span class="n">оценки</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="n">ненормированные</span><span class="w"> </span><span class="n">логарифмические</span><span class="w"> </span><span class="n">вероятности</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">каждого</span><span class="w"> </span><span class="n">класса</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">заменяем</span><span class="w">  </span><span class="n">_потерю</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">перегиба_</span><span class="w"> </span><span class="n">__потерю</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">перекрёстной</span><span class="w"> </span><span class="n">энтропии__</span><span class="p">,</span><span class="w"> </span><span class="n">которая</span><span class="w"> </span><span class="n">имеет</span><span class="w"> </span><span class="nl">вид</span><span class="p">:</span><span class="w">  </span>

<span class="err">$$</span>
<span class="n">L_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="err">\</span><span class="nf">log</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}}{</span><span class="w"> </span><span class="err">\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}</span><span class="w"> </span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">hspace</span><span class="err">{</span><span class="mf">0.5</span><span class="ow">in</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nc">text</span><span class="err">{</span><span class="ow">or</span><span class="w"> </span><span class="n">equivalently</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">hspace</span><span class="err">{</span><span class="mf">0.5</span><span class="ow">in</span><span class="err">}</span><span class="w"> </span><span class="n">L_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="err">\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}</span>
<span class="err">$$</span><span class="w">  </span>

<span class="n">где</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">используем</span><span class="w"> </span><span class="n">обозначение</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">f_j</span><span class="err">$</span><span class="n">__</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">обозначения</span><span class="w"> </span><span class="n">j</span><span class="o">-</span><span class="n">го</span><span class="w"> </span><span class="n">элемента</span><span class="w"> </span><span class="n">вектора</span><span class="w"> </span><span class="n">оценок</span><span class="w"> </span><span class="n">класса</span><span class="w"> </span><span class="n">__f__</span><span class="p">.</span><span class="w"> </span><span class="n">Как</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">прежде</span><span class="p">,</span><span class="w"> </span><span class="n">полная</span><span class="w"> </span><span class="n">потеря</span><span class="w"> </span><span class="n">набора</span><span class="w"> </span><span class="n">данных</span><span class="w"> </span><span class="n">является</span><span class="w"> </span><span class="n">средним</span><span class="w"> </span><span class="n">значением</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">L_i</span><span class="err">$</span><span class="n">__</span>
<span class="w"> </span><span class="n">по</span><span class="w"> </span><span class="n">всем</span><span class="w"> </span><span class="n">обучающим</span><span class="w"> </span><span class="n">примерам</span><span class="w"> </span><span class="n">вместе</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">термином</span><span class="w"> </span><span class="n">регуляризации</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">R</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="err">$</span><span class="n">__</span><span class="p">.</span><span class="w"> </span><span class="n">Функция</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">f_j</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">z_j</span><span class="err">}}{\</span><span class="n">sum_k</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">z_k</span><span class="err">}}</span><span class="w"> </span><span class="err">$</span><span class="n">__</span><span class="w"> </span><span class="n">называется</span><span class="w"> </span><span class="n">__функцией</span><span class="w"> </span><span class="nl">softmax__</span><span class="p">:</span><span class="w"> </span><span class="n">она</span><span class="w"> </span><span class="n">принимает</span><span class="w"> </span><span class="n">вектор</span><span class="w"> </span><span class="n">произвольных</span><span class="w"> </span><span class="n">числовых</span><span class="w"> </span><span class="n">значений</span><span class="w">  </span><span class="p">(</span><span class="n">в</span><span class="w"> </span><span class="err">$</span><span class="n">z</span><span class="err">$</span><span class="p">)</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">преобразует</span><span class="w"> </span><span class="n">его</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">вектор</span><span class="w"> </span><span class="n">значений</span><span class="w"> </span><span class="n">от</span><span class="w"> </span><span class="n">нуля</span><span class="w"> </span><span class="n">до</span><span class="w"> </span><span class="n">единицы</span><span class="p">,</span><span class="w"> </span><span class="n">сумма</span><span class="w"> </span><span class="n">которых</span><span class="w"> </span><span class="n">равна</span><span class="w"> </span><span class="n">единице</span><span class="p">.</span><span class="w"> </span><span class="n">Полная</span><span class="w"> </span><span class="n">функция</span><span class="w"> </span><span class="n">потерь</span><span class="w"> </span><span class="n">перекрёстной</span><span class="w"> </span><span class="n">энтропии</span><span class="p">,</span><span class="w"> </span><span class="n">включающая</span><span class="w"> </span><span class="n">функцию</span><span class="w"> </span><span class="n">softmax</span><span class="p">,</span><span class="w"> </span><span class="n">может</span><span class="w"> </span><span class="n">показаться</span><span class="w"> </span><span class="n">пугающей</span><span class="p">,</span><span class="w"> </span><span class="n">если</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">видите</span><span class="w"> </span><span class="n">её</span><span class="w"> </span><span class="n">впервые</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">её</span><span class="w"> </span><span class="n">относительно</span><span class="w"> </span><span class="n">легко</span><span class="w"> </span><span class="n">объяснить</span><span class="p">.</span><span class="w">  </span>

<span class="w"> </span><span class="n">__С</span><span class="w"> </span><span class="n">точки</span><span class="w"> </span><span class="n">зрения</span><span class="w"> </span><span class="n">теории</span><span class="w"> </span><span class="n">информации__</span><span class="p">.</span><span class="w"> </span><span class="n">_Перекрёстная</span><span class="w"> </span><span class="n">энтропия_</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="err">«</span><span class="n">истинным</span><span class="err">»</span><span class="w"> </span><span class="n">распределением</span><span class="w"> </span><span class="n">p</span>
<span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">предполагаемое</span><span class="w"> </span><span class="n">распределение</span><span class="w"> </span><span class="n">__q__</span><span class="w"> </span><span class="n">определяется</span><span class="w"> </span><span class="nl">как</span><span class="p">:</span><span class="w">  </span>

<span class="err">$$</span>
<span class="n">H</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">\</span><span class="n">sum_x</span><span class="w"> </span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="err">$$</span><span class="w">  </span>

<span class="n">Таким</span><span class="w"> </span><span class="n">образом</span><span class="p">,</span><span class="w"> </span><span class="n">классификатор</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="n">минимизирует</span><span class="w"> </span><span class="n">перекрестную</span><span class="w"> </span><span class="n">энтропию</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">оцененными</span><span class="w"> </span><span class="n">вероятностями</span><span class="w"> </span><span class="n">классов</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="err">$</span><span class="n">q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}</span><span class="w">  </span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}</span><span class="w"> </span><span class="err">$</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="n">показано</span><span class="w"> </span><span class="n">выше</span><span class="p">)</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">_</span><span class="err">«</span><span class="n">истинное</span><span class="err">»</span><span class="n">_</span><span class="w"> </span><span class="n">распределение</span><span class="p">,</span><span class="w"> </span><span class="n">которое</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">этой</span><span class="w"> </span><span class="n">интерпретации</span><span class="w"> </span><span class="n">представляет</span><span class="w"> </span><span class="n">собой</span><span class="w"> </span><span class="n">распределение</span><span class="p">,</span><span class="w"> </span><span class="n">при</span><span class="w"> </span><span class="n">котором</span><span class="w"> </span><span class="n">вся</span><span class="w"> </span><span class="n">масса</span><span class="w"> </span><span class="n">вероятностей</span><span class="w"> </span><span class="n">приходится</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">правильный</span><span class="w"> </span><span class="n">класс</span><span class="w"> </span>
<span class="n">то</span><span class="w"> </span><span class="n">есть</span><span class="w"> </span><span class="err">$</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">0, \ldots 1, \ldots, 0</span><span class="o">]</span><span class="err">\\</span><span class="w"> </span><span class="n">содержит</span><span class="w"> </span><span class="n">единственную</span><span class="w"> </span><span class="n">единицу</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">y_i</span><span class="err">$</span><span class="n">__</span><span class="o">-</span><span class="n">й</span><span class="w"> </span><span class="n">позиции</span><span class="p">.).</span><span class="w"> </span><span class="n">Более</span><span class="w"> </span><span class="n">того</span><span class="p">,</span><span class="w"> </span><span class="n">поскольку</span><span class="w"> </span><span class="n">кросс</span><span class="o">-</span><span class="n">энтропию</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">записать</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">терминах</span><span class="w"> </span><span class="n">энтропии</span><span class="p">,</span><span class="w"> </span><span class="n">а</span><span class="w"> </span><span class="n">дивергенцию</span><span class="w"> </span><span class="n">Кульбака</span><span class="o">-</span><span class="n">Лейблера</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="err">$</span><span class="n">H</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">H</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">D_</span><span class="err">{</span><span class="n">KL</span><span class="err">}</span><span class="p">(</span><span class="n">p</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="o">|</span><span class="n">q</span><span class="p">)</span><span class="err">$</span><span class="p">,</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">энтропия</span><span class="w"> </span><span class="n">дельта</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">функции</span><span class="w"> </span><span class="n">__p__</span><span class="w"> </span><span class="n">равно</span><span class="w"> </span><span class="n">нулю</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">также</span><span class="w"> </span><span class="n">эквивалентно</span><span class="w"> </span><span class="n">минимизации</span><span class="w"> </span><span class="n">расхождения</span><span class="w"> </span><span class="n">Кульбака</span><span class="w"> </span><span class="err">—</span><span class="w"> </span><span class="n">Лейблера</span><span class="w"> </span><span class="n">между</span><span class="w"> </span><span class="n">двумя</span><span class="w"> </span><span class="n">распределениями</span><span class="w"> </span><span class="p">(</span><span class="n">мера</span><span class="w"> </span><span class="n">расстояния</span><span class="p">).</span><span class="w"> </span><span class="n">Другими</span><span class="w"> </span><span class="n">словами</span><span class="p">,</span><span class="w"> </span><span class="n">цель</span><span class="w"> </span><span class="n">кросс</span><span class="o">-</span><span class="n">энтропии</span><span class="w"> </span><span class="n">_заключается</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">том_</span><span class="p">,</span><span class="w"> </span><span class="n">чтобы</span><span class="w"> </span><span class="n">прогнозируемое</span><span class="w"> </span><span class="n">распределение</span><span class="w"> </span><span class="n">было</span><span class="w"> </span><span class="n">сосредоточено</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">правильном</span><span class="w"> </span><span class="n">ответе</span><span class="p">.</span><span class="w">  </span>

<span class="n">__Вероятностная</span><span class="w"> </span><span class="n">интерпретация__</span><span class="p">.</span><span class="w"> </span><span class="n">Глядя</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">выражение</span><span class="p">,</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">видим</span><span class="p">,</span><span class="w"> </span><span class="nl">что</span><span class="p">:</span><span class="w">  </span>

<span class="err">$$</span>
<span class="n">P</span><span class="p">(</span><span class="n">y_i</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">x_i</span><span class="p">;</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}}{\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}</span><span class="w"> </span><span class="err">}</span>
<span class="err">$$</span>

<span class="n">может</span><span class="w"> </span><span class="n">быть</span><span class="w"> </span><span class="n">интерпретирована</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="p">(</span><span class="n">нормализованная</span><span class="p">)</span><span class="w"> </span><span class="n">вероятность</span><span class="p">,</span><span class="w"> </span><span class="n">присвоенная</span><span class="w"> </span><span class="n">правильной</span><span class="w"> </span><span class="n">метке</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">y_i</span><span class="err">$</span><span class="n">__</span><span class="w"> </span><span class="n">учитывая</span><span class="w"> </span><span class="n">изображение</span><span class="w"> </span><span class="n">__</span><span class="err">$</span><span class="n">x_i</span><span class="err">$</span><span class="n">__</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">параметризуется</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">помощью</span><span class="w"> </span><span class="n">__W__</span><span class="p">.</span><span class="n">Чтобы</span><span class="w"> </span><span class="n">понять</span><span class="w"> </span><span class="n">это</span><span class="p">,</span><span class="w"> </span><span class="n">вспомните</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">классификатор</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="n">интерпретирует</span><span class="w"> </span><span class="n">оценки</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">выходном</span><span class="w"> </span><span class="n">векторе</span><span class="w"> </span><span class="n">__f__</span><span class="p">,</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="n">ненормированные</span><span class="w"> </span><span class="n">логарифмические</span><span class="w"> </span><span class="n">вероятности</span><span class="p">.</span><span class="w"> </span><span class="n">Возведение</span><span class="w"> </span><span class="n">этих</span><span class="w"> </span><span class="n">величин</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">степень</span><span class="w"> </span><span class="n">даёт</span><span class="w"> </span><span class="p">(</span><span class="n">ненормированные</span><span class="p">)</span><span class="w"> </span><span class="n">вероятности</span><span class="p">,</span><span class="w"> </span><span class="n">а</span><span class="w"> </span><span class="n">деление</span><span class="w"> </span><span class="n">выполняет</span><span class="w"> </span><span class="n">нормализацию</span><span class="p">,</span><span class="w"> </span><span class="n">чтобы</span><span class="w"> </span><span class="n">сумма</span><span class="w"> </span><span class="n">вероятностей</span><span class="w"> </span><span class="n">равнялась</span><span class="w"> </span><span class="n">единице</span><span class="p">.</span><span class="w"> </span><span class="n">Таким</span><span class="w"> </span><span class="n">образом</span><span class="p">,</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">вероятностной</span><span class="w"> </span><span class="n">интерпретации</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">минимизируем</span><span class="w"> </span><span class="n">отрицательную</span><span class="w"> </span><span class="n">логарифмическую</span><span class="w"> </span><span class="n">вероятность</span><span class="w"> </span><span class="n">правильного</span><span class="w"> </span><span class="n">класса</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">можно</span><span class="w"> </span><span class="n">интерпретировать</span><span class="w"> </span><span class="n">как</span><span class="w"> </span><span class="n">выполнение</span><span class="w"> </span><span class="n">__оценки</span><span class="w"> </span><span class="n">максимального</span><span class="w"> </span><span class="n">правдоподобия__</span><span class="w"> </span><span class="p">(</span><span class="n">MLE</span><span class="p">).</span><span class="w"> </span><span class="n">Преимущество</span><span class="w"> </span><span class="n">такого</span><span class="w"> </span><span class="n">подхода</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">том</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">теперь</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">можем</span><span class="w"> </span><span class="n">интерпретировать</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">член</span><span class="w"> </span><span class="n">регуляризации</span><span class="w"> </span><span class="n">__R</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="n">__</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">полной</span><span class="w"> </span><span class="n">функции</span><span class="w"> </span><span class="n">потерь</span><span class="p">,</span><span class="w"> </span><span class="n">исходящей</span><span class="w"> </span><span class="n">из</span><span class="w"> </span><span class="n">гауссовского</span><span class="w"> </span><span class="n">априора</span><span class="w"> </span><span class="n">по</span><span class="w"> </span><span class="n">весовой</span><span class="w"> </span><span class="n">матрице</span><span class="w"> </span><span class="n">__W__</span><span class="p">,</span><span class="w"> </span><span class="n">где</span><span class="w"> </span><span class="n">вместо</span><span class="w"> </span><span class="n">MLE</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">выполняем</span><span class="w"> </span><span class="n">оценку</span><span class="w"> </span><span class="n">_максимального</span><span class="w"> </span><span class="n">апостериорного</span><span class="w"> </span><span class="n">значения_</span><span class="w"> </span><span class="p">(</span><span class="k">MAP</span><span class="p">).</span><span class="w"> </span><span class="n">Мы</span><span class="w"> </span><span class="n">приводим</span><span class="w"> </span><span class="n">эти</span><span class="w"> </span><span class="n">интерпретации</span><span class="p">,</span><span class="w"> </span><span class="n">чтобы</span><span class="w"> </span><span class="n">помочь</span><span class="w"> </span><span class="n">вам</span><span class="w"> </span><span class="n">разобраться</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">подробное</span><span class="w"> </span><span class="n">описание</span><span class="w"> </span><span class="n">этого</span><span class="w"> </span><span class="n">вывода</span><span class="w"> </span><span class="n">выходит</span><span class="w"> </span><span class="n">за</span><span class="w"> </span><span class="n">рамки</span><span class="w"> </span><span class="n">данного</span><span class="w"> </span><span class="n">курса</span><span class="p">.</span><span class="w">  </span>

<span class="n">__Практические</span><span class="w"> </span><span class="nl">вопросы</span><span class="p">:</span><span class="w"> </span><span class="n">числовая</span><span class="w"> </span><span class="n">стабильность__</span><span class="p">.</span><span class="w"> </span><span class="n">Когда</span><span class="w"> </span><span class="n">вы</span><span class="w"> </span><span class="n">пишете</span><span class="w"> </span><span class="n">код</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">вычисления</span><span class="w"> </span><span class="n">функции</span><span class="w"> </span><span class="n">Softmax</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">практике</span><span class="p">,</span><span class="w"> </span><span class="n">промежуточные</span><span class="w"> </span><span class="n">значения</span><span class="err">$</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}$</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="err">$\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}$</span><span class="w"> </span><span class="n">может</span><span class="w"> </span><span class="n">быть</span><span class="w"> </span><span class="n">очень</span><span class="w"> </span><span class="n">большим</span><span class="w"> </span><span class="n">из</span><span class="o">-</span><span class="n">за</span><span class="w"> </span><span class="n">экспоненциальных</span><span class="w"> </span><span class="n">функций</span><span class="p">.</span><span class="w"> </span><span class="n">Деление</span><span class="w"> </span><span class="n">больших</span><span class="w"> </span><span class="n">чисел</span><span class="w"> </span><span class="n">может</span><span class="w"> </span><span class="n">быть</span><span class="w"> </span><span class="n">неустойчивым</span><span class="w"> </span><span class="n">с</span><span class="w"> </span><span class="n">точки</span><span class="w"> </span><span class="n">зрения</span><span class="w"> </span><span class="n">вычислений</span><span class="p">,</span><span class="w"> </span><span class="n">поэтому</span><span class="w"> </span><span class="n">важно</span><span class="w"> </span><span class="n">использовать</span><span class="w"> </span><span class="n">приём</span><span class="w"> </span><span class="n">нормализации</span><span class="p">.</span><span class="w"> </span><span class="n">Обратите</span><span class="w"> </span><span class="n">внимание</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">если</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">умножим</span><span class="w"> </span><span class="n">числитель</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">знаменатель</span><span class="w"> </span><span class="n">дроби</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">константу</span><span class="w"> </span><span class="n">__C__</span><span class="w"> </span><span class="n">и</span><span class="w"> </span><span class="n">подставим</span><span class="w"> </span><span class="n">его</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">сумму</span><span class="p">,</span><span class="w"> </span><span class="n">получим</span><span class="w"> </span><span class="n">следующее</span><span class="w"> </span><span class="p">(</span><span class="n">математически</span><span class="w"> </span><span class="n">эквивалентное</span><span class="p">)</span><span class="w"> </span><span class="nl">выражение</span><span class="p">:</span><span class="w">  </span>

<span class="err">$$</span>
<span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}}{\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}}</span>
<span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">Ce</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}}}{</span><span class="n">C</span><span class="err">\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="err">}}</span>
<span class="o">=</span><span class="w"> </span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_</span><span class="err">{</span><span class="n">y_i</span><span class="err">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">C</span><span class="err">}}{\</span><span class="n">sum_j</span><span class="w"> </span><span class="n">e</span><span class="o">^</span><span class="err">{</span><span class="n">f_j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="nf">log</span><span class="w"> </span><span class="n">C</span><span class="err">}}</span>
<span class="err">$$</span><span class="w">  </span>

<span class="n">Мы</span><span class="w"> </span><span class="n">вольны</span><span class="w"> </span><span class="n">выбирать</span><span class="w"> </span><span class="n">стоимость</span><span class="w"> </span><span class="n">__C__</span><span class="p">.</span><span class="w"> </span><span class="n">Это</span><span class="w"> </span><span class="n">не</span><span class="w"> </span><span class="n">повлияет</span><span class="w"> </span><span class="n">ни</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">один</span><span class="w"> </span><span class="n">из</span><span class="w"> </span><span class="n">результатов</span><span class="p">,</span><span class="w"> </span><span class="n">но</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">можем</span><span class="w"> </span><span class="n">использовать</span><span class="w"> </span><span class="n">это</span><span class="w"> </span><span class="n">значение</span><span class="w"> </span><span class="n">для</span><span class="w"> </span><span class="n">повышения</span><span class="w"> </span><span class="n">численной</span><span class="w"> </span><span class="n">стабильности</span><span class="w"> </span><span class="n">вычислений</span><span class="p">.</span><span class="w"> </span><span class="n">Обычно</span><span class="w"> </span><span class="n">выбирают</span><span class="w"> </span><span class="n">__C__</span><span class="w"> </span><span class="n">заключается</span><span class="w"> </span><span class="n">в</span><span class="w"> </span><span class="n">том</span><span class="p">,</span><span class="w"> </span><span class="n">чтобы</span><span class="w"> </span><span class="n">установить</span><span class="w"> </span><span class="n">__</span><span class="err">$\</span><span class="nf">log</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="err">\</span><span class="n">max_j</span><span class="w"> </span><span class="n">f_j</span><span class="w"> </span><span class="err">$</span><span class="n">__</span><span class="p">.</span><span class="w"> </span><span class="n">Это</span><span class="w"> </span><span class="n">просто</span><span class="w"> </span><span class="n">указывает</span><span class="w"> </span><span class="n">на</span><span class="w"> </span><span class="n">то</span><span class="p">,</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">мы</span><span class="w"> </span><span class="n">должны</span><span class="w"> </span><span class="n">сместить</span><span class="w"> </span><span class="n">значения</span><span class="w"> </span><span class="n">внутри</span><span class="w"> </span><span class="n">вектора</span><span class="w"> </span><span class="n">__f__</span><span class="w"> </span><span class="n">так</span><span class="w"> </span><span class="n">что</span><span class="w"> </span><span class="n">наибольшее</span><span class="w"> </span><span class="n">значение</span><span class="w"> </span><span class="n">равно</span><span class="w"> </span><span class="n">нулю</span><span class="p">.</span><span class="w"> </span><span class="n">В</span><span class="w"> </span><span class="nl">коде</span><span class="p">:</span><span class="w">  </span>


<span class="err">```</span><span class="n">py</span>
<span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="k">array</span><span class="p">(</span><span class="o">[</span><span class="n">123, 456, 789</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">classes</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">having</span><span class="w"> </span><span class="k">large</span><span class="w"> </span><span class="n">scores</span>
<span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">f</span><span class="p">))</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="nl">Bad</span><span class="p">:</span><span class="w"> </span><span class="nc">Numeric</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">potential</span><span class="w"> </span><span class="n">blowup</span>

<span class="err">#</span><span class="w"> </span><span class="nl">instead</span><span class="p">:</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">shift</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">values</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">highest</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="n">f</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="n">becomes</span><span class="w"> </span><span class="o">[</span><span class="n">-666, -333, 0</span><span class="o">]</span>
<span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">f</span><span class="p">))</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">safe</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">do</span><span class="p">,</span><span class="w"> </span><span class="n">gives</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">answer</span>
</pre></div>

<p><strong>Возможно, сбивающие с толку соглашения об именовании</strong>. Чтобы быть точным, в классификаторе SVM используется потеря шарнира, или также иногда называемая <em>потерей максимальной маржи</em>. <em>Классификатор Softmax</em> использует <em>кросс-энтропийные</em> потери. Классификатор Softmax получил свое название от <em>функции softmax</em>, которая используется для преобразования необработанных оценок класса в нормализованные положительные значения, которые в сумме равны единице, чтобы можно было применить потери от перекрестной энтропии. В частности, обратите внимание, что технически не имеет смысла говорить о «потере при softmax», поскольку softmax — это просто функция сжатия, но это относительно часто используемое сокращение.  </p>
<h3>SVM против Softmax</h3>
<p>Изображение может помочь прояснить разницу между классификаторами Softmax и SVM:  </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/svmvssoftmax.png"></p>
<p>Пример разницы между классификаторами SVM и Softmax для одной точки данных. В обоих случаях мы вычисляем один и тот же вектор оценок <strong>f</strong> (например, путём умножения матриц в этом разделе). Разница заключается в интерпретации оценок в <strong>f</strong>: SVM интерпретирует их как оценки классов, и его функция потерь поощряет правильный класс (класс 2, выделен синим цветом) к получению более высокой оценки, чем у других классов. Вместо этого классификатор Softmax интерпретирует баллы как (ненормализованные) логарифмические вероятности для каждого класса, а затем стремится к тому, чтобы (нормализованная) логарифмическая вероятность правильного класса была высокой (эквивалентно, чтобы её отрицательная величина была низкой). Окончательное значение потерь для этого примера составляет 1,58 для SVM и 1,04 (обратите внимание, что это 1,04 с использованием натурального логарифма, а не логарифма по основанию 2 или 10) для классификатора Softmax, но обратите внимание, что эти числа несопоставимы. Они имеют смысл только в сравнении с потерями, рассчитанными для того же классификатора и с теми же данными.  </p>
<hr>
<p><strong>Классификатор Softmax предоставляет «вероятности» для каждого класса</strong>. В отличие от SVM, который вычисляет некалиброванные и трудно интерпретируемые оценки для всех классов, классификатор Softmax позволяет вычислять «вероятности» для всех меток. Например, для изображения классификатор SVM может выдать оценки [12,5, 0,6, -23,0] для классов «кошка», «собака» и «корабль». Вместо этого классификатор softmax может вычислить вероятности трёх меток как [0,9, 0,09, 0,01], что позволяет интерпретировать его уверенность в каждом классе. Однако мы взяли слово «вероятности» в кавычки, потому что то, насколько выраженными или размытыми будут эти вероятности, напрямую зависит от силы регуляризации <strong>λ</strong>, которые вы вводите в систему в качестве входных данных. Например, предположим, что ненормированные логарифмические вероятности для трёх классов равны [1, -2, 0]. Тогда функция softmax вычислит:  </p>
<p>$$
[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]
$$  </p>
<p>Где шаги, предпринятые для возведения в степень и нормализации, суммируются до единицы. Теперь, если сила регуляризации <strong>λ</strong> был выше, вес <strong>W</strong> будет больше штрафоваться, и это приведёт к уменьшению весов. Например, предположим, что веса стали в два раза меньше ([0,5, -1, 0]). Теперь <em>softmax</em> будет вычислять:  </p>
<p>$$
[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]
$$  </p>
<p>где вероятности теперь более размыты. Более того, в пределе, когда веса стремятся к малым значениям из-за очень сильной регуляризации __λ__Выходные вероятности были бы почти равномерными. Следовательно, вероятности, вычисляемые классификатором Softmax, лучше рассматривать как степени уверенности, где, как и в случае с SVM, порядок значений интерпретируется, но абсолютные значения (или их разница) технически не интерпретируются.  </p>
<p>На практике SVM и Softmax обычно сопоставимы по эффективности. Разница в производительности между SVM и Softmax обычно очень мала, и разные люди по-разному оценивают, какой классификатор работает лучше. По сравнению с классификатором Softmax, SVM является более <em>локальной</em> целью, что можно рассматривать как недостаток или преимущество. Рассмотрим пример, в котором достигаются баллы [10, -2, 3] и где первый класс является правильным. SVM (например, с желаемым запасом прочности <strong>Δ=1</strong>) увидит, что правильный класс уже имеет оценку выше, чем разница между классами, и вычислит нулевую потерю. SVM не обращает внимания на детали отдельных оценок: если бы они были [10, -100, -100] или [10, 9, 9], SVM было бы всё равно, так как разница в 1 соблюдена и, следовательно, потеря равна нулю. Однако эти сценарии не эквивалентны классификатору Softmax, который привёл бы к гораздо более высоким потерям для оценок [10, 9, 9], чем для [10, -100, -100]. Другими словами, классификатор Softmax никогда не будет полностью удовлетворён полученными оценками: правильный класс всегда может иметь более высокую вероятность, а неправильные классы — более низкую, и потери всегда будут уменьшаться. Однако SVM доволен, если соблюдены границы, и не контролирует точные оценки за пределами этого ограничения. Это можно интуитивно воспринимать как особенность: например, классификатор автомобилей, который, скорее всего, тратит большую часть своих «усилий» на решение сложной задачи по отделению автомобилей от грузовиков, не должен подвергаться влиянию примеров с лягушками, которым он уже присваивает очень низкие баллы и которые, скорее всего, группируются в совершенно другой части облака данных.  </p>
<h3>Интерактивная веб-демонстрация</h3>
<hr>
<p><img alt="" src="http://vision.stanford.edu/teaching/cs231n/linear-classify-demo"></p>
<p>Мы написали интерактивную веб-демонстрацию, чтобы помочь вашей интуиции в работе с линейными классификаторами. Демонстрация визуализирует функции потерь, обсуждаемые в этом разделе, с использованием игрушечной трехмерной классификации на 2D-данных. Демонстрационная версия также немного забегает вперед и выполняет оптимизацию, которую мы подробно обсудим в следующем разделе.  </p>
<hr>
<h3>Краткая сводка</h3>
<p>Подводя итог: 
- Мы определили <strong>функцию оценки</strong> от пикселей изображения к оценкам классов (в этом разделе — линейную функцию, которая зависит от весовых коэффициентов <strong>W</strong> и смещений <strong>b</strong>).
- В отличие от классификатора kNN, преимущество этого <strong>параметрического подхода</strong> заключается в том, что после определения параметров мы можем отказаться от обучающих данных. Кроме того, прогнозирование для нового тестового изображения выполняется быстро, поскольку требует лишь одного умножения матрицы на <strong>W</strong>, а не исчерпывающего сравнения с каждым отдельным обучающим примером.
- Мы ввели <strong>уловку со смещением</strong>, которая позволяет нам сложить вектор смещения в весовую матрицу для удобства, чтобы отслеживать только одну матрицу параметров.
- Мы определили <strong>функцию потерь</strong> (мы ввели две часто используемые функции потерь для линейных классификаторов: <strong>SVM</strong> и <strong>Softmax</strong>), которая измеряет, насколько заданный набор параметров соответствует истинным меткам в обучающем наборе данных. Мы также увидели, что функция потерь определена таким образом, что хорошие прогнозы на обучающих данных эквивалентны небольшим потерям.</p>
<p>Теперь мы увидели один из способов взять набор изображений и сопоставить каждое из них с баллами классов на основе набора параметров, а также увидели два примера функций потерь, которые можно использовать для оценки качества прогнозов. Но как эффективно определить параметры, которые дают наилучшие (наименьшие) потери? Этот процесс называется <strong>оптимизацией</strong>, и ему посвящён следующий раздел.  </p>
<h3>Дополнительные материалы</h3>
<p>Эти показания являются необязательными и содержат указания, представляющие интерес: 
- <a href="https://arxiv.org/abs/1306.0239">«Глубокое обучение с использованием линейных машин опорных векторов»</a> от Чарли Танга, 2013 г., представляет некоторые результаты, согласно которым L2SVM превосходит Softmax.</p>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../transfer-learning/" rel="prev" title="Трансфер обучения">Предыдущая запись</a>
            </li>
            <li class="next">
                <a href="../optimization/" rel="next" title="Оптимизация">Следующая запись</a>
            </li>
        </ul></nav></aside><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\\\[", right: "\\\\]", display: true},
    {left: "\\\\begin{equation*}", right: "\\\\end{equation*}", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\\\(", right: "\\\\)", display: false}
]

                    }
                );
            </script></article><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:andrej.labintsev@yandex.ru">Андрей Лабинцев</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
