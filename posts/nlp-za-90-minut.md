<!--
.. title: NLP за 90 минут
.. slug: nlp-za-90-minut
.. date: 2023-06-16 11:44:44 UTC+03:00
.. tags: NLP
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: true
-->

# Основы обработки естественного языка 

### Вопросы:
1. Краткая история машинной обработки текстов (5 мин)
2. Основные определения (5 мин)
3. Методы предварительной обработки текста (10 мин)
4. Моделирование языка (языковые модели) (10 мин)
5. Нейронные сети в NLP (30 мин)
6. GPT модели (30 мин)

### 1. Краткая история машинной обработки текстов

**NLP (Natural Language Processing)** - это область науки, которая изучает методы обработки текстов на естественных языках с помощью вычислительных машин. 
Основной акцент в NLP делается на прикладные методы, которые можно реализовать на языке программирования. 
Для вычислительно сложных методов используют языки низкого уровня (С, С++), потому что важна эффективность вычислений. 
Для проведение экспериментов используют языки высокого уровня (python), потому что  для проверки гипотез важна скорость написания кода. 
Сегодня исследователям доступно множество библиотек на python, которые служат оберткой для оптимизированного машинного кода.  

В 1913 году русский математик Андрей Андреевич Марков провел эксперимент по оценке частоты появления разных букв в тексте. 
Он выписал первые 20 000 букв поэмы А. С. Пушкина «Евгений Онегин» в одну длинную строчку из букв, опустив все пробелы и знаки пунктуации. 
Затем он переставил эти буквы в 200 решёток (по 10х10 символов в каждой), и начал подсчитывать гласные звуки в каждой строке и столбце, записывая результаты. 
Марков считал, что большинство явлений происходит по цепочке причинно-следственной связи и зависит от предыдущих результатов. 
Он хотел найти способ моделировать эти события посредством вероятностного анализа. 
Он обнаружил, что для любой буквы текста Пушкина выполнялось правило: если это была гласная, то скорее всего за ней будет стоять согласная, и наоборот.  

В 1950 году в работе "Вычислительные машины и разум" ученый Алан Тьюринг предложил тест разумности для искуственного интеллекта.  
Если компьютер сможет провести убедительную беседу с человеком в текстовом режиме, можно будет предположить, что он разумен. 

В 1954 году в штаб-квартире корпорации IBM состоялся Джорджтаунский эксперимент — демонстрация возможностей машинного перевода. 
В ходе эксперимента был продемонстрирован полностью автоматический перевод более 60 предложений с русского языка на английский. 
Программа выполнялась на мейнфрейме IBM 701. 
В том же году первый эксперимент по машинному переводу был произведён в Институте точной механики и вычислительной техники АН СССР, на компьютере БЭСМ.  

В 1966 году Джозеф Вейценбаум, работавший в лаборатории ИИ при MIT, разработал первый в мире чатбот "Элиза". 
Пользователь мог ввести некое утверждение или набор утверждений на обычном языке, нажать «ввод», и получить от машины ответ.  

В 1986 Давид Румельхарт разработал базовую концепцию рекуррентной нейросети (recurrent neural network, RNN). 
Этот метод позволял решать такие задачи как распознавание речи и текста.  

В 2017 году группа исследователей Google представила архитектуру трансформера (Transformer), которая позволяет обрабатывать тексты, в которых слова расположены в произвольном порядке. 
В настоящее время трансформеры используются в сервисах многих компаний, включая Яндекс и Google, являются основой для самых современных моделей GPT, Bert и т.д.  

В 2023 году OpenAI опубликовала языковую модель GPT-4, которая легко проходит тест Тьюринга и порождает споры об опасности искусственного интеллекта. 

### 2. Основные определения

**Символ** - это условный знак каких-либо понятий, идей, явлений. 
Первые наскальные символы обозначали зверей, охоту, солнце и другие предметы, которые отражались в сознании первобытных людей. 
Мы и сегодня часто используем символы эмоджи для отображения своих эмоций и скрытых смыслов. 
У разных народов сформировались свои уникальные алфавиты, которы представляют собой множества допустимых символов для письма.   

У компьютера тоже есть уникальный набор символов, определяемый кодировкой. 
Например, кодировка ASCII (American standard code for information interchange) была стандартизована в 1963 году и определяет символы:  
- десятичных цифр;  
- латинского алфавита;  
- национального алфавита;  
- знаков препинания;  
- управляющих символов.  

С математической точки зрения алфавит как множество символов обозначается символом $V$.  
Множество $V^*$ включает в себя всевозможные комбинации символов, образующих конечные слова, в т.ч. состоящие из одного символа и бессмысленные комбинации. 

**Слово** - наименьшая единица языка, служащая для именования предметов, качеств, характеристик, взаимодействий, а также для служебных целей. 
Например слово "Я" состоит всего из одного символа и в русском языке обозначает меня как субъект.  
Большинство слов состоят из нескольких символов, связанных в последовательность по определенным правилам. 

**Язык** - это множество слов, которые несут в себе хоть какой-то смысл. 
Например, слово "тывщштс" не имеет смысла в русском языке, хоть и состоит из символов кириллицы. 
А слово "дом", наоборот, является вполне известным и входит в множество слов русского языка.   
Формально язык обозначается символом $L$. В алфавите $V$ язык является подмножеством всех конечных слов $$L \in V^*$$

**Текст** - это зафиксированая на материальном носителе человеческая мысль в виде последовательности символов. 
Текст может состоять из одного или нескольких слов. 


