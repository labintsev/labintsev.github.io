<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="ru">
<head>
<meta charset="utf-8">
<meta name="description" content="Введение в классификацию изображений, классификатор ближайшего соседа, классификатор k - ближайших соседей, наборы данных для настройки гиперпараметров. ">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Классификация изображений  | Заметки по ML, DL</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="ru" href="../../rss.xml">
<link rel="canonical" href="https://mldl.ru/posts/image-classification/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Андрей Лабинцев">
<link rel="prev" href="../visualisation/" title="Понимание и визуализация" type="text/html">
<meta property="og:site_name" content="Заметки по ML, DL">
<meta property="og:title" content="Классификация изображений ">
<meta property="og:url" content="https://mldl.ru/posts/image-classification/">
<meta property="og:description" content="Введение в классификацию изображений, классификатор ближайшего соседа, классификатор k - ближайших соседей, наборы данных для настройки гиперпараметров. ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-03-14T08:00:00+03:00">
<meta property="article:tag" content="CV">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Перейти к главному содержимому</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">Заметки по ML, DL</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Источник</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Классификация изображений </a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Андрей Лабинцев
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2025-03-14T08:00:00+03:00" itemprop="datePublished" title="2025-03-14 08:00">2025-03-14 08:00</time></a>
            </p>
            
        <p class="sourceline"><a href="index.md" class="sourcelink">Источник</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2>Классификация изображений</h2>
<p>В этой лекции мы познакомимся с проблемой классификации изображений. 
Решение проблемы заключается в подходе, основанном на большом объеме размеченных данных.   </p>
<p>Содержание:<br>
<<<<<<< HEAD
- <a href="#%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8E-%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9">Введение в классификацию изображений</a><br>
  - <a href="#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B5%D0%B3%D0%BE-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B0">Классификатор ближайшего соседа</a><br>
  - <a href="#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-k---%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9">Классификатор k - ближайших соседей</a> <br>
  - <a href="#%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D1%8B-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B8-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B8-%D0%B3%D0%B8%D0%BF%D0%B5%D1%80%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2">Наборы данных для настройки гиперпараметров</a><br>
  - <a href="#%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-knn-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B5">Применение kNN на практике</a><br>
    - <a href="#%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%82%D0%B5%D1%80%D0%B8%D0%B0%D0%BB%D1%8B">Дополнительные материалы</a> </p>
<h3>Введение в классификацию изображений</h3>
<p><strong>Мотивация</strong>. В этом разделе мы рассмотрим задачу классификации изображений. Задача заключается в присвоении входному изображению одной метки из фиксированного набора категорий. Это одна из основных задач компьютерного зрения, которая, несмотря на свою простоту, имеет множество практических применений. Более того, многие другие задачи компьютерного зрения (детекция объектов, сегментация) могут быть сведены к классификации изображений.   </p>
<p><strong>Пример</strong>. Например, на изображении ниже модель классификации изображений принимает одно изображение и присваивает вероятности четырём меткам: <em>{«кошка», «собака», «шляпа», «кружка»}</em>. Как показано на изображении, для компьютера изображение представляет собой один большой трёхмерный массив чисел. В этом примере изображение кошки имеет ширину <strong>248</strong> пикселей, высоту <strong>400</strong> пикселей и три цветовых канала: красный, зелёный, синий (или сокращённо <em>RGB</em>). Таким образом, изображение состоит из <strong>248 x 400 x 3</strong> чисел, или в общей сложности 297 600 чисел. Каждое число представляет собой целое число от 0 (чёрный) до 255 (белый). Наша задача — превратить эти четверть миллиона чисел в одну метку, например <em>«кошка»</em>.  </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/classify.png"></p>
<p>Задача классификации изображений состоит в том, чтобы предсказать одну метку для заданного изображения. Так же мы можем предсказать распределение вероятностей для всех меток, что отражает степень нашей уверенности в результате классификации.   Изображения представляют собой трёхмерные массивы целых чисел от 0 до 255 размером «ширина x высота x 3». Число 3 обозначает три цветовых канала: красный, зелёный и синий.  </p>
<hr>
<p><strong>Проблемы</strong>.  Поскольку задача распознавания визуального образа (например, кошки) относительно проста для человека, стоит рассмотреть связанные с ней проблемы с точки зрения алгоритма компьютерного зрения.<br>
=======
1) <a href="#%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8E-%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9">Введение в классификацию изображений</a><br>
2) <a href="#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B5%D0%B3%D0%BE-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B0">Классификатор ближайшего соседа</a><br>
3) <a href="#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-k---%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9">Классификатор k - ближайших соседей</a> <br>
4) <a href="#%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D1%8B-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B8-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B8-%D0%B3%D0%B8%D0%BF%D0%B5%D1%80%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2">Наборы данных для настройки гиперпараметров</a><br>
5) <a href="#%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-knn-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B5">Применение kNN на практике</a><br>
6) <a href="#%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%82%D0%B5%D1%80%D0%B8%D0%B0%D0%BB%D1%8B">Дополнительные материалы</a> </p>
<h3>Введение в классификацию изображений</h3>
<p><strong>Мотивация</strong>. 
В этом разделе мы рассмотрим задачу классификации изображений. 
Задача заключается в присвоении входному изображению одной метки из фиксированного набора категорий. 
Это одна из основных задач компьютерного зрения, которая, несмотря на свою простоту, имеет множество практических применений. 
Более того, многие другие задачи компьютерного зрения (детекция объектов, сегментация) могут быть сведены к классификации изображений.   </p>
<p><strong>Пример</strong>. 
Например, на изображении ниже модель классификации изображений принимает одно изображение и присваивает вероятности четырём меткам: <em>{«кошка», «собака», «шляпа», «кружка»}</em>. 
Как показано на изображении, для компьютера изображение представляет собой один большой трёхмерный массив чисел. 
В этом примере изображение кошки имеет ширину <strong>248</strong> пикселей, высоту <strong>400</strong> пикселей и три цветовых канала: красный, зелёный, синий (или сокращённо <em>RGB</em>). 
Таким образом, изображение состоит из <strong>248 x 400 x 3</strong> чисел, или в общей сложности 297 600 чисел. 
Каждое число представляет собой целое число от 0 (чёрный) до 255 (белый). 
Наша задача — превратить эти четверть миллиона чисел в одну метку, например <em>«кошка»</em>.  </p>
<p><img alt="" src="https://cs231n.github.io/assets/classify.png"></p>
<p>Задача классификации изображений состоит в том, чтобы предсказать одну метку для заданного изображения. 
Так же мы можем предсказать распределение вероятностей для всех меток, что отражает степень нашей уверенности в результате классификации.   Изображения представляют собой трёхмерные массивы целых чисел от 0 до 255 размером «ширина x высота x 3». 
Число 3 обозначает три цветовых канала: красный, зелёный и синий.  </p>
<p><strong>Проблемы</strong>.<br>
Поскольку задача распознавания визуального образа (например, кошки) относительно проста для человека, стоит рассмотреть связанные с ней проблемы с точки зрения алгоритма компьютерного зрения.<br>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
Ниже мы приводим (неполный) список проблем, не забывая о том, что изображения представлены в виде трёхмерного массива значений яркости:<br>
- <strong>Изменение точки обзора</strong>. Один экземпляр объекта может быть ориентирован по-разному относительно камеры.<br>
- <strong>Изменение масштаба</strong>. Визуальные классы часто различаются по размеру (размеру в реальном мире, а не только по размеру на изображении).<br>
- <strong>Деформация</strong>. Многие интересующие нас объекты не являются твёрдыми телами и могут сильно деформироваться.<br>
- <strong>Окклюзия</strong>. Интересующие нас объекты могут быть частично скрыты. Иногда видна лишь небольшая часть объекта (всего несколько пикселей).<br>
- <strong>Условия освещения</strong>. Влияние освещения на пиксели очень велико.<br>
- <strong>Фоновый шум</strong>. Интересующие нас объекты могут сливаться с окружающей средой, что затрудняет их идентификацию.<br>
- <strong>Внутриклассовые различия</strong>. Классы, представляющие интерес, часто могут быть относительно обширными, например, <em>стулья</em>. 
Существует множество различных типов этих предметов, каждый из которых имеет отличный от других элементов класса внешний вид.    </p>
<p>Хорошая модель классификации изображений должна быть инвариантна к перекрёстному произведению всех этих вариаций, сохраняя при этом чувствительность к межклассовым вариациям.   </p>
<p><img alt="" src="https://cs231n.github.io/assets/challenges.jpeg"></p>
<<<<<<< HEAD
<hr>
<p><strong>Подход, основанный на данных</strong>. Как бы мы могли написать алгоритм, который сможет классифицировать изображения по отдельным категориям? В отличие от написания алгоритма, например, для сортировки списка чисел, не очевидно, как можно написать алгоритм для распознавания кошек на изображениях. Поэтому вместо того, чтобы пытаться описать каждую из интересующих нас категорий непосредственно в коде, мы воспользуемся подходом, похожим на обучение ребёнка. Мы предоставим компьютеру множество примеров, а затем используем алгоритм обучения, который связывает визуальное представление с меткой каждого класса. Этот подход предполагает, что у нас есть обучающий набор с размеченными изображениями. Вот пример того, как может выглядеть такой набор данных:   </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/trainset.jpg"></p>
<p>Пример обучающего набора для четырёх визуальных категорий. 
На практике у нас могут быть тысячи категорий и сотни тысяч изображений для каждой категории.  </p>
<hr>
<p><strong>Конвейер классификации изображений</strong>.  Мы увидели, что задача классификации изображений состоит в том, чтобы взять массив пикселей изображения и присвоить ему метку. 
=======
<p><strong>Подход, основанный на данных</strong>.<br>
Как бы мы могли написать алгоритм, который сможет классифицировать изображения по отдельным категориям? 
В отличие от написания алгоритма, например, для сортировки списка чисел, не очевидно, как можно написать алгоритм для распознавания кошек на изображениях. 
Поэтому вместо того, чтобы пытаться описать каждую из интересующих нас категорий непосредственно в коде, мы воспользуемся подходом, похожим на обучение ребёнка. 
Мы предоставим компьютеру множество примеров, а затем используем алгоритм обучения, который связывает визуальное представление с меткой каждого класса. 
Этот подход предполагает, что у нас есть обучающий набор с размеченными изображениями. 
Вот пример того, как может выглядеть такой набор данных:   </p>
<p><img alt="" src="https://cs231n.github.io/assets/trainset.jpg"></p>
<p>Пример обучающего набора для четырёх визуальных категорий. 
На практике у нас могут быть тысячи категорий и сотни тысяч изображений для каждой категории.  </p>
<p><strong>Конвейер классификации изображений</strong>.<br>
Мы увидели, что задача классификации изображений состоит в том, чтобы взять массив пикселей изображения и присвоить ему метку. 
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
Наш полный конвейер можно формализовать следующим образом:<br>
- <strong>Входные данные</strong>: состоят из набора <strong><em>N</em></strong> изображений, каждое из которых помечено одним из <strong><em>K</em></strong> различных классов. 
Эти данные называются <em>обучающей выборкой</em>.<br>
- <strong>Обучение</strong>: наша задача использовать обучающую выборку, чтобы узнать, как выглядит каждый из классов. 
Мы называем этот этап <em>обучением классификатора</em> или <em>обучением модели</em>.<br>
- <strong>Оценка</strong>: в конце мы оцениваем качество классификатора. 
Для этого нужно задать вопрос о том, какие метки предскажет классификатор для нового набора изображений, которые он никогда раньше не видел. 
Затем мы сравниваем истинные метки этих изображений с теми, которые предсказал классификатор. 
Интуитивно мы надеемся, что многие прогнозы совпадут с истинными ответами. 
Данные, которые используются для оценки точности классификатора называются <em>тестовой выборкой</em>. </p>
<<<<<<< HEAD
<h4>Классификатор ближайшего соседа</h4>
=======
<h3>Классификатор ближайшего соседа</h3>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>В качестве первого подхода мы используем так называемый <strong>классификатор ближайшего соседа</strong>. 
Этот классификатор не имеет ничего общего со свёрточными нейронными сетями и очень редко используется на практике. 
Однако он позволит нам получить представление о том, как решается задача классификации изображений.   </p>
<p><strong>Пример набора данных для классификации изображений: CIFAR-10</strong>.<br>
<<<<<<< HEAD
Одним из популярных наборов данных для классификации изображений является <a href="https://www.cs.toronto.edu/~kriz/cifar.html">набор данных CIFAR-10</a>. Этот набор данных состоит из 60 000 крошечных изображений высотой и шириной 32 пикселя. Каждое изображение относится к одному из 10 классов: самолет, автомобиль, птица и т. д. Эти 60 000 изображений разделены на обучающую выборку из 50 000 изображений и тестовую выборку из 10 000 изображений. 
На изображении ниже вы можете увидеть 10 случайных примеров изображений для каждого класса.    </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/nn.jpg"></p>
<p>Слева: примеры изображений из набора данных CIFAR-10.<br>
Справа: в первом столбце показаны несколько тестовых изображений.<br>
Рядом с каждым изображением мы видим 10 наиболее похожих "картинок" из обучающей выборки. </p>
<hr>
<p>Изначально, у нас есть обучающая выборка CIFAR-10 из 50 000 изображений (по 5000 изображений для каждой из 10 категорий). 
Мы хотим классифицировать оставшиеся 10 000. Классификатор ближайших соседей работает следующим образом. Берется тестовое изображение и сравается с каждым изображением из обучающей выборки. Будем считать, что метка тестового изображения будет такой же, как и у самого похожего на него изображения.  </p>
<p>На изображении выше и справа вы можете увидеть пример результата такой процедуры для 10 тестовых изображений. Обратите внимание, что только в 3 из 10 изображений являются элементами того же класса, в то время как в остальных 7 примерах возникает ошибка определения класса. Например, в 8-м ряду ближайшим обучающим изображением к голове лошади является красный автомобиль, предположительно из-за сильного чёрного фона. В результате этого, изображение лошади в данном случае будет ошибочно помечено как автомобиль. </p>
=======
Одним из популярных наборов данных для классификации изображений является <a href="https://www.cs.toronto.edu/~kriz/cifar.html">набор данных CIFAR-10</a>. 
Этот набор данных состоит из 60 000 крошечных изображений высотой и шириной 32 пикселя. 
Каждое изображение относится к одному из 10 классов: самолет, автомобиль, птица и т. д. 
Эти 60 000 изображений разделены на обучающую выборку из 50 000 изображений и тестовую выборку из 10 000 изображений. 
На изображении ниже вы можете увидеть 10 случайных примеров изображений для каждого класса.    </p>
<p><img alt="" src="https://cs231n.github.io/assets/nn.jpg"></p>
<p>Слева: примеры изображений из набора данных CIFAR-10.<br>
Справа: в первом столбце показаны несколько тестовых изображений.  </p>
<p>Рядом с каждым изображением изображены 10 наиболее похожих изображений из обучающей выборки. </p>
<p>Изначально, у нас есть обучающая выборка CIFAR-10 из 50 000 изображений (по 5000 изображений для каждой из 10 категорий). 
Мы хотим классифицировать оставшиеся 10 000 изображений. 
Классификатор ближайших соседей работает следующим образом. 
Берется тестовое изображение и сравается с каждым изображением из обучающей выборки. 
Будем считать, что метка тестового изображения будет такой же, как и у самого похожего на него изображения.  </p>
<p>На изображении выше и справа вы можете увидеть пример результата такой процедуры для 10 тестовых изображений. 
Обратите внимание, что только в 3 из 10 изображений являются элементами того же класса, в то время как в остальных 7 примерах возникает ошибка определения класса. 
Например, в 8-м ряду ближайшим обучающим изображением к голове лошади является красный автомобиль, предположительно из-за сильного чёрного фона. 
В результате этого, изображение лошади в данном случае будет ошибочно помечено как автомобиль. </p>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>Мы не уточнили, как именно мы сравниваем два изображения. 
Технически изображения представляют собой просто два блока (тензора) размером 32 x 32 x 3. 
Один из самых простых способов — сравнивать изображения попиксельно и суммировать все разности. 
Другими словами, если у вас есть два изображения, представленные в виде векторов $I_1$, $I_2$, разумным выбором для их сравнения может быть <strong>расстояние L1</strong>:   </p>
<p>$$
d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|
$$   </p>
<p>Сумма берется по всем пикселям. 
Вот как выглядит эта процедура:   </p>
<<<<<<< HEAD
<hr>
=======
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p><img alt="" src="https://cs231n.github.io/assets/nneg.jpeg"></p>
<p>Пример использования попиксельных различий для сравнения двух изображений с помощью расстояния $L_1$ (в данном примере для одного цветового канала). 
Два изображения вычитаются поэлементно, а затем все различия суммируются до получения одного числа. 
Если два изображения идентичны, результат будет равен нулю. 
Но если изображения сильно отличаются, результат будет большим.   </p>
<<<<<<< HEAD
<hr>
=======
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>Давайте также посмотрим, как можно реализовать классификатор в коде. Сначала загрузим данные CIFAR-10 в память в виде четырех массивов: обучающие данные/метки и тестовые данные/метки. В приведенном ниже коде <code>Xtr</code> хранятся все изображения из обучающей выборки  (объем 50 000 x 32 x 32 x 3), а соответствующий одномерный массив <code>Ytr</code> (длиной 50 000) содержит обучающие метки (от 0 до 9):   </p>
<div class="code"><pre class="code literal-block"><span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">load_CIFAR10</span><span class="p">(</span><span class="s1">'data/cifar10/'</span><span class="p">)</span> <span class="c1"># a magic function we provide</span>
<span class="c1"># flatten out all images to be one-dimensional </span>
<span class="n">Xtr_rows</span> <span class="o">=</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># Xtr_rows becomes 50000 x 3072</span>
<span class="n">Xte_rows</span> <span class="o">=</span> <span class="n">Xte</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xte</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># Xte_rows becomes 10000 x 3072 </span>
</pre></div>

<p>Теперь, когда все изображения вытянуты в ряд, мы можем обучить и оценить классификатор:   </p>
<div class="code"><pre class="code literal-block"><span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span> <span class="c1"># create a Nearest Neighbor classifier class</span>
<span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span> <span class="c1"># train the classifier on the training images and labels</span>
<span class="n">Yte_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte_rows</span><span class="p">)</span> <span class="c1"># predict labels on the test images</span>
<span class="c1"># and now print the classification accuracy, which is the average number</span>
<span class="c1"># of examples that are correctly predicted (i.e. label matches)</span>
<span class="nb">print</span> <span class="s1">'accuracy: </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yte_predict</span> <span class="o">==</span> <span class="n">Yte</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

<p>Обратите внимание, что в качестве критерия оценки обычно используется метрика <strong>accuracy</strong>, 
Эта метрика измеряет долю правильных прогнозов в тестовой выборке. 
Обратите внимание, что все классификаторы, которые мы создадим, имеют общий интерфейс (API). 
У них есть метод <code>train(X,y)</code>, который принимает на вход данные и метки для обучения. 
Внутри класса должна быть построена своего рода модель, которая предсказывает метки на основе данных. 
Метод <code>predict(X)</code> принимает новые данные и предсказывает метки. </p>
<p>Пример реализации простого классификатора ближайшего соседа с расстоянием $L_1$, который реализует интерфейс классификатора:   </p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NearestNeighbor</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""" X is N x D where each row is an example. Y is 1-dimension of size N</span>
<span class="sd">        The nearest neighbor classifier simply remembers all the training data """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span> <span class="o">=</span> <span class="n">y</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""" X is N x D where each row is an example we wish to predict label for """</span>
        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># lets make sure that the output type matches the input type</span>
        <span class="n">Ypred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># loop over all test rows</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
        <span class="c1"># find the nearest training image to the i'th test image</span>
        <span class="c1"># using the L1 distance (sum of absolute value differences)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">min_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span> <span class="c1"># get the index with smallest distance</span>
        <span class="n">Ypred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="p">[</span><span class="n">min_index</span><span class="p">]</span> <span class="c1"># predict the label of the nearest example</span>

        <span class="k">return</span> <span class="n">Ypred</span>
</pre></div>

<p>Если вы запустите этот код, то увидите, что классификатор достигает точности <strong>38,6%</strong> на тестовой выборке CIFAR-10. 
Это более впечатляющий результат, чем случайное угадывание (которое дало бы <strong>10%</strong> точности для 10 классов). 
Но он далёк от результатов человека, которые <a href="https://karpathy.github.io/2011/04/27/manually-classifying-cifar10/">оцениваются примерно в 94%</a>.  Еще лучший результат можно получить с помощью свёрточных нейронных сетей, которые достигают примерно 95% (см. таблицу соревнования <a href="https://www.kaggle.com/c/cifar-10/leaderboard">Kaggle</a> по CIFAR-10).  </p>
<<<<<<< HEAD
<p><strong>Выбор расстояния</strong>.  Существует множество других способов вычисления расстояний между векторами. Одним из распространённых вариантов может быть использование <strong>расстояния $L_2$</strong>, которое имеет геометрическую интерпретацию вычисления евклидова расстояния между двумя векторами. 
=======
<p><strong>Выбор расстояния</strong>.<br>
Существует множество других способов вычисления расстояний между векторами. 
Одним из распространённых вариантов может быть использование <strong>расстояния L_2</strong>, которое имеет геометрическую интерпретацию вычисления евклидова расстояния между двумя векторами. 
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
Формула для вычисления этого расстояния имеет вид:  </p>
<p>$$
d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}
$$  </p>
<p>Другими словами, мы вычисляем разницу по пикселям, как и раньше, но на этот раз возводим их в квадрат, складываем и, наконец, извлекаем квадратный корень. 
Используя приведенный выше код с numpy, нам нужно заменить только одну строку:   </p>
<div class="code"><pre class="code literal-block"><span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

<p>Обратите внимание на вычисление корня в функции <code>np.sqrt</code>. 
В практической реализации метода ближайшего соседа мы могли бы не использовать операцию извлечения квадратного корня, потому что он является <em>монотонной функцией</em>. 
То есть он масштабирует абсолютные значения расстояний, но сохраняет порядок. 
Поэтому с ним или без него, ближайшие соседи будут идентичны. 
<<<<<<< HEAD
Однако если применить классификатор ближайшего соседа к CIFAR-10 с $L_2$ расстоянием, получится всего <strong>35,4%</strong> точности. 
Это немного ниже, чем результат с расстоянием <strong>$L_1$</strong>.   </p>
<p><strong>Расстояние $L_1$ против $L_2$ .</strong>  Различие между этими двумя метриками в том, что расстояние <strong>$L_2$</strong> более строгое, чем расстояние <strong>$L_1$</strong>. Это значит, что если имеется множество небольших расхождений и всего одно большое, расстояние $L_2$ будет больше, чем $L_1$. Понятия расстояний <strong>$L_1$</strong> и  <strong>$L_2$</strong>  эквивалентно нормам, которые являются частными случаями <a href="https://planetmath.org/vectorpnorm">p-нормы</a>.   </p>
<h4>Классификатор k - ближайших соседей</h4>
<p>Когда мы хотим сделать более точный прогноз, нам не обязательно использовать только одну метку ближайшего изображения. Действительно, почти всегда можно добиться лучшего результата, используя так называемый <strong>классификатор k-ближайших соседей</strong>. 
Идея очень проста: вместо того, чтобы искать одно ближайшее изображение в обучающем наборе, мы найдём <strong>k</strong> ближайших изображений. 
=======
Однако если применить классификатор ближайшего соседа к CIFAR-10 с L2 расстоянием, получится всего <strong>35,4%</strong> точности. 
Это немного ниже, чем результат с расстоянием <strong>$L_1$</strong>.   </p>
<p><strong>Расстояние $L_1$ против $L_2$ .</strong><br>
Различие между этими двумя метриками в том, что расстояние <strong>$L_2$</strong> более строгое, чем расстояние <strong>$L_1$</strong>. 
Это значит, что если имеется множество небольших расхождений и всего одно большое, расстояние $L_2$ будет больше, чем $L_1$ . 
Понятия расстояний <strong>$L_1$</strong> и  <strong>$L_2$</strong>  эквивалентно нормам, которые являются частными случаями <a href="https://planetmath.org/vectorpnorm">p-нормы</a>.   </p>
<h3>Классификатор k - ближайших соседей</h3>
<p>когда мы хотим сделать более точный прогноз, нам не обязательно использовать только одну метку ближайшего изображения. 
Действительно, почти всегда можно добиться лучшего результата, используя так называемый <strong>классификатор k-ближайших соседей</strong>. 
Идея очень проста: вместо того, чтобы искать одно ближайшее изображение в обучающем наборе, мы найдём <strong>k</strong> ближайших изображений 
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
Дальше мы сравним их и устроим "голосование" за метку тестового изображения. 
В частности, когда <em>k = 1</em>, мы получаем классификатор ближайшего соседа. 
Интуитивно понятно, что чем больше значений <strong>k</strong> мы возьмем, тем больше будет сглаживающий эффект. 
Это первый пример гиперпараметра, который делает классификатор более устойчивым к ошибкам:   </p>
<<<<<<< HEAD
<hr>
=======
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p><img alt="" src="https://cs231n.github.io/assets/knn.jpeg"></p>
<p>Пример разницы между классификатором «один ближайший сосед» и классификатором «ближайшие 5 соседей» с использованием двумерных точек и 3 классов (красный, синий, зелёный). 
Цветные области показывают <strong>границы решений</strong>, создаваемые классификатором с использованием расстояния  $L_2$. 
Белые области показывают точки, которые классифицируются неоднозначно - голоса за классы равны как минимум для двух классов. 
Обратите внимание, что в случае классификатора 1-соседа ошибки создают небольшие островки вероятных неверных прогнозов. 
Например, зелёная точка в середине облака синих точек. 
В это же время классификатор 5-соседей сглаживает эти неровности, что, приводит к лучшему <strong>обобщению</strong> на тестовых данных. 
Также обратите внимание, что серые области на изображении 5-соседей вызваны равенством голосов ближайших соседей. 
<<<<<<< HEAD
Например, 2 соседа красные, следующие два соседа синие, последний сосед зелёный. </p>
<hr>
<p>На практике почти всегда используется метод k-ближайших соседей. 
Но какое значение k следует использовать? 
Давайте рассмотрим этот вопрос поподробнее.   </p>
<h4>Наборы данных и настройки гиперпараметров</h4>
=======
Например, 2 соседа красные, следующие два соседа синие, последний сосед зелёный.  </p>
<p>На практике почти всегда используется метод k-ближайших соседей. 
Но какое значение k следует использовать? 
Давайте рассмотрим этот вопрос поподробнее.   </p>
<h3>Наборы данных и настройки гиперпараметров</h3>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>Классификатор k-ближайших соседей требует настройки параметра <em>k</em>. 
Интуитивно можно предположить, что существует число, которое подходит лучше всего. 
Кроме того, мы увидели, что существует множество различных функций расстояния, которые мы могли бы использовать: норма  <strong>$L_1$</strong>, норма <strong>$L_2$</strong>, а также множество других вариантов, которые мы даже не рассматривали (например, скалярные произведения). 
Эти варианты называются <strong>гиперпараметрами</strong>, и они очень часто используются при разработке многих алгоритмов машинного обучения, которые обучаются на данных. 
Часто не очевидно, какие значения/настройки следует выбрать.   </p>
<p>У вас может возникнуть соблазн предложить попробовать множество различных значений и посмотреть, что работает лучше всего. 
Это хорошая идея, и именно это мы и сделаем, но делать это нужно очень осторожно. 
В частности, <strong>мы не можем использовать тестовый набор данных для настройки гиперпараметров</strong>. 
Всякий раз, когда вы разрабатываете алгоритмы машинного обучения, вы должны относиться к тестовому набору данных как к очень ценному ресурсу, к которому, в идеале, не следует прикасаться до самого конца. 
В противном случае существует реальная опасность того, что вы настроите гиперпараметры так, чтобы они хорошо работали на тестовом наборе данных, но при развёртывании модели показывали значительное снижение производительности. 
На практике можно сказать, что произошло <strong>переобучение</strong> на тестовом наборе данных. 
С другой стороны, если вы настраиваете гиперпараметры на тестовом наборе данных, вы фактически используете тестовый набор данных в качестве обучающего. 
По этой причине точность, которую вы достигаете на нём, будет слишком оптимистичной по сравнению с тем, что будет наблюдаться на реальных данных при развёртывании модели. 
Но если вы используете тестовый набор данных только один раз в конце, он остаётся хорошим показателем для того, чтобы измерить степень <strong>обобщения</strong> вашего классификатора.   </p>
<p><strong>Итого: Оценивайте модель на тестовой выборке только один раз, в самом конце!</strong></p>
<p>Существует правильный способ настройки гиперпараметров, который никак не затрагивает тестовый набор данных. 
Идея состоит в том, чтобы разделить обучающую выборку на две части: немного меньший обучающий набор и то, что называется <strong>выборкой для валидации</strong>. 
Используя в качестве примера CIFAR-10, мы могли бы использовать 49 000 обучающих изображений для обучения и оставить 1000 для валидации. 
Этот набор данных по сути используется для настройки гиперпараметров.   </p>
<p>Вот как это может выглядеть в случае CIFAR-10:   </p>
<div class="code"><pre class="code literal-block"><span class="c1"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span>
<span class="c1"># recall Xtr_rows is 50,000 x 3072 matrix</span>
<span class="n">Xval_rows</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># take first 1000 for validation</span>
<span class="n">Yval</span> <span class="o">=</span> <span class="n">Ytr</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">Xtr_rows</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[</span><span class="mi">1000</span><span class="p">:,</span> <span class="p">:]</span> <span class="c1"># keep last 49,000 for train</span>
<span class="n">Ytr</span> <span class="o">=</span> <span class="n">Ytr</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span>

<span class="c1"># find hyperparameters that work best on the validation set</span>
<span class="n">validation_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>

<span class="c1"># use a particular value of k and evaluation on validation data</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span>
<span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span>
<span class="c1"># here we assume a modified NearestNeighbor class that can take a k as input</span>
<span class="n">Yval_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xval_rows</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yval_predict</span> <span class="o">==</span> <span class="n">Yval</span><span class="p">)</span>
<span class="nb">print</span> <span class="s1">'accuracy: </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">,)</span>

<span class="c1"># keep track of what works on the validation set</span>
<span class="n">validation_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
</pre></div>

<p>По завершении этой процедуры построим график, который показывает, какие значения k работают лучше всего. 
Затем мы остановимся на этом значении и проведем оценку на реальном тестовом наборе данных.   </p>
<p>Разделите обучающую выборку на обучающую и валидационную. 
Используйте проверочную выборку для настройки всех гиперпараметров. 
В конце выполните один запуск на тестовой выборке и оцените производительность.</p>
<<<<<<< HEAD
<p><strong>Кросс-валидация</strong>. В случаях, когда размер обучающих данных (и, следовательно, проверочных данных) может быть небольшим, люди иногда используют более сложный метод настройки гиперпараметров, называемый <strong>кросс-валидацией</strong>.</p>
<p>Если вернуться к нашему предыдущему примеру, то идея заключается в том, что вместо произвольного выбора первых 1000 точек данных в качестве проверочного набора, а остальных — в качестве обучающего, можно получить более точную и менее зашумлённую оценку того, насколько хорошо работает определённое значение <strong>k</strong>, перебирая различные проверочные наборы и усредняя результаты по ним. Например, при 5-кратной перекрёстной проверке мы разделили бы обучающие данные на 5 равных частей, использовали 4 из них для обучения, а 1 — для проверки. Затем мы бы определили, какая из выборок является контрольной, оценили бы производительность и, наконец, усреднили бы производительность по разным выборкам.   </p>
=======
<p><strong>Кросс-валидация</strong><br>
В случаях, когда размер обучающих данных (и, следовательно, проверочных данных) может быть небольшим, люди иногда используют более сложный метод настройки гиперпараметров, называемый <strong>Кросс-валидацией</strong>. 
Если вернуться к нашему предыдущему примеру, то идея заключается в том, что вместо произвольного выбора первых 1000 точек данных в качестве проверочного набора, а остальных — в качестве обучающего, можно получить более точную и менее зашумлённую оценку того, насколько хорошо работает определённое значение <strong>k</strong>, перебирая различные проверочные наборы и усредняя результаты по ним. Например, при 5-кратной перекрёстной проверке мы разделили бы обучающие данные на 5 равных частей, использовали 4 из них для обучения, а 1 — для проверки. Затем мы бы определили, какая из выборок является контрольной, оценили бы производительность и, наконец, усреднили бы производительность по разным выборкам.   </p>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/cvplot.png"><br>
Пример 5-кратного выполнения перекрестной проверки для параметра <strong>k</strong>. Для каждого значения <strong>k</strong> мы тренируемся на 4 сгибах и оцениваем на 5-м. Следовательно, для каждого k мы получаем 5 значений точности для проверочного сгиба (точность отражается на оси y, и каждый результат равен точке). Линия тренда проводится через среднее значение результатов для каждого <strong>k</strong>, а столбики ошибок указывают на стандартное отклонение. Обратите внимание, что в данном конкретном случае перекрёстная проверка показывает, что значение около <strong>k = 7</strong> лучше всего подходит для этого конкретного набора данных (соответствует пику на графике). Если бы мы использовали более 5 циклов, то могли бы ожидать более плавную (то есть менее шумную) кривую.   </p>
<hr>
<<<<<<< HEAD
<p>На практике люди предпочитают избегать перекрёстной проверки в пользу одного проверочного набора данных, поскольку перекрёстная проверка может быть ресурсозатратной. Обычно люди используют от <strong>50%</strong> до <strong>90%</strong> обучающих данных для обучения и остальную часть для проверки. Однако это зависит от множества факторов: например, если количество гиперпараметров велико, вы можете предпочесть использовать более крупные проверочные наборы данных. Если количество примеров в проверочном наборе невелико (возможно, всего несколько сотен или около того), безопаснее использовать перекрёстную проверку. На практике обычно используется 3-кратная, 5-кратная или 10-кратная перекрёстная проверка.     </p>
=======
<p>На практике люди предпочитают избегать перекрёстной проверки в пользу одного проверочного набора данных, поскольку перекрёстная проверка может быть ресурсозатратной. Обычно люди используют от <strong>50%</strong> до <strong>90%</strong> обучающих данных для обучения и остальную часть для проверки. Однако это зависит от множества факторов: например, если количество гиперпараметров велико, вы можете предпочесть использовать более крупные проверочные наборы данных. Если количество примеров в проверочном наборе невелико (возможно, всего несколько сотен или около того), безопаснее использовать перекрёстную проверку. На практике обычно используется 3-кратная, 5-кратная или 10-кратная перекрёстная проверка.   </p>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/crossval.jpeg"></p>
<p>Обычное разделение данных. Выделяются обучающий и тестовый наборы данных. Обучающий набор данных делится на части (например, здесь их 5). Части 1-4 становятся обучающим набором данных. Одна часть (например, часть 5, выделенная здесь жёлтым цветом) называется проверочной частью и используется для настройки гиперпараметров. Перекрёстная проверка идёт дальше и позволяет выбрать, какая часть будет проверочной, отдельно от частей 1-5. Это называется 5-кратной перекрёстной проверкой. В самом конце, когда модель обучена и определены все наилучшие гиперпараметры, модель один раз оценивается на тестовых данных (красный цвет). </p>
<hr>
<p><strong>Плюсы и минусы классификатора ближайших соседей.</strong>   </p>
<p>Стоит рассмотреть некоторые преимущества и недостатки классификатора «ближайший сосед». 
Очевидно, что одним из преимуществ является простота реализации и понимания. 
Кроме того, обучение классификатора не занимает много времени, поскольку всё, что требуется, — это хранить и, возможно, индексировать обучающие данные. 
Однако мы платим за это вычислительными затратами во время тестирования, поскольку для классификации тестового примера требуется сравнение с каждым обучающим примером. 
Это неправильно, поскольку на практике мы часто уделяем больше внимания эффективности во время тестирования, чем во время обучения. 
На самом деле, объемные нейронные сети, которые мы будем разрабатывать в этом классе, смещают этот компромисс в другую крайность: их обучение обходится очень дорого, но после завершения обучения классифицировать новый тестовый пример очень дёшево. 
Такой режим работы гораздо более желателен на практике.   </p>
<p>Кроме того, вычислительная сложность классификатора «ближайший сосед» является активной областью исследований, и существует несколько алгоритмов и библиотек <strong>приблизительного поиска ближайшего соседа</strong> (<em>ANN</em>), которые могут ускорить поиск ближайшего соседа в наборе данных (например, <a href="https://github.com/mariusmuja/flann">FLANN</a> ). 
Эти алгоритмы позволяют найти компромисс между точностью поиска ближайшего соседа и его пространственной/временной сложностью во время поиска и обычно полагаются на этап предварительной обработки/индексирования, который включает в себя построение KD-дерева или запуск алгоритма k-средних.  </p>
<p>В некоторых случаях классификатор ближайших соседей может быть хорошим выбором (особенно если данные имеют низкую размерность), но он редко подходит для использования в практических задачах классификации изображений. 
Одна из проблем заключается в том, что изображения — это объекты с высокой размерностью (то есть они часто содержат много пикселей), а расстояния в многомерных пространствах могут быть очень нелогичными. 
На изображении ниже показано, что сходство на основе пикселей, которое мы описали выше, сильно отличается от сходства с точки зрения восприятия:   </p>
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/samenorm.png"></p>
<p>Расстояния на основе пикселей в многомерных данных (и особенно в изображениях) могут быть очень неинтуитивными. 
Исходное изображение (слева) и три других изображения рядом с ним, которые находятся на одинаковом расстоянии от него на основе пиксельного расстояния <strong>$L_2$</strong>. 
<<<<<<< HEAD
Очевидно, что пиксельное расстояние никак не соответствует перцептивному или семантическому сходству. </p>
<hr>
<p>Вот ещё одна визуализация, которая убедит вас в том, что использование разницы в пикселях для сравнения изображений недостаточно. Мы можем использовать метод визуализации под названием <a href="https://lvdmaaten.github.io/tsne/">t-SNE</a>, чтобы взять изображения CIFAR-10 и разместить их в двух измерениях так, чтобы их  парные (локальные) расстояния сохранялись наилучшим образом. В этой визуализации изображения, которые показаны рядом, считаются очень близкими в соответствии с расстоянием <strong>$L_2$</strong> по пикселям, которое мы разработали выше:    </p>
=======
Очевидно, что пиксельное расстояние никак не соответствует перцептивному или семантическому сходству.   </p>
<hr>
<p>Вот ещё одна визуализация, которая убедит вас в том, что использование разницы в пикселях для сравнения изображений недостаточно. Мы можем использовать метод визуализации под названием <a href="https://lvdmaaten.github.io/tsne/">t-SNE</a>, чтобы взять изображения CIFAR-10 и разместить их в двух измерениях так, чтобы их  парные (локальные) расстояния сохранялись наилучшим образом. В этой визуализации изображения, которые показаны рядом, считаются очень близкими в соответствии с расстоянием <strong>$L_2$</strong> по пикселям, которое мы разработали выше:   </p>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<hr>
<p><img alt="" src="https://cs231n.github.io/assets/pixels_embed_cifar10.jpg"></p>
<p>Изображения CIFAR-10, размещённые в двух измерениях с помощью <em>t-SNE</em>. Изображения, расположенные рядом на этом изображении, считаются близкими на основе пиксельного расстояния <strong>$L_2$</strong>. 
Обратите внимание на сильное влияние фона, а не семантических различий между классами. 
<<<<<<< HEAD
Нажмите <a href="https://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg">здесь</a>, чтобы увидеть увеличенную версию этой визуализации. </p>
<hr>
<p>В частности, обратите внимание, что изображения, расположенные друг рядом с другом, в большей степени зависят от общего цветового распределения изображений или типа фона, а не от их семантической идентичности. Например, собаку можно увидеть рядом с лягушкой, потому что они обе находятся на белом фоне. В идеале мы хотели бы, чтобы изображения всех 10 классов образовывали собственные кластеры, чтобы изображения одного класса находились рядом друг с другом независимо от нерелевантных характеристик и вариаций (например, фона). Однако, чтобы добиться этого, нам придётся выйти за рамки необработанных пикселей.   </p>
<h4>Применение kNN на практике</h4>
=======
Нажмите <a href="https://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg">здесь</a>, чтобы увидеть увеличенную версию этой визуализации.   </p>
<hr>
<p>В частности, обратите внимание, что изображения, расположенные друг рядом с другом, в большей степени зависят от общего цветового распределения изображений или типа фона, а не от их семантической идентичности. Например, собаку можно увидеть рядом с лягушкой, потому что они обе находятся на белом фоне. В идеале мы хотели бы, чтобы изображения всех 10 классов образовывали собственные кластеры, чтобы изображения одного класса находились рядом друг с другом независимо от нерелевантных характеристик и вариаций (например, фона). Однако, чтобы добиться этого, нам придётся выйти за рамки необработанных пикселей.   </p>
<h3>Применение kNN на практике</h3>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>Подводя итог: 
- Мы рассмотрели задачу <strong>классификации изображений</strong>, в которой нам даётся набор изображений, каждое из которых помечено одной категорией. Затем нас просят предсказать эти категории для нового набора тестовых изображений и оценить точность прогнозов.
- Мы представили простой классификатор под названием <em>«классификатор ближайших соседей»</em>.  Мы увидели, что существует множество гиперпараметров (например, значение k или тип расстояния, используемого для сравнения примеров), связанных с этим классификатором, и что не существует очевидного способа их выбора.
- Мы увидели, что правильный способ задать эти гиперпараметры — разделить обучающие данные на две части: <em>обучающий набор</em> и <em>поддельный тестовый набор</em>, который мы называем <strong>набором для проверки</strong>. Мы пробуем разные значения гиперпараметров и оставляем те, которые обеспечивают наилучшую производительность на наборе для проверки.
- Если вас беспокоит нехватка обучающих данных, мы обсудили процедуру под названием <strong>перекрёстная проверка</strong>, которая может помочь уменьшить погрешность при оценке наиболее эффективных гиперпараметров.
- Как только мы находим оптимальные гиперпараметры, мы фиксируем их и проводим одну <strong>оценку</strong> на реальном тестовом наборе данных.
- Мы увидели, что метод ближайшего соседа может обеспечить нам точность около <strong>40%</strong> на CIFAR-10. Он прост в реализации, но требует хранения всего обучающего набора данных, и его сложно оценивать на тестовых изображениях.
<<<<<<< HEAD
- В итоге мы увидели, что использование расстояний <strong>$L_1$</strong> или <strong>$L_2$</strong> по необработанным значениям пикселей нецелесообразно, поскольку эти расстояния сильнее коррелируют с фоном и цветовыми распределениями изображений, чем с их семантическим содержанием.   </p>
=======
- В итоге мы увидели, что использование расстояний <strong>$L_1$</strong> или <strong>$L_2\</strong>) по необработанным значениям пикселей нецелесообразно, поскольку эти расстояния сильнее коррелируют с фоном и цветовыми распределениями изображений, чем с их семантическим содержанием.   </p>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>На следующих лекциях мы приступим к решению этих задач и в конечном итоге придём к решениям, которые обеспечат точность <strong>90%</strong>, 
позволят полностью отказаться от обучающего набора данных после завершения обучения и позволят оценивать тестовые изображения менее чем за миллисекунду.   </p>
<p>Если вы хотите применить <em>kNN</em> на практике (не на изображениях), действуйте следующим образом:  </p>
<ol>
<li>
<p><strong>Предварительная обработка данных.</strong> 
Нормализуйте признаки в ваших данных (например, один пиксель на изображениях), чтобы среднее значение было равно нулю, а дисперсия — единице. 
Мы рассмотрим этот прием более подробно в следующих разделах. 
Сейчас нормализация данных не используется, потому что распределение яркости пикселей на изображениях достаточно однородны.  </p>
</li>
<li>
<p><strong>Рассмотрите возможность снижения размерности данных</strong>. 
На практике для снижения размерности используются следующие методы: </p>
</li>
<li>метод главных компонент <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">ссылка на вики-страницу</a>, <a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">ссылка на CS229</a>, <a href="https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/">ссылка на блог</a>, </li>
<li>метод независимых компонент <a href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis">ссылка на вики-страницу</a>, <a href="https://kevinzakka.github.io/2020/02/10/nca/">ссылка на блог</a> </li>
<li>
<p><a href="https://scikit-learn.org/stable/modules/random_projection.html">случайные проекции</a>.  </p>
</li>
<li>
<p><strong>Разделите обучающие данные случайным образом на обучающую и проверочную (валидационную) выборки.</strong> 
Как правило, в обучающую выборку попадает от <strong>70</strong> до <strong>90%</strong> данных. 
Этот параметр зависит от того, сколько у вас гиперпараметров и насколько сильно они влияют на результат. 
Если нужно оценить множество гиперпараметров, лучше использовать более крупную проверочную выборку для их эффективной оценки. 
Если вас беспокоит размер проверочной выборки, лучше разделить обучающие данные на части и выполнить кросс-валидацию. </p>
</li>
<li>
<p><strong>Обучите и оцените классификатор kNN на кросс-валидации.</strong> 
По возможности выполняйте кросс-валидацию для множества вариантов k и для разных типов расстояний (<strong>$L_1$ и $L_2$</strong> — хорошие кандидаты).<br>
Если вы можете позволить себе потратить больше времени на вычисления, всегда безопаснее использовать кросс-валидацию. 
Чем больше циклов обучения пройдет, тем лучше, но тем дороже с точки зрения вычислений.  </p>
</li>
<li>
<p><strong>Оцените задержку классификатора.</strong> 
Если ваш классификатор kNN работает слишком долгo, рассмотрите возможность использования библиотеки приближённых ближайших соседей. 
Например, библиотека <a href="https://github.com/mariusmuja/flann">FLANN</a> позволяет ускорить поиск за счёт некоторой потери точности.  </p>
</li>
<li>
<p><strong>Обратите внимание на гиперпараметры, которые дали наилучшие результаты.</strong> 
Возникает вопрос, следует ли использовать валидационный набор для финального обучения с наилучшими гиперпараметрами. 
Дело в том, что добавить данные для валидации в набор обучающих данных, оптимальные гиперпараметры могут измениться, поскольку размер данных увеличится. 
На практике лучше не использовать данные валидации в итоговом классификаторе и считать их потерянными при оценке гиперпараметров. </p>
</li>
<li>
<p><strong>Оцените наилучшую модель на тестовом наборе данных.</strong> 
Вычислите точность на тестовой выборке и объявите результат производительностью классификатора <em>kNN</em> на ваших данных.  </p>
</li>
</ol>
<<<<<<< HEAD
<h5>Дополнительные материалы</h5>
=======
<h3>Дополнительные материалы</h3>
>>>>>>> d57914c0bf70c9a9ee0eec1512a65385579bcbae
<p>Вот несколько дополнительных ссылок, которые могут быть интересными для дальнейшего чтения:<br>
- <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">Несколько полезных фактов о машинном обучении</a>, особенно раздел 6, но рекомендуется к прочтению вся статья.<br>
- <a href="https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">Распознавание и изучение категорий объектов</a>, краткий курс по категоризации объектов на ICCV 2005.  </p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/cv/" rel="tag">CV</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../visualisation/" rel="prev" title="Понимание и визуализация">Предыдущая запись</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:andrej.labintsev@yandex.ru">Андрей Лабинцев</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
