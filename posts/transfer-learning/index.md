<!--
.. title: Трансфер обучения
.. slug: transfer-learning
.. date: 2025-02-14 19:42:16 UTC+03:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text
.. has_math: true
-->

# Трансфер обучения

___(Эти заметки в настоящее время находятся в черновой форме и находятся в разработке)___

Содержание:
- [Трансферное обучение]()
- [Дополнительные примечания]()

## Трансферное обучение  
  
На практике очень немногие обучают всю сверточную сеть с нуля (со случайной инициализацией), потому что относительно редко удается иметь набор данных достаточного размера. Вместо этого обычно предварительно обучают *ConvNet* на очень большом наборе данных (например, *ImageNet*, который содержит 1,2 миллиона изображений с 1000 категориями), а затем используют ConvNet либо в качестве инициализации, либо в качестве фиксированного экстрактора признаков для интересующей задачи. Три основных сценария трансферного обучения выглядят следующим образом:
- __ConvNet в качестве фиксированного экстрактора признаков__. Возьмите предварительно обученный *ConvNet* на *ImageNet*, удалите последний полностью подключенный слой (выходные данные этого слоя представляют собой 1000 баллов класса для другой задачи, такой как *ImageNet*), а затем обрабатывайте остальную часть *ConvNet* как фиксированный экстрактор признаков для нового набора данных. В *AlexNet* это позволило бы вычислить вектор **4096-D** для каждого изображения, содержащего активации скрытого слоя непосредственно перед классификатором. Мы называем эти функции __кодами CNN__. Для производительности важно, чтобы эти коды были *ReLUd* (т.е. пороговыми на нуле), если они также были пороговыми во время обучения *ConvNet* на *ImageNet* (как это обычно бывает). После извлечения кодов **4096-D** для всех изображений обучите линейный классификатор (например, Linear *SVM* или классификатор Softmax) для нового набора данных.
- __Тонкая настройка ConvNet__. Вторая стратегия заключается не только в замене и переобучении классификатора поверх *ConvNet* на новом наборе данных, но и в тонкой настройке весов предварительно обученной сети путем продолжения обратного распространения. Можно тонко настроить все уровни *ConvNet*, или можно оставить некоторые из более ранних уровней фиксированными (из-за опасений переобучения) и выполнить тонкую настройку только некоторой части сети более высокого уровня. Это мотивировано наблюдением, что более ранние функции *ConvNet* содержат более общие функции (например, детекторы краев или детекторы цветных пятен), которые должны быть полезны для многих задач, но более поздние уровни *ConvNet* становятся все более специфичными для деталей классов, содержащихся в исходном наборе данных. Например, в случае *ImageNet*, который содержит множество пород собак, значительная часть репрезентативной мощности *ConvNet* может быть направлена на функции, специфичные для дифференциации между породами собак.
- __Предварительно обученные модели__. Поскольку обучение современных *ConvNet* на нескольких графических процессорах *ImageNet* занимает 2–3 недели, часто можно увидеть, как люди выпускают свои окончательные контрольные точки *ConvNet* в пользу других пользователей, которые могут использовать сети для тонкой настройки. Например, в библиотеке *Caffe* есть [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo), где люди делятся своими сетевыми весами.  
  

__Когда и как проводить тонкую настройку?__ Как вы решаете, какой тип переносного обучения вы должны выполнять на новом наборе данных? Это зависит от нескольких факторов, но два наиболее важных из них — это размер нового набора данных (маленький или большой) и его сходство с исходным набором данных (например, похожий на ImageNet с точки зрения содержимого изображений и классов, или сильно отличающийся, например, изображения микроскопа). Помня о том, что объекты *ConvNet* более универсальны в ранних слоях и более специфичны для исходного набора данных в более поздних слоях, вот некоторые общие эмпирические правила для навигации по *4* основным сценариям:  
1. _Новый набор данных имеет небольшой размер и похож на исходный набор данных_. Поскольку объем данных невелик, тонкая настройка *ConvNet* не является хорошей идеей из-за проблем с переобучением. Поскольку данные аналогичны исходным данным, мы ожидаем, что функции более высокого уровня в *ConvNet* также будут иметь отношение к этому набору данных. Следовательно, лучшей идеей может быть обучение линейного классификатора на кодах *CNN*.
2. _Новый набор данных имеет большой размер и похож на исходный набор данных_. Поскольку у нас больше данных, мы можем быть уверены в том, что не переучимся, если попытаемся выполнить тонкую настройку через всю сеть.
3. _Новый набор данных небольшой, но сильно отличается от исходного_. Поскольку данные невелики, вероятно, лучше всего обучить только линейный классификатор. Поскольку набор данных сильно отличается, возможно, не стоит обучать классификатор с вершины сети, которая содержит больше функций, специфичных для набора данных. Вместо этого, возможно, лучше обучить классификатор *SVM* от активаций где-то раньше в сети.
4. _Новый набор данных имеет большой размер и сильно отличается от исходного набора данных_. Поскольку набор данных очень большой, можно ожидать, что мы сможем позволить себе обучить *ConvNet* с нуля. Однако на практике очень часто все же полезно инициализировать весами из предварительно обученной модели. В этом случае у нас было бы достаточно данных и уверенности для тонкой настройки по всей сети.  
  

__Практические советы__. Есть несколько дополнительных моментов, о которых следует помнить при выполнении *Transfer Learning*:

- _Ограничения из предварительно обученных моделей_. Обратите внимание, что если вы хотите использовать предварительно обученную сеть, вы можете быть немного ограничены с точки зрения архитектуры, которую вы можете использовать для вашего нового набора данных. Например, вы не можете произвольно удалять слои Conv из предварительно обученной сети. Тем не менее, некоторые изменения просты: благодаря совместному использованию параметров вы можете легко запустить предварительно обученную сеть на изображениях разного пространственного размера. Это ясно видно в случае слоев Conv/Pool, потому что их прямая функция не зависит от пространственного размера входного объема (до тех пор, пока шаги «подходят»). В случае слоев FC это по-прежнему верно, потому что слои FC могут быть преобразованы в сверточный слой: например, в AlexNet окончательный объем пула перед первым слоем *FC* имеет размер **[6x6x512]**. Следовательно, слой *FC*, рассматривающий этот объем, эквивалентен наличию сверточного слоя, который имеет размер рецептивного поля 6x6 и применяется с отступом **0**.
- _Скорость обучения_. Обычно для тонко настраиваемых весов *ConvNet* используется меньшая скорость обучения по сравнению с весами (случайно инициализированными) для нового линейного классификатора, который вычисляет баллы классов нового набора данных. Это связано с тем, что мы ожидаем, что веса *ConvNet* относительно хороши, поэтому мы не хотим искажать их слишком быстро и слишком сильно (особенно когда новый линейный классификатор над ними обучается на основе случайной инициализации).  
  

## Дополнительные примечания
- [Готовые функции CNN: A Astounding Baseline for Recognition](http://arxiv.org/abs/1403.6382) обучает *SVM* функциям из предварительно обученного *ImageNet* *ConvNet* и сообщает о нескольких современных результатах.
- [DeCAF](http://arxiv.org/abs/1310.1531) сообщал об аналогичных выводах в 2013 году. Фреймворк в этой статье (*DeCAF*) был предшественником библиотеки *C++* *Caffe* на основе *Python*.
- [Насколько переносимы функции в глубоких нейронных сетях?](http://arxiv.org/abs/1411.1792) Подробно изучает эффективность обучения переносу, включая некоторые неинтуитивные выводы о коадаптациях слоев.