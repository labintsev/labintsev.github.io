<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Заметки по ML, DL</title><link>https://mldl.ru/</link><description>Заметки по machine learning, deep learning.</description><atom:link href="https://mldl.ru/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>ru</language><copyright>Contents © 2025 &lt;a href="mailto:andrej.labintsev@yandex.ru"&gt;Андрей Лабинцев&lt;/a&gt; </copyright><lastBuildDate>Fri, 14 Mar 2025 18:50:39 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Трансфер обучения</title><link>https://mldl.ru/posts/transfer-learning/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Трансфер обучения&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;(Эти заметки в настоящее время находятся в черновой форме и находятся в разработке)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Содержание:
- &lt;a href="https://mldl.ru/posts/transfer-learning/"&gt;Трансферное обучение&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/transfer-learning/"&gt;Дополнительные примечания&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Трансферное обучение&lt;/h3&gt;
&lt;p&gt;На практике очень немногие обучают всю сверточную сеть с нуля (со случайной инициализацией), потому что относительно редко удается иметь набор данных достаточного размера. Вместо этого обычно предварительно обучают &lt;em&gt;ConvNet&lt;/em&gt; на очень большом наборе данных (например, &lt;em&gt;ImageNet&lt;/em&gt;, который содержит 1,2 миллиона изображений с 1000 категориями), а затем используют ConvNet либо в качестве инициализации, либо в качестве фиксированного экстрактора признаков для интересующей задачи. Три основных сценария трансферного обучения выглядят следующим образом:
- &lt;strong&gt;ConvNet в качестве фиксированного экстрактора признаков&lt;/strong&gt;. Возьмите предварительно обученный &lt;em&gt;ConvNet&lt;/em&gt; на &lt;em&gt;ImageNet&lt;/em&gt;, удалите последний полностью подключенный слой (выходные данные этого слоя представляют собой 1000 баллов класса для другой задачи, такой как &lt;em&gt;ImageNet&lt;/em&gt;), а затем обрабатывайте остальную часть &lt;em&gt;ConvNet&lt;/em&gt; как фиксированный экстрактор признаков для нового набора данных. В &lt;em&gt;AlexNet&lt;/em&gt; это позволило бы вычислить вектор &lt;strong&gt;4096-D&lt;/strong&gt; для каждого изображения, содержащего активации скрытого слоя непосредственно перед классификатором. Мы называем эти функции &lt;strong&gt;кодами CNN&lt;/strong&gt;. Для производительности важно, чтобы эти коды были &lt;em&gt;ReLUd&lt;/em&gt; (т.е. пороговыми на нуле), если они также были пороговыми во время обучения &lt;em&gt;ConvNet&lt;/em&gt; на &lt;em&gt;ImageNet&lt;/em&gt; (как это обычно бывает). После извлечения кодов &lt;strong&gt;4096-D&lt;/strong&gt; для всех изображений обучите линейный классификатор (например, Linear &lt;em&gt;SVM&lt;/em&gt; или классификатор Softmax) для нового набора данных.
- &lt;strong&gt;Тонкая настройка ConvNet&lt;/strong&gt;. Вторая стратегия заключается не только в замене и переобучении классификатора поверх &lt;em&gt;ConvNet&lt;/em&gt; на новом наборе данных, но и в тонкой настройке весов предварительно обученной сети путем продолжения обратного распространения. Можно тонко настроить все уровни &lt;em&gt;ConvNet&lt;/em&gt;, или можно оставить некоторые из более ранних уровней фиксированными (из-за опасений переобучения) и выполнить тонкую настройку только некоторой части сети более высокого уровня. Это мотивировано наблюдением, что более ранние функции &lt;em&gt;ConvNet&lt;/em&gt; содержат более общие функции (например, детекторы краев или детекторы цветных пятен), которые должны быть полезны для многих задач, но более поздние уровни &lt;em&gt;ConvNet&lt;/em&gt; становятся все более специфичными для деталей классов, содержащихся в исходном наборе данных. Например, в случае &lt;em&gt;ImageNet&lt;/em&gt;, который содержит множество пород собак, значительная часть репрезентативной мощности &lt;em&gt;ConvNet&lt;/em&gt; может быть направлена на функции, специфичные для дифференциации между породами собак.
- &lt;strong&gt;Предварительно обученные модели&lt;/strong&gt;. Поскольку обучение современных &lt;em&gt;ConvNet&lt;/em&gt; на нескольких графических процессорах &lt;em&gt;ImageNet&lt;/em&gt; занимает 2–3 недели, часто можно увидеть, как люди выпускают свои окончательные контрольные точки &lt;em&gt;ConvNet&lt;/em&gt; в пользу других пользователей, которые могут использовать сети для тонкой настройки. Например, в библиотеке &lt;em&gt;Caffe&lt;/em&gt; есть &lt;a href="https://github.com/BVLC/caffe/wiki/Model-Zoo"&gt;Model Zoo&lt;/a&gt;, где люди делятся своими сетевыми весами.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Когда и как проводить тонкую настройку?&lt;/strong&gt; Как вы решаете, какой тип переносного обучения вы должны выполнять на новом наборе данных? Это зависит от нескольких факторов, но два наиболее важных из них — это размер нового набора данных (маленький или большой) и его сходство с исходным набором данных (например, похожий на ImageNet с точки зрения содержимого изображений и классов, или сильно отличающийся, например, изображения микроскопа). Помня о том, что объекты &lt;em&gt;ConvNet&lt;/em&gt; более универсальны в ранних слоях и более специфичны для исходного набора данных в более поздних слоях, вот некоторые общие эмпирические правила для навигации по &lt;em&gt;4&lt;/em&gt; основным сценариям:&lt;br&gt;
1. &lt;em&gt;Новый набор данных имеет небольшой размер и похож на исходный набор данных&lt;/em&gt;. Поскольку объем данных невелик, тонкая настройка &lt;em&gt;ConvNet&lt;/em&gt; не является хорошей идеей из-за проблем с переобучением. Поскольку данные аналогичны исходным данным, мы ожидаем, что функции более высокого уровня в &lt;em&gt;ConvNet&lt;/em&gt; также будут иметь отношение к этому набору данных. Следовательно, лучшей идеей может быть обучение линейного классификатора на кодах &lt;em&gt;CNN&lt;/em&gt;.
2. &lt;em&gt;Новый набор данных имеет большой размер и похож на исходный набор данных&lt;/em&gt;. Поскольку у нас больше данных, мы можем быть уверены в том, что не переучимся, если попытаемся выполнить тонкую настройку через всю сеть.
3. &lt;em&gt;Новый набор данных небольшой, но сильно отличается от исходного&lt;/em&gt;. Поскольку данные невелики, вероятно, лучше всего обучить только линейный классификатор. Поскольку набор данных сильно отличается, возможно, не стоит обучать классификатор с вершины сети, которая содержит больше функций, специфичных для набора данных. Вместо этого, возможно, лучше обучить классификатор &lt;em&gt;SVM&lt;/em&gt; от активаций где-то раньше в сети.
4. &lt;em&gt;Новый набор данных имеет большой размер и сильно отличается от исходного набора данных&lt;/em&gt;. Поскольку набор данных очень большой, можно ожидать, что мы сможем позволить себе обучить &lt;em&gt;ConvNet&lt;/em&gt; с нуля. Однако на практике очень часто все же полезно инициализировать весами из предварительно обученной модели. В этом случае у нас было бы достаточно данных и уверенности для тонкой настройки по всей сети.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Практические советы&lt;/strong&gt;. Есть несколько дополнительных моментов, о которых следует помнить при выполнении &lt;em&gt;Transfer Learning&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ограничения из предварительно обученных моделей&lt;/em&gt;. Обратите внимание, что если вы хотите использовать предварительно обученную сеть, вы можете быть немного ограничены с точки зрения архитектуры, которую вы можете использовать для вашего нового набора данных. Например, вы не можете произвольно удалять слои Conv из предварительно обученной сети. Тем не менее, некоторые изменения просты: благодаря совместному использованию параметров вы можете легко запустить предварительно обученную сеть на изображениях разного пространственного размера. Это ясно видно в случае слоев Conv/Pool, потому что их прямая функция не зависит от пространственного размера входного объема (до тех пор, пока шаги «подходят»). В случае слоев FC это по-прежнему верно, потому что слои FC могут быть преобразованы в сверточный слой: например, в AlexNet окончательный объем пула перед первым слоем &lt;em&gt;FC&lt;/em&gt; имеет размер &lt;strong&gt;[6x6x512]&lt;/strong&gt;. Следовательно, слой &lt;em&gt;FC&lt;/em&gt;, рассматривающий этот объем, эквивалентен наличию сверточного слоя, который имеет размер рецептивного поля 6x6 и применяется с отступом &lt;strong&gt;0&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Скорость обучения&lt;/em&gt;. Обычно для тонко настраиваемых весов &lt;em&gt;ConvNet&lt;/em&gt; используется меньшая скорость обучения по сравнению с весами (случайно инициализированными) для нового линейного классификатора, который вычисляет баллы классов нового набора данных. Это связано с тем, что мы ожидаем, что веса &lt;em&gt;ConvNet&lt;/em&gt; относительно хороши, поэтому мы не хотим искажать их слишком быстро и слишком сильно (особенно когда новый линейный классификатор над ними обучается на основе случайной инициализации).  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Дополнительные примечания&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1403.6382"&gt;Готовые функции CNN: A Astounding Baseline for Recognition&lt;/a&gt; обучает &lt;em&gt;SVM&lt;/em&gt; функциям из предварительно обученного &lt;em&gt;ImageNet&lt;/em&gt; &lt;em&gt;ConvNet&lt;/em&gt; и сообщает о нескольких современных результатах.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1310.1531"&gt;DeCAF&lt;/a&gt; сообщал об аналогичных выводах в 2013 году. Фреймворк в этой статье (&lt;em&gt;DeCAF&lt;/em&gt;) был предшественником библиотеки &lt;em&gt;C++&lt;/em&gt; &lt;em&gt;Caffe&lt;/em&gt; на основе &lt;em&gt;Python&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1411.1792"&gt;Насколько переносимы функции в глубоких нейронных сетях?&lt;/a&gt; Подробно изучает эффективность обучения переносу, включая некоторые неинтуитивные выводы о коадаптациях слоев.&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/transfer-learning/</guid><pubDate>Fri, 14 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Классификация изображений </title><link>https://mldl.ru/posts/image-classification/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Классификация изображений&lt;/h2&gt;
&lt;p&gt;В этой лекции мы познакомимся с проблемой классификации изображений. 
Решение проблемы заключается в подходе, основанном на большом объеме размеченных данных.   &lt;/p&gt;
&lt;p&gt;Содержание:&lt;br&gt;
1) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8E-%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9"&gt;Введение в классификацию изображений&lt;/a&gt;&lt;br&gt;
2) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B5%D0%B3%D0%BE-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B0"&gt;Классификатор ближайшего соседа&lt;/a&gt;&lt;br&gt;
3) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80-k---%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9"&gt;Классификатор k - ближайших соседей&lt;/a&gt; &lt;br&gt;
4) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D1%8B-%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85-%D0%B8-%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B8-%D0%B3%D0%B8%D0%BF%D0%B5%D1%80%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2"&gt;Наборы данных для настройки гиперпараметров&lt;/a&gt;&lt;br&gt;
5) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-knn-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B5"&gt;Применение kNN на практике&lt;/a&gt;&lt;br&gt;
6) &lt;a href="https://mldl.ru/posts/image-classification/#%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%82%D0%B5%D1%80%D0%B8%D0%B0%D0%BB%D1%8B"&gt;Дополнительные материалы&lt;/a&gt; &lt;/p&gt;
&lt;h3&gt;Введение в классификацию изображений&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Мотивация&lt;/strong&gt;. 
В этом разделе мы рассмотрим задачу классификации изображений. 
Задача заключается в присвоении входному изображению одной метки из фиксированного набора категорий. 
Это одна из основных задач компьютерного зрения, которая, несмотря на свою простоту, имеет множество практических применений. 
Более того, многие другие задачи компьютерного зрения (детекция объектов, сегментация) могут быть сведены к классификации изображений.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Пример&lt;/strong&gt;. 
Например, на изображении ниже модель классификации изображений принимает одно изображение и присваивает вероятности четырём меткам: &lt;em&gt;{«кошка», «собака», «шляпа», «кружка»}&lt;/em&gt;. 
Как показано на изображении, для компьютера изображение представляет собой один большой трёхмерный массив чисел. 
В этом примере изображение кошки имеет ширину &lt;strong&gt;248&lt;/strong&gt; пикселей, высоту &lt;strong&gt;400&lt;/strong&gt; пикселей и три цветовых канала: красный, зелёный, синий (или сокращённо &lt;em&gt;RGB&lt;/em&gt;). 
Таким образом, изображение состоит из &lt;strong&gt;248 x 400 x 3&lt;/strong&gt; чисел, или в общей сложности 297 600 чисел. 
Каждое число представляет собой целое число от 0 (чёрный) до 255 (белый). 
Наша задача — превратить эти четверть миллиона чисел в одну метку, например &lt;em&gt;«кошка»&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/classify.png"&gt;   &lt;/p&gt;
&lt;p&gt;Задача классификации изображений состоит в том, чтобы предсказать одну метку для заданного изображения. 
Так же мы можем предсказать распределение вероятностей для всех меток, что отражает степень нашей уверенности в результате классификации.   Изображения представляют собой трёхмерные массивы целых чисел от 0 до 255 размером «ширина x высота x 3». 
Число 3 обозначает три цветовых канала: красный, зелёный и синий.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Проблемы&lt;/strong&gt;.&lt;br&gt;
Поскольку задача распознавания визуального образа (например, кошки) относительно проста для человека, стоит рассмотреть связанные с ней проблемы с точки зрения алгоритма компьютерного зрения.&lt;br&gt;
Ниже мы приводим (неполный) список проблем, не забывая о том, что изображения представлены в виде трёхмерного массива значений яркости:&lt;br&gt;
- &lt;strong&gt;Изменение точки обзора&lt;/strong&gt;. Один экземпляр объекта может быть ориентирован по-разному относительно камеры.&lt;br&gt;
- &lt;strong&gt;Изменение масштаба&lt;/strong&gt;. Визуальные классы часто различаются по размеру (размеру в реальном мире, а не только по размеру на изображении).&lt;br&gt;
- &lt;strong&gt;Деформация&lt;/strong&gt;. Многие интересующие нас объекты не являются твёрдыми телами и могут сильно деформироваться.&lt;br&gt;
- &lt;strong&gt;Окклюзия&lt;/strong&gt;. Интересующие нас объекты могут быть частично скрыты. Иногда видна лишь небольшая часть объекта (всего несколько пикселей).&lt;br&gt;
- &lt;strong&gt;Условия освещения&lt;/strong&gt;. Влияние освещения на пиксели очень велико.&lt;br&gt;
- &lt;strong&gt;Фоновый шум&lt;/strong&gt;. Интересующие нас объекты могут сливаться с окружающей средой, что затрудняет их идентификацию.&lt;br&gt;
- &lt;strong&gt;Внутриклассовые различия&lt;/strong&gt;. Классы, представляющие интерес, часто могут быть относительно обширными, например, &lt;em&gt;стулья&lt;/em&gt;. 
Существует множество различных типов этих предметов, каждый из которых имеет отличный от других элементов класса внешний вид.    &lt;/p&gt;
&lt;p&gt;Хорошая модель классификации изображений должна быть инвариантна к перекрёстному произведению всех этих вариаций, сохраняя при этом чувствительность к межклассовым вариациям.   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/challenges.jpeg"&gt;   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Подход, основанный на данных&lt;/strong&gt;.&lt;br&gt;
Как бы мы могли написать алгоритм, который сможет классифицировать изображения по отдельным категориям? 
В отличие от написания алгоритма, например, для сортировки списка чисел, не очевидно, как можно написать алгоритм для распознавания кошек на изображениях. 
Поэтому вместо того, чтобы пытаться описать каждую из интересующих нас категорий непосредственно в коде, мы воспользуемся подходом, похожим на обучение ребёнка. 
Мы предоставим компьютеру множество примеров, а затем используем алгоритм обучения, который связывает визуальное представление с меткой каждого класса. 
Этот подход предполагает, что у нас есть обучающий набор с размеченными изображениями. 
Вот пример того, как может выглядеть такой набор данных:   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/trainset.jpg"&gt;   &lt;/p&gt;
&lt;p&gt;Пример обучающего набора для четырёх визуальных категорий. 
На практике у нас могут быть тысячи категорий и сотни тысяч изображений для каждой категории.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Конвейер классификации изображений&lt;/strong&gt;.&lt;br&gt;
Мы увидели, что задача классификации изображений состоит в том, чтобы взять массив пикселей изображения и присвоить ему метку. 
Наш полный конвейер можно формализовать следующим образом:&lt;br&gt;
- &lt;strong&gt;Входные данные&lt;/strong&gt;: состоят из набора &lt;strong&gt;&lt;em&gt;N&lt;/em&gt;&lt;/strong&gt; изображений, каждое из которых помечено одним из &lt;strong&gt;&lt;em&gt;K&lt;/em&gt;&lt;/strong&gt; различных классов. 
Эти данные называются &lt;em&gt;обучающей выборкой&lt;/em&gt;.&lt;br&gt;
- &lt;strong&gt;Обучение&lt;/strong&gt;: наша задача использовать обучающую выборку, чтобы узнать, как выглядит каждый из классов. 
Мы называем этот этап &lt;em&gt;обучением классификатора&lt;/em&gt; или &lt;em&gt;обучением модели&lt;/em&gt;.&lt;br&gt;
- &lt;strong&gt;Оценка&lt;/strong&gt;: в конце мы оцениваем качество классификатора. 
Для этого нужно задать вопрос о том, какие метки предскажет классификатор для нового набора изображений, которые он никогда раньше не видел. 
Затем мы сравниваем истинные метки этих изображений с теми, которые предсказал классификатор. 
Интуитивно мы надеемся, что многие прогнозы совпадут с истинными ответами. 
Данные, которые используются для оценки точности классификатора называются &lt;em&gt;тестовой выборкой&lt;/em&gt;. &lt;/p&gt;
&lt;h3&gt;Классификатор ближайшего соседа&lt;/h3&gt;
&lt;p&gt;В качестве первого подхода мы используем так называемый &lt;strong&gt;классификатор ближайшего соседа&lt;/strong&gt;. 
Этот классификатор не имеет ничего общего со свёрточными нейронными сетями и очень редко используется на практике. 
Однако он позволит нам получить представление о том, как решается задача классификации изображений.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Пример набора данных для классификации изображений: CIFAR-10&lt;/strong&gt;.&lt;br&gt;
Одним из популярных наборов данных для классификации изображений является &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;набор данных CIFAR-10&lt;/a&gt;. 
Этот набор данных состоит из 60 000 крошечных изображений высотой и шириной 32 пикселя. 
Каждое изображение относится к одному из 10 классов: самолет, автомобиль, птица и т. д. 
Эти 60 000 изображений разделены на обучающую выборку из 50 000 изображений и тестовую выборку из 10 000 изображений. 
На изображении ниже вы можете увидеть 10 случайных примеров изображений для каждого класса.    &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn.jpg"&gt;  &lt;/p&gt;
&lt;p&gt;Слева: примеры изображений из набора данных CIFAR-10.&lt;br&gt;
Справа: в первом столбце показаны несколько тестовых изображений.  &lt;/p&gt;
&lt;p&gt;Рядом с каждым изображением изображены 10 наиболее похожих изображений из обучающей выборки. &lt;/p&gt;
&lt;p&gt;Изначально, у нас есть обучающая выборка CIFAR-10 из 50 000 изображений (по 5000 изображений для каждой из 10 категорий). 
Мы хотим классифицировать оставшиеся 10 000 изображений. 
Классификатор ближайших соседей работает следующим образом. 
Берется тестовое изображение и сравается с каждым изображением из обучающей выборки. 
Будем считать, что метка тестового изображения будет такой же, как и у самого похожего на него изображения.  &lt;/p&gt;
&lt;p&gt;На изображении выше и справа вы можете увидеть пример результата такой процедуры для 10 тестовых изображений. 
Обратите внимание, что только в 3 из 10 изображений являются элементами того же класса, в то время как в остальных 7 примерах возникает ошибка определения класса. 
Например, в 8-м ряду ближайшим обучающим изображением к голове лошади является красный автомобиль, предположительно из-за сильного чёрного фона. 
В результате этого, изображение лошади в данном случае будет ошибочно помечено как автомобиль. &lt;/p&gt;
&lt;p&gt;Мы не уточнили, как именно мы сравниваем два изображения. 
Технически изображения представляют собой просто два блока (тензора) размером 32 x 32 x 3. 
Один из самых простых способов — сравнивать изображения попиксельно и суммировать все разности. 
Другими словами, если у вас есть два изображения, представленные в виде векторов $I_1$, $I_2$, разумным выбором для их сравнения может быть &lt;strong&gt;расстояние L1&lt;/strong&gt;:   &lt;/p&gt;
&lt;p&gt;$$
d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|
$$   &lt;/p&gt;
&lt;p&gt;Сумма берется по всем пикселям. 
Вот как выглядит эта процедура:   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nneg.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Пример использования попиксельных различий для сравнения двух изображений с помощью расстояния $L_1$ (в данном примере для одного цветового канала). 
Два изображения вычитаются поэлементно, а затем все различия суммируются до получения одного числа. 
Если два изображения идентичны, результат будет равен нулю. 
Но если изображения сильно отличаются, результат будет большим.   &lt;/p&gt;
&lt;p&gt;Давайте также посмотрим, как можно реализовать классификатор в коде. Сначала загрузим данные CIFAR-10 в память в виде четырех массивов: обучающие данные/метки и тестовые данные/метки. В приведенном ниже коде &lt;code&gt;Xtr&lt;/code&gt; хранятся все изображения из обучающей выборки  (объем 50 000 x 32 x 32 x 3), а соответствующий одномерный массив &lt;code&gt;Ytr&lt;/code&gt; (длиной 50 000) содержит обучающие метки (от 0 до 9):   &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xte&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Yte&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_CIFAR10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data/cifar10/'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# a magic function we provide&lt;/span&gt;
&lt;span class="c1"&gt;# flatten out all images to be one-dimensional &lt;/span&gt;
&lt;span class="n"&gt;Xtr_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Xtr_rows becomes 50000 x 3072&lt;/span&gt;
&lt;span class="n"&gt;Xte_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Xte_rows becomes 10000 x 3072 &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Теперь, когда все изображения вытянуты в ряд, мы можем обучить и оценить классификатор:   &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;nn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NearestNeighbor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# create a Nearest Neighbor classifier class&lt;/span&gt;
&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr_rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# train the classifier on the training images and labels&lt;/span&gt;
&lt;span class="n"&gt;Yte_predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xte_rows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# predict labels on the test images&lt;/span&gt;
&lt;span class="c1"&gt;# and now print the classification accuracy, which is the average number&lt;/span&gt;
&lt;span class="c1"&gt;# of examples that are correctly predicted (i.e. label matches)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'accuracy: &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Yte_predict&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Yte&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что в качестве критерия оценки обычно используется метрика &lt;strong&gt;accuracy&lt;/strong&gt;, 
Эта метрика измеряет долю правильных прогнозов в тестовой выборке. 
Обратите внимание, что все классификаторы, которые мы создадим, имеют общий интерфейс (API). 
У них есть метод &lt;code&gt;train(X,y)&lt;/code&gt;, который принимает на вход данные и метки для обучения. 
Внутри класса должна быть построена своего рода модель, которая предсказывает метки на основе данных. 
Метод &lt;code&gt;predict(X)&lt;/code&gt; принимает новые данные и предсказывает метки. &lt;/p&gt;
&lt;p&gt;Пример реализации простого классификатора ближайшего соседа с расстоянием $L_1$, который реализует интерфейс классификатора:   &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;NearestNeighbor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;""" X is N x D where each row is an example. Y is 1-dimension of size N&lt;/span&gt;
&lt;span class="sd"&gt;        The nearest neighbor classifier simply remembers all the training data """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ytr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;""" X is N x D where each row is an example we wish to predict label for """&lt;/span&gt;
        &lt;span class="n"&gt;num_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="c1"&gt;# lets make sure that the output type matches the input type&lt;/span&gt;
        &lt;span class="n"&gt;Ypred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ytr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# loop over all test rows&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# find the nearest training image to the i'th test image&lt;/span&gt;
        &lt;span class="c1"&gt;# using the L1 distance (sum of absolute value differences)&lt;/span&gt;
        &lt;span class="n"&gt;distances&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;min_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distances&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# get the index with smallest distance&lt;/span&gt;
        &lt;span class="n"&gt;Ypred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ytr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;min_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# predict the label of the nearest example&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Ypred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Если вы запустите этот код, то увидите, что классификатор достигает точности &lt;strong&gt;38,6%&lt;/strong&gt; на тестовой выборке CIFAR-10. 
Это более впечатляющий результат, чем случайное угадывание (которое дало бы &lt;strong&gt;10%&lt;/strong&gt; точности для 10 классов). 
Но он далёк от результатов человека, которые &lt;a href="https://karpathy.github.io/2011/04/27/manually-classifying-cifar10/"&gt;оцениваются примерно в 94%&lt;/a&gt;.  Еще лучший результат можно получить с помощью свёрточных нейронных сетей, которые достигают примерно 95% (см. таблицу соревнования &lt;a href="https://www.kaggle.com/c/cifar-10/leaderboard"&gt;Kaggle&lt;/a&gt; по CIFAR-10).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Выбор расстояния&lt;/strong&gt;.&lt;br&gt;
Существует множество других способов вычисления расстояний между векторами. 
Одним из распространённых вариантов может быть использование &lt;strong&gt;расстояния L_2&lt;/strong&gt;, которое имеет геометрическую интерпретацию вычисления евклидова расстояния между двумя векторами. 
Формула для вычисления этого расстояния имеет вид:  &lt;/p&gt;
&lt;p&gt;$$
d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}
$$  &lt;/p&gt;
&lt;p&gt;Другими словами, мы вычисляем разницу по пикселям, как и раньше, но на этот раз возводим их в квадрат, складываем и, наконец, извлекаем квадратный корень. 
Используя приведенный выше код с numpy, нам нужно заменить только одну строку:   &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;distances&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание на вычисление корня в функции &lt;code&gt;np.sqrt&lt;/code&gt;. 
В практической реализации метода ближайшего соседа мы могли бы не использовать операцию извлечения квадратного корня, потому что он является &lt;em&gt;монотонной функцией&lt;/em&gt;. 
То есть он масштабирует абсолютные значения расстояний, но сохраняет порядок. 
Поэтому с ним или без него, ближайшие соседи будут идентичны. 
Однако если применить классификатор ближайшего соседа к CIFAR-10 с L2 расстоянием, получится всего &lt;strong&gt;35,4%&lt;/strong&gt; точности. 
Это немного ниже, чем результат с расстоянием &lt;strong&gt;$L_1$&lt;/strong&gt;.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Расстояние $L_1$ против $L_2$ .&lt;/strong&gt;&lt;br&gt;
Различие между этими двумя метриками в том, что расстояние &lt;strong&gt;$L_2$&lt;/strong&gt; более строгое, чем расстояние &lt;strong&gt;$L_1$&lt;/strong&gt;. 
Это значит, что если имеется множество небольших расхождений и всего одно большое, расстояние $L_2$ будет больше, чем $L_1$ . 
Понятия расстояний &lt;strong&gt;$L_1$&lt;/strong&gt; и  &lt;strong&gt;$L_2$&lt;/strong&gt;  эквивалентно нормам, которые являются частными случаями &lt;a href="https://planetmath.org/vectorpnorm"&gt;p-нормы&lt;/a&gt;.   &lt;/p&gt;
&lt;h3&gt;Классификатор k - ближайших соседей&lt;/h3&gt;
&lt;p&gt;когда мы хотим сделать более точный прогноз, нам не обязательно использовать только одну метку ближайшего изображения. 
Действительно, почти всегда можно добиться лучшего результата, используя так называемый &lt;strong&gt;классификатор k-ближайших соседей&lt;/strong&gt;. 
Идея очень проста: вместо того, чтобы искать одно ближайшее изображение в обучающем наборе, мы найдём &lt;strong&gt;k&lt;/strong&gt; ближайших изображений 
Дальше мы сравним их и устроим "голосование" за метку тестового изображения. 
В частности, когда &lt;em&gt;k = 1&lt;/em&gt;, мы получаем классификатор ближайшего соседа. 
Интуитивно понятно, что чем больше значений &lt;strong&gt;k&lt;/strong&gt; мы возьмем, тем больше будет сглаживающий эффект. 
Это первый пример гиперпараметра, который делает классификатор более устойчивым к ошибкам:   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/knn.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Пример разницы между классификатором «один ближайший сосед» и классификатором «ближайшие 5 соседей» с использованием двумерных точек и 3 классов (красный, синий, зелёный). 
Цветные области показывают &lt;strong&gt;границы решений&lt;/strong&gt;, создаваемые классификатором с использованием расстояния  $L_2$. 
Белые области показывают точки, которые классифицируются неоднозначно - голоса за классы равны как минимум для двух классов. 
Обратите внимание, что в случае классификатора 1-соседа ошибки создают небольшие островки вероятных неверных прогнозов. 
Например, зелёная точка в середине облака синих точек. 
В это же время классификатор 5-соседей сглаживает эти неровности, что, приводит к лучшему &lt;strong&gt;обобщению&lt;/strong&gt; на тестовых данных. 
Также обратите внимание, что серые области на изображении 5-соседей вызваны равенством голосов ближайших соседей. 
Например, 2 соседа красные, следующие два соседа синие, последний сосед зелёный.  &lt;/p&gt;
&lt;p&gt;На практике почти всегда используется метод k-ближайших соседей. 
Но какое значение k следует использовать? 
Давайте рассмотрим этот вопрос поподробнее.   &lt;/p&gt;
&lt;h3&gt;Наборы данных и настройки гиперпараметров&lt;/h3&gt;
&lt;p&gt;Классификатор k-ближайших соседей требует настройки параметра &lt;em&gt;k&lt;/em&gt;. 
Интуитивно можно предположить, что существует число, которое подходит лучше всего. 
Кроме того, мы увидели, что существует множество различных функций расстояния, которые мы могли бы использовать: норма  &lt;strong&gt;$L_1$&lt;/strong&gt;, норма &lt;strong&gt;$L_2$&lt;/strong&gt;, а также множество других вариантов, которые мы даже не рассматривали (например, скалярные произведения). 
Эти варианты называются &lt;strong&gt;гиперпараметрами&lt;/strong&gt;, и они очень часто используются при разработке многих алгоритмов машинного обучения, которые обучаются на данных. 
Часто не очевидно, какие значения/настройки следует выбрать.   &lt;/p&gt;
&lt;p&gt;У вас может возникнуть соблазн предложить попробовать множество различных значений и посмотреть, что работает лучше всего. 
Это хорошая идея, и именно это мы и сделаем, но делать это нужно очень осторожно. 
В частности, &lt;strong&gt;мы не можем использовать тестовый набор данных для настройки гиперпараметров&lt;/strong&gt;. 
Всякий раз, когда вы разрабатываете алгоритмы машинного обучения, вы должны относиться к тестовому набору данных как к очень ценному ресурсу, к которому, в идеале, не следует прикасаться до самого конца. 
В противном случае существует реальная опасность того, что вы настроите гиперпараметры так, чтобы они хорошо работали на тестовом наборе данных, но при развёртывании модели показывали значительное снижение производительности. 
На практике можно сказать, что произошло &lt;strong&gt;переобучение&lt;/strong&gt; на тестовом наборе данных. 
С другой стороны, если вы настраиваете гиперпараметры на тестовом наборе данных, вы фактически используете тестовый набор данных в качестве обучающего. 
По этой причине точность, которую вы достигаете на нём, будет слишком оптимистичной по сравнению с тем, что будет наблюдаться на реальных данных при развёртывании модели. 
Но если вы используете тестовый набор данных только один раз в конце, он остаётся хорошим показателем для того, чтобы измерить степень &lt;strong&gt;обобщения&lt;/strong&gt; вашего классификатора.   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Итого: Оценивайте модель на тестовой выборке только один раз, в самом конце!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Существует правильный способ настройки гиперпараметров, который никак не затрагивает тестовый набор данных. 
Идея состоит в том, чтобы разделить обучающую выборку на две части: немного меньший обучающий набор и то, что называется &lt;strong&gt;выборкой для валидации&lt;/strong&gt;. 
Используя в качестве примера CIFAR-10, мы могли бы использовать 49 000 обучающих изображений для обучения и оставить 1000 для валидации. 
Этот набор данных по сути используется для настройки гиперпараметров.   &lt;/p&gt;
&lt;p&gt;Вот как это может выглядеть в случае CIFAR-10:   &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before&lt;/span&gt;
&lt;span class="c1"&gt;# recall Xtr_rows is 50,000 x 3072 matrix&lt;/span&gt;
&lt;span class="n"&gt;Xval_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xtr_rows&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="c1"&gt;# take first 1000 for validation&lt;/span&gt;
&lt;span class="n"&gt;Yval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Xtr_rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xtr_rows&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="c1"&gt;# keep last 49,000 for train&lt;/span&gt;
&lt;span class="n"&gt;Ytr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="c1"&gt;# find hyperparameters that work best on the validation set&lt;/span&gt;
&lt;span class="n"&gt;validation_accuracies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

&lt;span class="c1"&gt;# use a particular value of k and evaluation on validation data&lt;/span&gt;
&lt;span class="n"&gt;nn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NearestNeighbor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr_rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# here we assume a modified NearestNeighbor class that can take a k as input&lt;/span&gt;
&lt;span class="n"&gt;Yval_predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xval_rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Yval_predict&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Yval&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'accuracy: &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;

&lt;span class="c1"&gt;# keep track of what works on the validation set&lt;/span&gt;
&lt;span class="n"&gt;validation_accuracies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;По завершении этой процедуры построим график, который показывает, какие значения k работают лучше всего. 
Затем мы остановимся на этом значении и проведем оценку на реальном тестовом наборе данных.   &lt;/p&gt;
&lt;p&gt;Разделите обучающую выборку на обучающую и валидационную. 
Используйте проверочную выборку для настройки всех гиперпараметров. 
В конце выполните один запуск на тестовой выборке и оцените производительность.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Кросс-валидация&lt;/strong&gt;&lt;br&gt;
В случаях, когда размер обучающих данных (и, следовательно, проверочных данных) может быть небольшим, люди иногда используют более сложный метод настройки гиперпараметров, называемый &lt;strong&gt;Кросс-валидацией&lt;/strong&gt;. 
Если вернуться к нашему предыдущему примеру, то идея заключается в том, что вместо произвольного выбора первых 1000 точек данных в качестве проверочного набора, а остальных — в качестве обучающего, можно получить более точную и менее зашумлённую оценку того, насколько хорошо работает определённое значение &lt;strong&gt;k&lt;/strong&gt;, перебирая различные проверочные наборы и усредняя результаты по ним. Например, при 5-кратной перекрёстной проверке мы разделили бы обучающие данные на 5 равных частей, использовали 4 из них для обучения, а 1 — для проверки. Затем мы бы определили, какая из выборок является контрольной, оценили бы производительность и, наконец, усреднили бы производительность по разным выборкам.   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cvplot.png"&gt;  &lt;/p&gt;
&lt;p&gt;Пример 5-кратного выполнения перекрестной проверки для параметра &lt;strong&gt;k&lt;/strong&gt;. Для каждого значения &lt;strong&gt;k&lt;/strong&gt; мы тренируемся на 4 сгибах и оцениваем на 5-м. Следовательно, для каждого k мы получаем 5 значений точности для проверочного сгиба (точность отражается на оси y, и каждый результат равен точке). Линия тренда проводится через среднее значение результатов для каждого &lt;strong&gt;k&lt;/strong&gt;, а столбики ошибок указывают на стандартное отклонение. Обратите внимание, что в данном конкретном случае перекрёстная проверка показывает, что значение около &lt;strong&gt;k = 7&lt;/strong&gt; лучше всего подходит для этого конкретного набора данных (соответствует пику на графике). Если бы мы использовали более 5 циклов, то могли бы ожидать более плавную (то есть менее шумную) кривую.   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;На практике люди предпочитают избегать перекрёстной проверки в пользу одного проверочного набора данных, поскольку перекрёстная проверка может быть ресурсозатратной. Обычно люди используют от &lt;strong&gt;50%&lt;/strong&gt; до &lt;strong&gt;90%&lt;/strong&gt; обучающих данных для обучения и остальную часть для проверки. Однако это зависит от множества факторов: например, если количество гиперпараметров велико, вы можете предпочесть использовать более крупные проверочные наборы данных. Если количество примеров в проверочном наборе невелико (возможно, всего несколько сотен или около того), безопаснее использовать перекрёстную проверку. На практике обычно используется 3-кратная, 5-кратная или 10-кратная перекрёстная проверка.   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/crossval.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Обычное разделение данных. Выделяются обучающий и тестовый наборы данных. Обучающий набор данных делится на части (например, здесь их 5). Части 1-4 становятся обучающим набором данных. Одна часть (например, часть 5, выделенная здесь жёлтым цветом) называется проверочной частью и используется для настройки гиперпараметров. Перекрёстная проверка идёт дальше и позволяет выбрать, какая часть будет проверочной, отдельно от частей 1-5. Это называется 5-кратной перекрёстной проверкой. В самом конце, когда модель обучена и определены все наилучшие гиперпараметры, модель один раз оценивается на тестовых данных (красный цвет).   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Плюсы и минусы классификатора ближайших соседей.&lt;/strong&gt;   &lt;/p&gt;
&lt;p&gt;Стоит рассмотреть некоторые преимущества и недостатки классификатора «ближайший сосед». 
Очевидно, что одним из преимуществ является простота реализации и понимания. 
Кроме того, обучение классификатора не занимает много времени, поскольку всё, что требуется, — это хранить и, возможно, индексировать обучающие данные. 
Однако мы платим за это вычислительными затратами во время тестирования, поскольку для классификации тестового примера требуется сравнение с каждым обучающим примером. 
Это неправильно, поскольку на практике мы часто уделяем больше внимания эффективности во время тестирования, чем во время обучения. 
На самом деле, объемные нейронные сети, которые мы будем разрабатывать в этом классе, смещают этот компромисс в другую крайность: их обучение обходится очень дорого, но после завершения обучения классифицировать новый тестовый пример очень дёшево. 
Такой режим работы гораздо более желателен на практике.   &lt;/p&gt;
&lt;p&gt;Кроме того, вычислительная сложность классификатора «ближайший сосед» является активной областью исследований, и существует несколько алгоритмов и библиотек &lt;strong&gt;приблизительного поиска ближайшего соседа&lt;/strong&gt; (&lt;em&gt;ANN&lt;/em&gt;), которые могут ускорить поиск ближайшего соседа в наборе данных (например, &lt;a href="https://github.com/mariusmuja/flann"&gt;FLANN&lt;/a&gt; ). 
Эти алгоритмы позволяют найти компромисс между точностью поиска ближайшего соседа и его пространственной/временной сложностью во время поиска и обычно полагаются на этап предварительной обработки/индексирования, который включает в себя построение KD-дерева или запуск алгоритма k-средних.  &lt;/p&gt;
&lt;p&gt;В некоторых случаях классификатор ближайших соседей может быть хорошим выбором (особенно если данные имеют низкую размерность), но он редко подходит для использования в практических задачах классификации изображений. 
Одна из проблем заключается в том, что изображения — это объекты с высокой размерностью (то есть они часто содержат много пикселей), а расстояния в многомерных пространствах могут быть очень нелогичными. 
На изображении ниже показано, что сходство на основе пикселей, которое мы описали выше, сильно отличается от сходства с точки зрения восприятия:   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/samenorm.png"&gt;   &lt;/p&gt;
&lt;p&gt;Расстояния на основе пикселей в многомерных данных (и особенно в изображениях) могут быть очень неинтуитивными. 
Исходное изображение (слева) и три других изображения рядом с ним, которые находятся на одинаковом расстоянии от него на основе пиксельного расстояния &lt;strong&gt;$L_2$&lt;/strong&gt;. 
Очевидно, что пиксельное расстояние никак не соответствует перцептивному или семантическому сходству.   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Вот ещё одна визуализация, которая убедит вас в том, что использование разницы в пикселях для сравнения изображений недостаточно. Мы можем использовать метод визуализации под названием &lt;a href="https://lvdmaaten.github.io/tsne/"&gt;t-SNE&lt;/a&gt;, чтобы взять изображения CIFAR-10 и разместить их в двух измерениях так, чтобы их  парные (локальные) расстояния сохранялись наилучшим образом. В этой визуализации изображения, которые показаны рядом, считаются очень близкими в соответствии с расстоянием &lt;strong&gt;$L_2$&lt;/strong&gt; по пикселям, которое мы разработали выше:   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/pixels_embed_cifar10.jpg"&gt;  &lt;/p&gt;
&lt;p&gt;Изображения CIFAR-10, размещённые в двух измерениях с помощью &lt;em&gt;t-SNE&lt;/em&gt;. Изображения, расположенные рядом на этом изображении, считаются близкими на основе пиксельного расстояния &lt;strong&gt;$L_2$&lt;/strong&gt;. 
Обратите внимание на сильное влияние фона, а не семантических различий между классами. 
Нажмите &lt;a href="https://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg"&gt;здесь&lt;/a&gt;, чтобы увидеть увеличенную версию этой визуализации.   &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;В частности, обратите внимание, что изображения, расположенные друг рядом с другом, в большей степени зависят от общего цветового распределения изображений или типа фона, а не от их семантической идентичности. Например, собаку можно увидеть рядом с лягушкой, потому что они обе находятся на белом фоне. В идеале мы хотели бы, чтобы изображения всех 10 классов образовывали собственные кластеры, чтобы изображения одного класса находились рядом друг с другом независимо от нерелевантных характеристик и вариаций (например, фона). Однако, чтобы добиться этого, нам придётся выйти за рамки необработанных пикселей.   &lt;/p&gt;
&lt;h3&gt;Применение kNN на практике&lt;/h3&gt;
&lt;p&gt;Подводя итог: 
- Мы рассмотрели задачу &lt;strong&gt;классификации изображений&lt;/strong&gt;, в которой нам даётся набор изображений, каждое из которых помечено одной категорией. Затем нас просят предсказать эти категории для нового набора тестовых изображений и оценить точность прогнозов.
- Мы представили простой классификатор под названием &lt;em&gt;«классификатор ближайших соседей»&lt;/em&gt;.  Мы увидели, что существует множество гиперпараметров (например, значение k или тип расстояния, используемого для сравнения примеров), связанных с этим классификатором, и что не существует очевидного способа их выбора.
- Мы увидели, что правильный способ задать эти гиперпараметры — разделить обучающие данные на две части: &lt;em&gt;обучающий набор&lt;/em&gt; и &lt;em&gt;поддельный тестовый набор&lt;/em&gt;, который мы называем &lt;strong&gt;набором для проверки&lt;/strong&gt;. Мы пробуем разные значения гиперпараметров и оставляем те, которые обеспечивают наилучшую производительность на наборе для проверки.
- Если вас беспокоит нехватка обучающих данных, мы обсудили процедуру под названием &lt;strong&gt;перекрёстная проверка&lt;/strong&gt;, которая может помочь уменьшить погрешность при оценке наиболее эффективных гиперпараметров.
- Как только мы находим оптимальные гиперпараметры, мы фиксируем их и проводим одну &lt;strong&gt;оценку&lt;/strong&gt; на реальном тестовом наборе данных.
- Мы увидели, что метод ближайшего соседа может обеспечить нам точность около &lt;strong&gt;40%&lt;/strong&gt; на CIFAR-10. Он прост в реализации, но требует хранения всего обучающего набора данных, и его сложно оценивать на тестовых изображениях.
- В итоге мы увидели, что использование расстояний &lt;strong&gt;$L_1$&lt;/strong&gt; или &lt;strong&gt;$L_2\&lt;/strong&gt;) по необработанным значениям пикселей нецелесообразно, поскольку эти расстояния сильнее коррелируют с фоном и цветовыми распределениями изображений, чем с их семантическим содержанием.   &lt;/p&gt;
&lt;p&gt;На следующих лекциях мы приступим к решению этих задач и в конечном итоге придём к решениям, которые обеспечат точность &lt;strong&gt;90%&lt;/strong&gt;, 
позволят полностью отказаться от обучающего набора данных после завершения обучения и позволят оценивать тестовые изображения менее чем за миллисекунду.   &lt;/p&gt;
&lt;p&gt;Если вы хотите применить &lt;em&gt;kNN&lt;/em&gt; на практике (не на изображениях), действуйте следующим образом:  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Предварительная обработка данных.&lt;/strong&gt; 
Нормализуйте признаки в ваших данных (например, один пиксель на изображениях), чтобы среднее значение было равно нулю, а дисперсия — единице. 
Мы рассмотрим этот прием более подробно в следующих разделах. 
Сейчас нормализация данных не используется, потому что распределение яркости пикселей на изображениях достаточно однородны.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Рассмотрите возможность снижения размерности данных&lt;/strong&gt;. 
На практике для снижения размерности используются следующие методы: &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;метод главных компонент &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;ссылка на вики-страницу&lt;/a&gt;, &lt;a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf"&gt;ссылка на CS229&lt;/a&gt;, &lt;a href="https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/"&gt;ссылка на блог&lt;/a&gt;, &lt;/li&gt;
&lt;li&gt;метод независимых компонент &lt;a href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis"&gt;ссылка на вики-страницу&lt;/a&gt;, &lt;a href="https://kevinzakka.github.io/2020/02/10/nca/"&gt;ссылка на блог&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/modules/random_projection.html"&gt;случайные проекции&lt;/a&gt;.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Разделите обучающие данные случайным образом на обучающую и проверочную (валидационную) выборки.&lt;/strong&gt; 
Как правило, в обучающую выборку попадает от &lt;strong&gt;70&lt;/strong&gt; до &lt;strong&gt;90%&lt;/strong&gt; данных. 
Этот параметр зависит от того, сколько у вас гиперпараметров и насколько сильно они влияют на результат. 
Если нужно оценить множество гиперпараметров, лучше использовать более крупную проверочную выборку для их эффективной оценки. 
Если вас беспокоит размер проверочной выборки, лучше разделить обучающие данные на части и выполнить кросс-валидацию. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Обучите и оцените классификатор kNN на кросс-валидации.&lt;/strong&gt; 
По возможности выполняйте кросс-валидацию для множества вариантов k и для разных типов расстояний (&lt;strong&gt;$L_1$ и $L_2$&lt;/strong&gt; — хорошие кандидаты).&lt;br&gt;
Если вы можете позволить себе потратить больше времени на вычисления, всегда безопаснее использовать кросс-валидацию. 
Чем больше циклов обучения пройдет, тем лучше, но тем дороже с точки зрения вычислений.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Оцените задержку классификатора.&lt;/strong&gt; 
Если ваш классификатор kNN работает слишком долгo, рассмотрите возможность использования библиотеки приближённых ближайших соседей. 
Например, библиотека &lt;a href="https://github.com/mariusmuja/flann"&gt;FLANN&lt;/a&gt; позволяет ускорить поиск за счёт некоторой потери точности.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Обратите внимание на гиперпараметры, которые дали наилучшие результаты.&lt;/strong&gt; 
Возникает вопрос, следует ли использовать валидационный набор для финального обучения с наилучшими гиперпараметрами. 
Дело в том, что добавить данные для валидации в набор обучающих данных, оптимальные гиперпараметры могут измениться, поскольку размер данных увеличится. 
На практике лучше не использовать данные валидации в итоговом классификаторе и считать их потерянными при оценке гиперпараметров. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Оцените наилучшую модель на тестовом наборе данных.&lt;/strong&gt; 
Вычислите точность на тестовой выборке и объявите результат производительностью классификатора &lt;em&gt;kNN&lt;/em&gt; на ваших данных.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Дополнительные материалы&lt;/h3&gt;
&lt;p&gt;Вот несколько дополнительных ссылок, которые могут быть интересными для дальнейшего чтения:&lt;br&gt;
- &lt;a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"&gt;Несколько полезных фактов о машинном обучении&lt;/a&gt;, особенно раздел 6, но рекомендуется к прочтению вся статья.&lt;br&gt;
- &lt;a href="https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html"&gt;Распознавание и изучение категорий объектов&lt;/a&gt;, краткий курс по категоризации объектов на ICCV 2005.  &lt;/p&gt;</description><category>CV</category><guid>https://mldl.ru/posts/image-classification/</guid><pubDate>Fri, 14 Mar 2025 05:00:00 GMT</pubDate></item><item><title>Понимание и визуализация</title><link>https://mldl.ru/posts/visualisation/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Понимание и визуализация&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;(эта страница в настоящее время находится в черновом варианте)&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;В литературе было разработано несколько подходов к пониманию и визуализации сверточных сетей, отчасти в ответ на распространенную критику о том, что изученные признаки в нейронной сети не поддаются интерпретации. В этом разделе мы кратко рассмотрим некоторые из этих подходов и связанную с ними работу.&lt;/p&gt;
&lt;h3&gt;Визуализация активаций и веса первого слоя&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Активации слоев&lt;/strong&gt;. Наиболее простой метод визуализации заключается в том, чтобы показать активации сети во время прямого прохода. В сетях &lt;em&gt;ReLU&lt;/em&gt; активации обычно выглядят относительно неровными и плотными, но по мере обучения активации обычно становятся более редкими и локализованными. Одна из опасных ловушек, которую можно легко заметить с помощью этой визуализации, заключается в том, что некоторые карты активации могут быть равны нулю для множества различных входных данных, что может указывать на &lt;em&gt;мертвые фильтры&lt;/em&gt; и может быть симптомом высокой скорости обучения.    &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/act1.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/act2.jpeg"&gt;&lt;br&gt;
Типичные активации на первом слое &lt;em&gt;CONV&lt;/em&gt; (&lt;strong&gt;сверху&lt;/strong&gt;) и на 5-м слое &lt;em&gt;CONV&lt;/em&gt; (снизу) обученного &lt;em&gt;AlexNet&lt;/em&gt; смотрят на изображение кошки. В каждом боксе отображается карта активации, соответствующая какому-либо фильтру. Обратите внимание, что активации редкие (большинство значений равны нулю, на этой визуализации показаны черным цветом) и в основном локальные.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Фильтры Conv/FC&lt;/strong&gt;. Вторая распространенная стратегия заключается в визуализации весов. Обычно они наиболее интерпретируемы на первом слое &lt;em&gt;CONV&lt;/em&gt;, который смотрит непосредственно на необработанные пиксельные данные, но также можно показать веса фильтров в более глубоких слоях сети. Весовые коэффициенты полезны для визуализации, потому что хорошо обученные сети обычно отображают красивые и плавные фильтры без каких-либо зашумленных узоров. Зашумленные паттерны могут быть индикатором сети, которая не обучалась достаточно долго, или, возможно, очень низкой интенсивности регуляризации, которая могла привести к переобучению.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/filt1.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/filt2.jpeg"&gt;&lt;br&gt;
Типичные фильтры на первом слое &lt;em&gt;CONV&lt;/em&gt; (&lt;strong&gt;сверху&lt;/strong&gt;) и на 2-м слое &lt;em&gt;CONV&lt;/em&gt; (&lt;strong&gt;снизу&lt;/strong&gt;) обученного &lt;em&gt;AlexNet&lt;/em&gt;. Обратите внимание, что веса первого слоя очень красивые и гладкие, что указывает на хорошо сходящуюся сеть. Функции цвета/оттенков серого сгруппированы, потому что &lt;em&gt;AlexNet&lt;/em&gt; содержит два отдельных потока обработки, и очевидным следствием такой архитектуры является то, что один поток развивает высокочастотные элементы оттенков серого, а другой — низкочастотные цветовые функции. Веса 2-го слоя &lt;em&gt;CONV&lt;/em&gt; не так легко интерпретируемы, но очевидно, что они все еще гладкие, хорошо сформированные и лишены зашумленных узоров.  &lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Получение изображений, которые максимально активируют нейрон&lt;/h3&gt;
&lt;p&gt;Еще один метод визуализации заключается в том, чтобы взять большой набор изображений, пропустить их через сеть и отслеживать, какие изображения максимально активируют тот или иной нейрон. Затем мы можем визуализировать изображения, чтобы понять, что нейрон ищет в своем рецептивном поле. Одна из таких визуализаций (среди прочих) показана в &lt;a href="http://arxiv.org/abs/1311.2524"&gt;статье Богатые иерархии функций для точного обнаружения объектов и семантической сегментации&lt;/a&gt; Росса Гиршика и др.:&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/pool5max.jpeg"&gt;&lt;br&gt;
Максимально активизирующие изображения для некоторых нейронов &lt;em&gt;POOL5&lt;/em&gt; (5-й слой пула) AlexNet. Значения активации и рецептивное поле конкретного нейрона показаны белым цветом. (В частности, обратите внимание, что нейроны &lt;em&gt;POOL5&lt;/em&gt; являются функцией относительно большой части входного изображения!) Можно видеть, что некоторые нейроны реагируют на верхнюю часть тела, текст или зеркальные блики.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Одна из проблем с этим подходом заключается в том, что нейроны &lt;em&gt;ReLU&lt;/em&gt; не обязательно имеют какое-либо семантическое значение сами по себе. Скорее, более уместно думать о множественных нейронах &lt;em&gt;ReLU&lt;/em&gt; как о базисных векторах некоторого пространства, представленного в виде участков изображения. Другими словами, визуализация показывает участки на краю облака представлений, вдоль (произвольных) осей, которые соответствуют весам фильтра. Это также можно увидеть по тому факту, что нейроны в ConvNet работают линейно над входным пространством, поэтому любое произвольное вращение этого пространства является запретным. Этот момент был далее аргументирован в книге Сегеди и др. &lt;a href="http://arxiv.org/abs/1312.6199"&gt;«Интригующие свойства нейронных сетей»&lt;/a&gt;, где они выполняют аналогичную визуализацию вдоль произвольных направлений в пространстве представления.&lt;/p&gt;
&lt;h3&gt;Встраивание кодов с помощью t-SNE&lt;/h3&gt;
&lt;p&gt;ConvNet можно интерпретировать как постепенное преобразование изображений в представление, в котором классы разделяются линейным классификатором. Мы можем получить приблизительное представление о топологии этого пространства, встроив изображения в два измерения таким образом, чтобы их низкоразмерное представление имело примерно равные расстояния, чем их высокомерное представление. Существует множество методов вложения, которые были разработаны с помощью интуиции вложения векторов высокой размерности в пространство низкой размерности с сохранением парных расстояний точек. Среди них &lt;a href="http://lvdmaaten.github.io/tsne/"&gt;t-SNE&lt;/a&gt; является одним из самых известных методов, который неизменно дает визуально приятные результаты.  &lt;/p&gt;
&lt;p&gt;Чтобы произвести встраивание, мы можем взять набор изображений и использовать ConvNet для извлечения кодов &lt;em&gt;CNN&lt;/em&gt; (например, в &lt;em&gt;AlexNet&lt;/em&gt; 4096-мерный вектор прямо перед классификатором, и, что особенно важно, включая нелинейность &lt;em&gt;ReLU&lt;/em&gt;). Затем мы можем подключить их к &lt;em&gt;t-SNE&lt;/em&gt; и получить двумерный вектор для каждого изображения. Соответствующие изображения могут быть визуализированы в виде сетки:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/tsne.jpeg"&gt;&lt;br&gt;
Встраивание набора изображений в &lt;em&gt;t-SNE&lt;/em&gt; на основе их кодов &lt;em&gt;CNN&lt;/em&gt;. Изображения, которые находятся рядом друг с другом, также близки в пространстве репрезентации &lt;em&gt;CNN&lt;/em&gt;, что подразумевает, что &lt;em&gt;CNN&lt;/em&gt; «видит» их как очень похожие. Обратите внимание, что сходства чаще всего основаны на классах и семантике, а не на пикселях и цветах. Для получения более подробной информации о том, как была создана эта визуализация, связанный код, а также другие связанные визуализации в разных масштабах см. &lt;a href="http://cs.stanford.edu/people/karpathy/cnnembed/"&gt;Визуализация кодов CNN в t-SNE&lt;/a&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Окклюзия частей изображения&lt;/h3&gt;
&lt;p&gt;Предположим, что &lt;em&gt;ConvNet&lt;/em&gt; классифицирует изображение как собаку. Как мы можем быть уверены, что он на самом деле улавливает собаку на изображении, а не какие-то контекстуальные подсказки на фоне или какой-то другой объект? Одним из способов исследования того, из какой части изображения исходит предсказание классификации, является построение графика вероятности интересующего класса (например, класса собаки) в зависимости от положения объекта-окклюдера. То есть, мы перебираем области изображения, устанавливаем участок изображения равным нулю и смотрим на вероятность класса. Мы можем визуализировать вероятность в виде двумерной тепловой карты. Этот подход был использован в книге Мэтью Цайлера &lt;a href="http://arxiv.org/abs/1311.2901"&gt;«Визуализация и понимание сверточных сетей»&lt;/a&gt;:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnnvis/occlude.jpeg"&gt;&lt;br&gt;
Три входных изображения (&lt;strong&gt;вверху&lt;/strong&gt;). Обратите внимание, что окклюдерная область показана серым цветом. Когда мы проводим окклюдером по изображению, мы записываем вероятность правильного класса, а затем визуализируем его в виде тепловой карты (&lt;em&gt;показанной под каждым изображением&lt;/em&gt;). Например, на крайнем левом изображении мы видим, что вероятность померанского шпица резко падает, когда окклюдер закрывает морду собаки, что дает нам некоторую степень уверенности в том, что морда собаки в первую очередь ответственна за высокий балл классификации. И наоборот, обнуление других частей изображения имеет относительно незначительное влияние.  &lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Визуализация градиента данных и его друзей&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Градиент данных.|&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1312.6034"&gt;Глубоко внутри сверточных сетей: визуализация моделей классификации изображений и карт заметности&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeconvNet.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1311.2901"&gt;Визуализация и понимание сверточных сетей&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Управляемое обратное распространение.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6806"&gt;Стремление к простоте: Всесвёрточная сеть&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Восстановление оригинальных изображений на основе кодов CNN&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.0035"&gt;Понимание глубоких представлений изображений путем их инвертирования&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Какой объем пространственной информации сохраняется?&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf"&gt;Учатся ли ConvNet переписываться?&lt;/a&gt; (Вкратце: да)&lt;/p&gt;
&lt;h3&gt;Производительность построения графиков в зависимости от атрибутов изображения&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1409.0575"&gt;ImageNet Wide Scale Visual Recognition Challenge&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Обман ConvNet&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6572"&gt;Объяснение и использование состязательных примеров&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Сравнение ConvNet с людьми-маркировщиками&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/"&gt;Что я узнал, соревнуясь с ConvNet на ImageNet&lt;/a&gt;&lt;/p&gt;</description><guid>https://mldl.ru/posts/visualisation/</guid><pubDate>Thu, 13 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Арзитектура нейросетей </title><link>https://mldl.ru/posts/architecture/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Архитектура нейросетей&lt;/h2&gt;
&lt;p&gt;Содержание:
- &lt;a href="https://mldl.ru/posts/architecture/"&gt;Обзор архитектуры&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/architecture/"&gt;Слои ConvNet&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Сверточный слой&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Слой пула&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Слой нормализации&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Полностью подключенный слой&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Преобразование полносвязных слоев в сверточные слои&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/architecture/"&gt;Архитектуры ConvNet&lt;/a&gt;
     - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Узоры слоев&lt;/a&gt;
     - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Шаблоны для определения размеров слоев&lt;/a&gt;
     - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Тематические исследования &lt;/a&gt; (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)
     - &lt;a href="https://mldl.ru/posts/architecture/"&gt;Вычислительные соображения&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/architecture/"&gt;Дополнительные материалы&lt;/a&gt;  &lt;/p&gt;
&lt;h3&gt;Сверточные нейронные сети (CNNs / ConvNets)&lt;/h3&gt;
&lt;p&gt;Сверточные нейронные сети очень похожи на обычные нейронные сети из предыдущей главы: они состоят из нейронов, которые имеют обучаемые веса и смещения. Каждый нейрон получает некоторые входные данные, выполняет скалярное произведение и опционально следует за ним с нелинейностью. Вся сеть по-прежнему выражает одну дифференцируемую функцию оценки: от пикселей необработанного изображения на одном конце до оценок классов на другом. И у них по-прежнему есть функция потерь (например, &lt;em&gt;SVM/Softmax&lt;/em&gt;) на последнем (полностью подключенном) слое, и все советы/рекомендации, которые мы разработали для обучения обычным нейронным сетям, по-прежнему применимы.  &lt;/p&gt;
&lt;p&gt;Так что же меняется? Архитектуры &lt;em&gt;ConvNet&lt;/em&gt; явно предполагают, что входные данные являются изображениями, что позволяет нам закодировать определенные свойства в архитектуре. Это делает функцию &lt;strong&gt;forward&lt;/strong&gt; более эффективной для реализации и значительно сокращает количество параметров в сети.  &lt;/p&gt;
&lt;h4&gt;Обзор архитектуры&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Напомним: обычные нейронные сети&lt;/em&gt;. Как мы видели в предыдущей главе, нейронные сети получают входные данные (один вектор) и преобразуют их через серию &lt;em&gt;скрытых слоев&lt;/em&gt;. Каждый скрытый слой состоит из набора нейронов, где каждый нейрон полностью связан со всеми нейронами предыдущего слоя, и где нейроны в одном слое функционируют совершенно независимо и не имеют общих связей. Последний полносвязный слой называется «выходным слоем» и в настройках классификации представляет собой баллы класса.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Обычные нейронные сети плохо масштабируются до полных изображений&lt;/em&gt;. В CIFAR-10 изображения имеют размер всего &lt;strong&gt;32x32x3&lt;/strong&gt; (&lt;strong&gt;32&lt;/strong&gt; в ширину, &lt;strong&gt;32&lt;/strong&gt; в высоту, &lt;strong&gt;3&lt;/strong&gt; цветных канала), поэтому один полностью связанный нейрон в первом скрытом слое обычной нейронной сети будет иметь &lt;strong&gt;32 * 32 * 3 = 3072&lt;/strong&gt; веса. Это количество все еще кажется управляемым, но очевидно, что эта полностью связанная структура не масштабируется до более крупных изображений. Например, изображение более приличного размера, например, &lt;strong&gt;200x200x3&lt;/strong&gt;, приведет к нейронам с весом &lt;strong&gt;200&lt;em&gt;200&lt;/em&gt;3 = 120 000&lt;/strong&gt;. Более того, мы почти наверняка хотели бы иметь несколько таких нейронов, чтобы параметры быстро складывались! Очевидно, что такая полная связность является расточительной, а огромное количество параметров быстро приведет к переобучению.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;3D объемы нейронов&lt;/em&gt;. Сверточные нейронные сети используют тот факт, что входные данные состоят из изображений, и они ограничивают архитектуру более разумным образом. В частности, в отличие от обычной нейронной сети, слои ConvNet имеют нейроны, расположенные в трех измерениях: &lt;strong&gt;ширина, высота, глубина&lt;/strong&gt;. (Обратите внимание, что слово &lt;em&gt;«глубина»&lt;/em&gt; здесь относится к третьему измерению объема активации, а не к глубине полной нейронной сети, которая может относиться к общему количеству слоев в сети.) Например, входные изображения в CIFAR-10 представляют собой входной объем активаций, а объем имеет размеры &lt;strong&gt;32х32х3&lt;/strong&gt; (ширина, высота, глубина соответственно). Как мы вскоре увидим, нейроны в слое будут соединены только с небольшой областью слоя перед ним, а не со всеми нейронами в полном объеме. Более того, итоговый выходной слой для CIFAR-10 будет иметь размеры &lt;strong&gt;1x1x10&lt;/strong&gt;, так как к концу архитектуры &lt;em&gt;ConvNet&lt;/em&gt; мы сведем полное изображение к единому вектору оценок классов, расположенных по размерности глубины. Вот визуализация:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neural_net2.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/cnn.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Сверху&lt;/strong&gt;: обычная 3-слойная нейронная сеть. &lt;br&gt;
&lt;strong&gt;Снизу&lt;/strong&gt;: &lt;em&gt;ConvNet&lt;/em&gt; располагает свои нейроны в трех измерениях (ширина, высота, глубина), как это визуализировано в одном из слоев. Каждый слой ConvNet преобразует входной объем &lt;strong&gt;3D&lt;/strong&gt; в объем активации нейронов на выходе &lt;strong&gt;3D&lt;/strong&gt;. В этом примере красный входной слой содержит изображение, поэтому его ширина и высота будут соответствовать размерам изображения, а глубина будет равна &lt;strong&gt;3&lt;/strong&gt; (красный, зеленый, синий каналы).  &lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;ConvNet состоит из слоев. У каждого слоя есть простой API: он преобразует &lt;em&gt;входной 3D-объем&lt;/em&gt; в &lt;em&gt;выходной 3D-объем&lt;/em&gt; с помощью некоторой дифференцируемой функции, которая может иметь или не иметь параметры.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Слои, используемые для построения ConvNet&lt;/h3&gt;
&lt;p&gt;Как мы уже описывали выше, простая &lt;em&gt;ConvNet&lt;/em&gt; представляет собой последовательность слоев, и каждый слой &lt;em&gt;ConvNet&lt;/em&gt; преобразует один объем активаций в другой с помощью дифференцируемой функции. Мы используем три основных типа слоев для построения архитектур &lt;em&gt;ConvNet&lt;/em&gt;: &lt;strong&gt;сверточный слой, слой пула&lt;/strong&gt; и &lt;strong&gt;полносвязный слой&lt;/strong&gt; (точно так же, как это видно в обычных нейронных сетях). Мы сложим эти слои, чтобы сформировать полноценную &lt;strong&gt;архитектуру&lt;/strong&gt; &lt;em&gt;ConvNet&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Пример архитектуры&lt;/em&gt;: обзор. Мы рассмотрим это более подробно ниже, но простой &lt;em&gt;ConvNet&lt;/em&gt; для классификации CIFAR-10 может иметь архитектуру &lt;strong&gt;[INPUT - CONV - RELU - POOL - FC]&lt;/strong&gt;. Более подробно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;INPUT &lt;strong&gt;[32x32x3]&lt;/strong&gt; будет содержать исходные значения пикселей изображения, в данном случае изображение шириной &lt;strong&gt;32&lt;/strong&gt;, высотой &lt;strong&gt;32&lt;/strong&gt; и с тремя цветовыми каналами &lt;strong&gt;R,G,B&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Слой &lt;strong&gt;CONV&lt;/strong&gt; будет вычислять выходные данные нейронов, которые соединены с локальными областями на входных данных, каждый из которых вычисляет скалярное произведение между их весами и небольшой областью, к которой они подключены во входном объеме. Это может привести к объему &lt;strong&gt;[32x32x12]&lt;/strong&gt;, если мы решили использовать &lt;strong&gt;12&lt;/strong&gt; фильтров.&lt;/li&gt;
&lt;li&gt;Слой &lt;strong&gt;RELU&lt;/strong&gt; будет применять функцию поэлементной активации, такую как &lt;strong&gt;max(0,х)&lt;/strong&gt; с пороговым значением на нуле. При этом размер тома остается неизменным (&lt;strong&gt;[32x32x12]&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Слой &lt;strong&gt;POOL&lt;/strong&gt; выполнит операцию понижения дискретизации вдоль пространственных измерений (ширина, высота), в результате чего будет получен объем, такой как &lt;strong&gt;[16x16x12]&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Уровень &lt;strong&gt;FC&lt;/strong&gt; (т.е. полностью подключенный) будет вычислять баллы класса, в результате чего будет получен объем размера &lt;strong&gt;[1x1x10]&lt;/strong&gt;, где каждое из &lt;strong&gt;10&lt;/strong&gt; чисел соответствует баллу класса, например, среди &lt;strong&gt;10&lt;/strong&gt; категорий CIFAR-10. Как и в случае с обычными нейронными сетями и как следует из названия, каждый нейрон в этом слое будет связан со всеми числами в предыдущем объеме.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, &lt;em&gt;ConvNet&lt;/em&gt; слой за слоем преобразуют исходное изображение от исходных значений пикселей до итоговых оценок класса. Обратите внимание, что некоторые слои содержат параметры, а другие нет. В частности, слои &lt;em&gt;CONV/FC&lt;/em&gt; выполняют преобразования, которые являются функцией не только активации входного объема, но и параметров (весов и смещений нейронов). С другой стороны, слои &lt;em&gt;RELU/POOL&lt;/em&gt; будут реализовывать фиксированную функцию. Параметры в слоях &lt;em&gt;CONV/FC&lt;/em&gt; будут обучаться с помощью градиентного спуска, чтобы оценки классов, вычисляемые &lt;em&gt;ConvNet&lt;/em&gt;, соответствовали меткам в обучающем наборе для каждого изображения.  &lt;/p&gt;
&lt;p&gt;Вкратце:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Архитектура &lt;em&gt;ConvNet&lt;/em&gt; в простейшем случае представляет собой список слоев, которые преобразуют объем изображения в выходной объем (например, содержат оценки классов)&lt;/li&gt;
&lt;li&gt;Существует несколько различных типов слоев (например, &lt;em&gt;CONV/FC/RELU/POOL&lt;/em&gt; на сегодняшний день являются наиболее популярными)&lt;/li&gt;
&lt;li&gt;Каждый слой принимает входной &lt;strong&gt;3D-объем&lt;/strong&gt; и преобразует его в выходной 3D-объем с помощью дифференцируемой функции&lt;/li&gt;
&lt;li&gt;Каждый слой может иметь или не иметь параметры (например, у &lt;em&gt;CONV/FC&lt;/em&gt; есть, у &lt;em&gt;RELU/POOL&lt;/em&gt; нет)&lt;/li&gt;
&lt;li&gt;Каждый слой может иметь или не иметь дополнительные гиперпараметры (например, у &lt;em&gt;CONV/FC/POOL&lt;/em&gt; есть, у &lt;em&gt;RELU&lt;/em&gt; нет)  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/convnet.jpeg"&gt;&lt;br&gt;
Активация примера архитектуры &lt;em&gt;ConvNet&lt;/em&gt;. Начальный том хранит необработанные пиксели изображения (&lt;strong&gt;слева&lt;/strong&gt;), а последний том хранит оценки класса (&lt;strong&gt;справа&lt;/strong&gt;). Каждый объем активаций на пути обработки отображается в виде столбца. Так как визуализировать &lt;strong&gt;3D&lt;/strong&gt;-объемы сложно, мы выкладываем срезы каждого тома в ряды. Последний объем слоя содержит баллы для каждого класса, но здесь мы визуализируем только отсортированные &lt;strong&gt;5&lt;/strong&gt; лучших баллов и печатаем этикетки каждого из них. Полный &lt;a href="http://cs231n.stanford.edu/"&gt;прототип веб-версии&lt;/a&gt; приведен в шапке нашего веб-сайта. Архитектура, показанная здесь, представляет собой крошечную сеть &lt;em&gt;VGG&lt;/em&gt;, о которой мы поговорим позже.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Теперь мы опишем отдельные слои и детали их гиперпараметров и связуемости&lt;/em&gt;.    &lt;/p&gt;
&lt;h4&gt;Сверточный слой&lt;/h4&gt;
&lt;p&gt;Уровень &lt;em&gt;Conv&lt;/em&gt; является основным строительным блоком сверточной сети, который выполняет большую часть тяжелой вычислительной работы.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Обзор и интуиция без мозгов&lt;/strong&gt;. Давайте сначала обсудим, что вычисляет слой &lt;em&gt;CONV&lt;/em&gt; без аналогий между мозгом и нейронами. Параметры слоя &lt;em&gt;CONV&lt;/em&gt; состоят из набора обучаемых фильтров. Каждый фильтр имеет небольшие пространственные размеры (по ширине и высоте), но простирается на всю глубину входного объема. Например, типичный фильтр на первом слое ConvNet может иметь размер &lt;strong&gt;5x5x3&lt;/strong&gt; (&lt;em&gt;т. е. 5 пикселей в ширину и высоту, и 3, поскольку изображения имеют глубину 3, цветовые каналы&lt;/em&gt;). Во время прямого прохода мы скользим (точнее, свертываем) каждый фильтр по ширине и высоте входного объема и вычисляем точечные произведения между входами фильтра и входом в любом положении. Когда мы перемещаем фильтр по ширине и высоте входного объема, мы создадим двумерную карту активации, которая дает ответы этого фильтра в каждом пространственном положении. Интуитивно сеть будет изучать фильтры, которые активируются, когда они видят какой-либо визуальный признак, такой как край определенной ориентации или пятно определенного цвета на первом слое, или, в конечном итоге, целые соты или узоры, похожие на колеса, на более высоких слоях сети. Теперь у нас будет целый набор фильтров в каждом слое &lt;em&gt;CONV&lt;/em&gt; (например, &lt;strong&gt;12&lt;/strong&gt; фильтров), и каждый из них создаст отдельную двухмерную карту активации. Мы наложим эти карты активации вдоль измерения глубины и получим выходной объем.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Взгляд на мозг&lt;/strong&gt;. Если вы являетесь поклонником аналогий между мозгом и нейронами, то каждая запись в &lt;em&gt;3D&lt;/em&gt;-объеме вывода также может быть интерпретирована как выход нейрона, который смотрит только на небольшую область на входе и разделяет параметры со всеми нейронами слева и справа в пространстве (поскольку все эти числа являются результатом применения одного и того же фильтра).  &lt;/p&gt;
&lt;p&gt;Теперь мы обсудим детали соединений нейронов, их расположение в пространстве и схему совместного использования параметров.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Местная связность&lt;/strong&gt;. Когда речь идет о многомерных входных данных, таких как изображения, как мы видели выше, нецелесообразно соединять нейроны со всеми нейронами в предыдущем томе. Вместо этого мы будем подключать каждый нейрон только к локальной области входного объема. Пространственная протяженность этой связности является гиперпараметром, называемым &lt;strong&gt;рецептивным полем&lt;/strong&gt; нейрона (эквивалентно размеру фильтра). Степень связности вдоль оси глубины всегда равна глубине входного объема. Важно еще раз подчеркнуть эту асимметрию в том, как мы трактуем пространственные размеры (ширину и высоту) и размеры глубины: соединения локальны в &lt;em&gt;2D&lt;/em&gt;-пространстве (по ширине и высоте), но всегда полны по всей глубине входного объема.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Пример 1&lt;/em&gt;. Например, предположим, что входной объем имеет размер &lt;strong&gt;[32x32x3]&lt;/strong&gt; (например, изображение &lt;em&gt;RGB&lt;/em&gt; CIFAR-10). Если рецептивное поле (или размер фильтра) равно &lt;em&gt;5x5&lt;/em&gt;, то каждый нейрон в слое Conv будет иметь веса в области &lt;strong&gt;[5x5x3]&lt;/strong&gt; во входном объеме, что в сумме составляет &lt;strong&gt;5x5x3 = 75&lt;/strong&gt; весов (и параметр смещения &lt;strong&gt;+1&lt;/strong&gt;). Обратите внимание, что степень связности вдоль оси глубины должна быть равна &lt;strong&gt;3&lt;/strong&gt;, так как это глубина входного объема.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Пример 2&lt;/em&gt;. Предположим, что входной объем имеет размер &lt;strong&gt;[16x16x20]&lt;/strong&gt;. Затем, используя пример с размером рецептивного поля &lt;strong&gt;3x3&lt;/strong&gt;, каждый нейрон в слое Conv теперь будет иметь в общей сложности &lt;strong&gt;3x3x20 = 180&lt;/strong&gt; соединений с входным объемом. Обратите внимание, что, опять же, связность является локальной в &lt;strong&gt;2D&lt;/strong&gt;-пространстве (например, &lt;strong&gt;3x3&lt;/strong&gt;), но полной по глубине ввода (&lt;strong&gt;20&lt;/strong&gt;).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/depthcol.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neuron_model.jpeg"&gt;&lt;br&gt;
&lt;strong&gt;Сверху&lt;/strong&gt;: Пример входного объема красным цветом (например, изображение CIFAR-10 размером &lt;strong&gt;32x32x3&lt;/strong&gt;) и пример объема нейронов в первом сверточном слое. Каждый нейрон в сверточном слое пространственно связан только с локальной областью во входном объеме, но на всю глубину (&lt;em&gt;т.е. со всеми цветовыми каналами&lt;/em&gt;). Обратите внимание, что в глубине есть несколько нейронов (&lt;strong&gt;5&lt;/strong&gt; в этом примере), все они смотрят на одну и ту же область на входе: линии, которые соединяют этот столбец из &lt;strong&gt;5&lt;/strong&gt; нейронов, не представляют веса (т.е. эти &lt;strong&gt;5&lt;/strong&gt; нейронов не имеют одинаковых весов, но они связаны с &lt;strong&gt;5&lt;/strong&gt; разными фильтрами), они просто указывают на то, что эти нейроны связаны или смотрят на одно и то же рецептивное поле или область входного объема. &lt;em&gt;т.е. они имеют одно и то же рецептивное поле, но не одинаковые веса.&lt;/em&gt; &lt;strong&gt;Снизу&lt;/strong&gt;: Нейроны из главы «Нейронные сети» остаются неизменными: они по-прежнему вычисляют скалярное произведение своих весов с последующим нелинейным значением, но их связность теперь ограничена локальными пространственными данными.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Пространственное расположение&lt;/strong&gt;. Мы объяснили связь каждого нейрона в слое Conv с входным объемом, но мы еще не обсуждали, сколько нейронов находится в выходном объеме или как они организованы. Три гиперпараметра контролируют размер выходного объема: &lt;strong&gt;глубина, шаг&lt;/strong&gt; и &lt;strong&gt;нулевое отступление&lt;/strong&gt;. Мы обсудим их далее:&lt;br&gt;
- &lt;em&gt;Во-первых&lt;/em&gt;, &lt;strong&gt;глубина&lt;/strong&gt; выходного объема — это гиперпараметр: он соответствует количеству фильтров, которые мы хотели бы использовать, каждый из которых учится искать что-то свое во входных данных. Например, если первый сверточный слой принимает в качестве входных данных исходное изображение, то различные нейроны в измерении глубины могут активироваться в присутствии различных ориентированных краев или цветовых пятен. Мы будем называть набор нейронов, которые смотрят на одну и ту же область входных данных, &lt;strong&gt;столбцом глубины&lt;/strong&gt; (некоторые люди также предпочитают термин &lt;em&gt;«волокно»&lt;/em&gt;).
-&lt;em&gt; Во-вторых&lt;/em&gt;, мы должны указать &lt;em&gt;шаг&lt;/em&gt;, с которым мы перемещаем фильтр. Когда шаг равен 1, мы перемещаем фильтры по одному пикселю за раз. Когда шаг равен 2 (или редко 3 или более, хотя на практике это редкость), фильтры прыгают на 2 пикселя за раз, когда мы их перемещаем. Это позволит производить меньшие объемы выпуска в пространственном отношении.
-&lt;em&gt; Как мы скоро увидим&lt;/em&gt;, иногда будет удобно заполнять входной объем нулями по границе. Размер этого &lt;strong&gt;нулевого отступа&lt;/strong&gt; является гиперпараметром. Приятная особенность нулевого заполнения заключается в том, что он позволяет нам контролировать пространственный размер выходных объемов (чаще всего, как мы скоро увидим, мы будем использовать его для точного сохранения пространственного размера входного объема, чтобы ширина и высота входного и выходного объема были одинаковыми).  &lt;/p&gt;
&lt;p&gt;Мы можем вычислить пространственный размер выходного объема как функцию от размера входного объема (&lt;strong&gt;W&lt;/strong&gt;), размер рецептивного поля нейронов Conv слоя (&lt;strong&gt;F&lt;/strong&gt;), шаг, с которым они наносятся (&lt;strong&gt;S&lt;/strong&gt;) и количество использованного нулевого заполнения (&lt;strong&gt;P&lt;/strong&gt;) на границе. Вы можете убедиться в том, что правильная формула для расчета количества нейронов «поместится» по формуле &lt;strong&gt;(W−F+2P)/S+1&lt;/strong&gt;. Например, для входа &lt;strong&gt;7x7&lt;/strong&gt; и фильтра &lt;strong&gt;3x3&lt;/strong&gt; со stride 1 и pad 0 мы получим выход &lt;strong&gt;5x5&lt;/strong&gt;. С помощью шага &lt;strong&gt;2&lt;/strong&gt; мы получим выход &lt;strong&gt;3x3&lt;/strong&gt;. Давайте также посмотрим еще на один графический пример:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/stride.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Иллюстрация пространственного расположения. В этом примере есть только одно пространственное измерение (&lt;strong&gt;ось x&lt;/strong&gt;), один нейрон с размером рецептивного поля &lt;strong&gt;F = 3&lt;/strong&gt;, входной размер &lt;strong&gt;W = 5&lt;/strong&gt; и нулевое заполнение &lt;strong&gt;P = 1&lt;/strong&gt;. &lt;strong&gt;Сверху&lt;/strong&gt;: Нейрон шагал по входу с шагом S = 1, давая на выходе размер &lt;strong&gt;(5 - 3 + 2)/1 + 1 = 5&lt;/strong&gt;. &lt;strong&gt;Снизу&lt;/strong&gt;: Нейрон использует шаг &lt;strong&gt;S = 2&lt;/strong&gt;, давая выход размера &lt;strong&gt;(5 - 3 + 2)/2 + 1 = 3&lt;/strong&gt;. Обратите внимание, что шаг &lt;strong&gt;S = 3&lt;/strong&gt; не может быть использован, так как он не будет аккуратно помещаться по объему. С точки зрения уравнения, это можно определить, так как &lt;strong&gt;(5 - 3 + 2) = 4&lt;/strong&gt; не делится на &lt;strong&gt;3&lt;/strong&gt;.
Веса нейронов в этом примере &lt;strong&gt;[1,0,-1]&lt;/strong&gt; (&lt;em&gt;показаны справа&lt;/em&gt;), и их смещение равно нулю. Эти веса являются общими для всех желтых нейронов (см. общие параметры ниже).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Использование нулевой набивки&lt;/em&gt;. В приведенном выше примере слева обратите внимание, что входной размер был равен &lt;strong&gt;5&lt;/strong&gt;, а выходной размер был равен: также &lt;strong&gt;5&lt;/strong&gt;. Это сработало так, потому что наши рецептивные поля были равны &lt;strong&gt;3&lt;/strong&gt;, и мы использовали нулевую набивку &lt;strong&gt;1&lt;/strong&gt;. Если бы не использовалось заполнение нуля, то выходной объем имел бы пространственную размерность только &lt;strong&gt;3&lt;/strong&gt;, потому что именно столько нейронов «поместилось» бы на исходном входе. Как правило, установка нулевого заполнения равным &lt;strong&gt;P=(F−1)/2&lt;/strong&gt;. Когда шаг &lt;strong&gt;S=1&lt;/strong&gt; гарантирует, что входной и выходной объем будут иметь одинаковый пространственный размер. Очень часто используется нулевое заполнение таким образом, и мы обсудим все причины, когда будем говорить больше об архитектурах ConvNet.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ограничения на шаг&lt;/em&gt;. Обратите внимание, что гиперпараметры пространственного расположения имеют взаимные ограничения. Например, когда входные данные имеют размер &lt;strong&gt;W=10&lt;/strong&gt;, нулевой отступ не используется &lt;strong&gt;P=0&lt;/strong&gt;, а размер фильтра равен &lt;strong&gt;F=3&lt;/strong&gt;, то использовать stride было бы невозможно &lt;strong&gt;S=2__с &lt;/strong&gt;(W−F+2P)/S+1=(10−3+0)/2+1=4.5__, т.е. не целое число, указывающее на то, что нейроны не «помещаются» аккуратно и симметрично на входе. Таким образом, эта настройка гиперпараметров считается недопустимой, и библиотека ConvNet может выдать исключение или обнулить заполнение оставшейся части, чтобы она поместилась, или обрезать входные данные, чтобы она поместилась, или что-то еще. Как мы увидим в разделе Архитектуры &lt;em&gt;ConvNet&lt;/em&gt;, правильный выбор размеров ConvNet, чтобы все размеры «отрабатывались», может стать настоящей головной болью, которую использование нулевого заполнения и некоторые рекомендации по проектированию значительно облегчат.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Пример из жизни&lt;/strong&gt;.  &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"&gt;Архитектура Крижевского и др.&lt;/a&gt;, которая выиграла конкурс ImageNet в 2012 году, принимала изображения размером [227x227x3]. На первом сверточном слое использовались нейроны с размером рецептивного поля &lt;strong&gt;F=11__шаг __S=4&lt;/strong&gt; и без нулевой набивки &lt;strong&gt;P=0&lt;/strong&gt;. Так как &lt;strong&gt;(227 - 11)/4 + 1 = 55&lt;/strong&gt;, и так как слой Conv имел глубину &lt;strong&gt;K=96&lt;/strong&gt;
, выходной объем слоя Conv имел размер &lt;strong&gt;[55x55x96]&lt;/strong&gt;. Каждый из &lt;strong&gt;55x55x96&lt;/strong&gt; нейронов в этом объеме был соединен с областью размера &lt;strong&gt;[11x11x3]&lt;/strong&gt; во входном объеме. Более того, все 96 нейронов в каждой глубинной колонке подключены к одной и той же области входного канала &lt;strong&gt;[11x11x3]&lt;/strong&gt;, но, конечно, с разными весами. В качестве забавного отступления, если вы прочитаете реальную статью, она утверждает, что входные изображения были &lt;strong&gt;224x224&lt;/strong&gt;, что, безусловно, неверно, потому что &lt;strong&gt;(224 - 11)/4 + 1&lt;/strong&gt; совершенно очевидно не является целым числом. Это сбило с толку многих людей в истории &lt;em&gt;ConvNets&lt;/em&gt;, и мало что известно о том, что произошло. Мое собственное предположение заключается в том, что Алекс использовал нулевое заполнение из &lt;strong&gt;3&lt;/strong&gt; дополнительных пикселей, о которых он не упоминает в статье.    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Совместное использование параметров&lt;/strong&gt;. Схема совместного использования параметров используется в сверточных слоях для управления количеством параметров. Используя приведенный выше пример из реальной жизни, мы видим, что в первом слое &lt;em&gt;Conv&lt;/em&gt; &lt;strong&gt;55 * 55 * 96 = 290 400&lt;/strong&gt; нейронов, и каждый из них имеет &lt;strong&gt;11 * 11 * 3 = 363&lt;/strong&gt; веса и 1 смещение. В совокупности это дает &lt;strong&gt;290400 * 364 = 105 705 600 параметров&lt;/strong&gt; только на первом уровне &lt;em&gt;ConvNet&lt;/em&gt;. Понятно, что это очень большое число.  &lt;/p&gt;
&lt;p&gt;Оказывается, что мы можем значительно сократить число параметров, если сделать одно разумное допущение: если один признак полезен для вычисления в некотором пространственном положении (&lt;strong&gt;x,y&lt;/strong&gt;), то он также должен быть полезен для вычисления в другом положении (&lt;strong&gt;\(x_2, y_2\)&lt;/strong&gt;). Другими словами, обозначив один двумерный срез глубины как &lt;strong&gt;срез глубины&lt;/strong&gt; (например, объем размером &lt;strong&gt;[55x55x96]&lt;/strong&gt; имеет &lt;strong&gt;96&lt;/strong&gt; срезов глубины, каждый размером &lt;strong&gt;[55x55]&lt;/strong&gt;), мы собираемся ограничить нейроны в каждом срезе глубины, чтобы они использовали одни и те же веса и смещение. При такой схеме распределения параметров первый слой Conv в нашем примере теперь будет иметь только 96 уникальных наборов весов (по одному для каждого среза глубины), что в сумме составит &lt;strong&gt;96 * 11 * 11 * 3 = 34 848 уникальных весов&lt;/strong&gt;, или &lt;strong&gt;34 944&lt;/strong&gt; параметра (&lt;em&gt;+96 смещений&lt;/em&gt;). В качестве альтернативы, все &lt;strong&gt;55*55&lt;/strong&gt; нейронов в каждом срезе глубины теперь будут использовать одни и те же параметры. На практике во время обратного распространения каждый нейрон в объеме будет вычислять градиент для своих весов, но эти градиенты будут суммироваться для каждого среза глубины и обновлять только один набор весов для каждого среза.  &lt;/p&gt;
&lt;p&gt;Обратите внимание, что если все нейроны в одном срезе глубины используют один и тот же вектор весов, то прямой проход слоя &lt;em&gt;CONV&lt;/em&gt; в каждом глубинном срезе может быть вычислен как &lt;strong&gt;свертка&lt;/strong&gt; весов нейрона с входным объемом (отсюда и название: сверточный слой). Вот почему принято называть наборы весов &lt;strong&gt;фильтром&lt;/strong&gt; (или &lt;strong&gt;ядром&lt;/strong&gt;), который свертывается с входными данными.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/weights.jpeg"&gt;&lt;br&gt;
Примеры фильтров, изученных Крижским и др. Каждый из &lt;strong&gt;96&lt;/strong&gt; показанных здесь фильтров имеет размер &lt;strong&gt;[11x11x3]&lt;/strong&gt;, и каждый из них является общим для нейронов &lt;strong&gt;55 * 55&lt;/strong&gt; в одном глубинном срезе. Обратите внимание, что предположение о совместном использовании параметров относительно разумно: если обнаружение горизонтального края важно в каком-то месте изображения, оно должно быть интуитивно полезным и в каком-то другом месте из-за трансляционно-инвариантной структуры изображений. Таким образом, нет необходимости заново учиться обнаруживать горизонтальный ребро в каждом из &lt;strong&gt;55 * 55&lt;/strong&gt; различных мест в выходном объеме слоя &lt;em&gt;Conv&lt;/em&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Обратите внимание, что иногда предположение о совместном использовании параметров может не иметь смысла. Это особенно верно в том случае, когда входные изображения в &lt;em&gt;ConvNet&lt;/em&gt; имеют некоторую специфическую центрированную структуру, где мы должны ожидать, например, что на одной стороне изображения должны быть изучены совершенно разные функции, чем на другой. Одним из практических примеров является ситуация, когда входными данными являются лица, которые были центрированы на изображении. Можно ожидать, что различные особенности, специфичные для глаз или волос, могут (и должны) быть изучены в разных пространственных местах. В этом случае обычно ослабляют схему совместного использования параметров и вместо этого просто называют слой &lt;strong&gt;локально подключенным слоем&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Нумерные примеры&lt;/strong&gt;. Чтобы сделать обсуждение выше более конкретным, давайте выразим те же идеи, но в коде и на конкретном примере. Предположим, что входной объем представляет собой массив &lt;strong&gt;numpy&lt;/strong&gt;. Тогда:&lt;code&gt;X&lt;/code&gt;
- Колонка глубины (или волокно) в позиции будет активацией.&lt;code&gt;(x,y)&lt;/code&gt; &lt;code&gt;X[x,y,:]&lt;/code&gt;
- Глубинным срезом или, что эквивалентно&lt;code&gt;d&lt;/code&gt;, картой активации на глубине были бы активации &lt;code&gt;X[:,:,d]&lt;/code&gt; .  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Пример слоя conv&lt;/em&gt;. Предположим, что входной объем имеет форму . Предположим далее, что мы не используем нулевое заполнение (&lt;code&gt;X&lt;/code&gt; &lt;code&gt;X.shape: (11,11,4)&lt;/code&gt;&lt;strong&gt;P=0&lt;/strong&gt;), что размер фильтра равен &lt;strong&gt;F=5&lt;/strong&gt;, и что шаг является &lt;strong&gt;S=2&lt;/strong&gt;. Таким образом, выходной объем будет иметь пространственный размер &lt;strong&gt;(11-5)/2+1 = 4&lt;/strong&gt;, что дает объем с шириной и высотой 4. Карта активации в выходном объеме (назовем его ) будет выглядеть следующим образом (в этом примере вычисляются только некоторые элементы):&lt;code&gt;V&lt;/code&gt;
- &lt;code&gt;V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0&lt;/code&gt;
- &lt;code&gt;V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0&lt;/code&gt;
- &lt;code&gt;V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0&lt;/code&gt; 
- &lt;code&gt;V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0&lt;/code&gt;  &lt;/p&gt;
&lt;p&gt;Помните, что в numpy приведенная выше операция обозначает поэлементное умножение между массивами. Заметьте также, что вектор веса — это вектор веса этого нейрона и смещение. Здесь предполагается, что он имеет форму , так как размер фильтра равен &lt;strong&gt;5&lt;/strong&gt;, а глубина входного объема равна &lt;strong&gt;4&lt;/strong&gt;. Обратите внимание, что в каждой точке мы вычисляем скалярное произведение, как это было показано ранее в обычных нейронных сетях. Кроме того, мы видим, что мы используем тот же вес и смещение (из-за совместного использования параметров), и где размеры по ширине увеличиваются с шагом &lt;strong&gt;2&lt;/strong&gt; (&lt;em&gt;т.е. шаг&lt;/em&gt;). Чтобы построить вторую карту активации в выходном объеме, у нас есть:&lt;code&gt;*&lt;/code&gt; &lt;code&gt;W0&lt;/code&gt; &lt;code&gt;b0&lt;/code&gt; &lt;code&gt;W0&lt;/code&gt; &lt;code&gt;W0.shape: (5,5,4)&lt;/code&gt;&lt;br&gt;
- &lt;code&gt;V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1&lt;/code&gt;
- &lt;code&gt;V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1&lt;/code&gt;
- &lt;code&gt;V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1&lt;/code&gt;
- &lt;code&gt;V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1&lt;/code&gt;
- &lt;code&gt;V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1&lt;/code&gt; (пример перехода по y)
- &lt;code&gt;V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1&lt;/code&gt; (&lt;em&gt;или по обоим&lt;/em&gt;)  &lt;/p&gt;
&lt;p&gt;где мы видим, что мы индексируем второе измерение глубины в &lt;code&gt;V&lt;/code&gt; (по индексу &lt;em&gt;1&lt;/em&gt;), потому что мы вычисляем вторую карту активации, и что теперь используется другой набор параметров (&lt;code&gt;W1&lt;/code&gt;). В приведенном выше примере мы для краткости опускаем некоторые другие операции, которые &lt;em&gt;Conv Layer&lt;/em&gt; выполнил бы для заполнения других частей выходного массива &lt;code&gt;V&lt;/code&gt;. Кроме того, вспомните, что эти карты активации часто отслеживаются по элементам с помощью функции активации, такой как &lt;strong&gt;ReLU&lt;/strong&gt;, но здесь это не показано.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Резюме&lt;/strong&gt;. Подводя итог, можно сказать, что слой Conv:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Принимает объем любого размера &lt;strong&gt;\(W_1 \times H_1 \times D_1\)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Требуется четыре гиперпараметра:&lt;ul&gt;
&lt;li&gt;Количество фильтров &lt;strong&gt;K&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;их пространственная протяженность &lt;strong&gt;F&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;Шаг вперед &lt;strong&gt;S&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;Величина нулевого отступа &lt;strong&gt;P&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Производит объем большого размера &lt;strong&gt;\(W_2 \times H_2 \times D_2\)&lt;/strong&gt; где:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;W2=(W1−F+2P)/S+1&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;H2=(H1−F+2P)/S+1&lt;/strong&gt; (т.е. ширина и высота вычисляются поровну по симметрии)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;D2=K&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Благодаря совместному использованию параметров он вводит &lt;strong&gt;\(F \cdot F \cdot D_1\)&lt;/strong&gt; веса на фильтр, итого &lt;strong&gt;\((F \cdot F \cdot D_1) \cdot K\)&lt;/strong&gt; веса и &lt;strong&gt;K&lt;/strong&gt; cмещений.&lt;/li&gt;
&lt;li&gt;В выходном объеме метод &lt;strong&gt;d&lt;/strong&gt;-я глубина среза (размера &lt;strong&gt;\(W_2 \times H_2\)&lt;/strong&gt; ) является результатом выполнения валидной свертки &lt;strong&gt;d&lt;/strong&gt;-й фильтр по входной громкости с шагом &lt;strong&gt;S&lt;/strong&gt;, а затем сместить на &lt;strong&gt;d&lt;/strong&gt;-ое смещение.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Общая настройка гиперпараметров выглядит следующим образом: &lt;strong&gt;F=3,S=1,P=1&lt;/strong&gt;. Тем не менее, существуют общие условности и эмпирические правила, которые мотивируют эти гиперпараметры. Смотрите раздел &lt;a href="https://cs231n.github.io/convolutional-networks/#architectures"&gt;Архитектуры ConvNet&lt;/a&gt; ниже.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Демо свертки&lt;/strong&gt;. Ниже приведена бегущая демонстрация слоя CONV. Поскольку &lt;strong&gt;3D&lt;/strong&gt;-объемы трудно визуализировать, все объемы (входной объем (синий), весовой объем (красный), выходной объем (зеленый)) визуализируются с каждым срезом глубины, уложенным в ряды. Входной объем имеет размер  &lt;strong&gt;\(W_1 = 5, H_1 = 5, D_1 = 3\)&lt;/strong&gt;, а параметры слоя &lt;em&gt;CONV&lt;/em&gt; равны &lt;strong&gt;\(K = 2, F = 3, S = 2, P = 1\)&lt;/strong&gt;. То есть у нас есть два фильтра размера &lt;strong&gt;3×3&lt;/strong&gt;, и наносятся они с шагом &lt;strong&gt;2&lt;/strong&gt;. Следовательно, размер выходного объема имеет пространственный размер &lt;strong&gt;(5 - 3 + 2)/2 + 1 = 3&lt;/strong&gt;. Кроме того, обратите внимание, что отступ &lt;strong&gt;P=1&lt;/strong&gt; применяется к входному объему, при этом внешняя граница входного объема обнуляется. На приведенной ниже визуализации перебираются выходные активации (&lt;em&gt;зеленый&lt;/em&gt;) и показано, что каждый элемент вычисляется путем поэлементного умножения выделенных входных данных (&lt;em&gt;синий&lt;/em&gt;) на фильтр (&lt;em&gt;красный&lt;/em&gt;), суммирования его и последующего смещения результата.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://mldl.ru/posts/architecture/"&gt;
- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
- &lt;/p&gt;
&lt;div class="fig figcenter fighighlight"&gt;
  &lt;iframe src="https://mldl.ru/assets/conv-demo/index.html" width="100%" height="700px;" style="border:none;"&gt;&lt;/iframe&gt;
  &lt;div class="figcaption"&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Реализация в виде умножения матриц&lt;/strong&gt;. Обратите внимание, что операция свертки по сути выполняет скалярное произведение между фильтрами и локальными областями входных данных. Общий шаблон реализации слоя &lt;em&gt;CONV&lt;/em&gt; заключается в том, чтобы воспользоваться этим фактом и сформулировать прямой проход сверточного слоя в виде умножения одной большой матрицы следующим образом:
_ Локальные регионы на входном изображении растягиваются в столбцы с помощью операции, обычно называемой &lt;strong&gt;im2col&lt;/strong&gt;. Например, если входной параметр имеет размер &lt;strong&gt;[227x227x3]&lt;/strong&gt; и он должен быть свернут с помощью фильтров &lt;strong&gt;11x11x3&lt;/strong&gt; на шаге &lt;strong&gt;4&lt;/strong&gt;, то мы возьмем &lt;strong&gt;[11x11x3]&lt;/strong&gt; блоков пикселей на входе и растянем каждый блок в вектор-столбец размером &lt;strong&gt;11&lt;em&gt;11&lt;/em&gt;3 = 363&lt;/strong&gt;. Повторение этого процесса на входе с шагом &lt;strong&gt;4&lt;/strong&gt; дает &lt;strong&gt;(227-11)/4+1 = 55&lt;/strong&gt; позиций как по ширине, так и по высоте, что приводит к выходной матрице &lt;em&gt;im2col&lt;/em&gt; размера &lt;strong&gt;[363 x 3025&lt;/strong&gt;], где каждый столбец представляет собой растянутое восприимчивое поле, и всего их &lt;strong&gt;55 x 55 = 3025&lt;/strong&gt;. Обратите внимание, что поскольку рецептивные поля перекрываются, каждое число во входном объеме может дублироваться в нескольких отдельных столбцах.&lt;code&gt;X_col&lt;/code&gt;
- Грузы слоя &lt;em&gt;CONV&lt;/em&gt; аналогичным образом растягиваются в ряды. Например, если имеется &lt;strong&gt;96&lt;/strong&gt; фильтров размера &lt;strong&gt;[11x11x3]&lt;/strong&gt;, то получится матрица размера &lt;strong&gt;[96 x 363]&lt;/strong&gt;. &lt;code&gt;W_row&lt;/code&gt;
- Результат свертки теперь эквивалентен выполнению одного умножения большой матрицы , которое вычисляет скалярное произведение между каждым фильтром и каждым местоположением восприимчивого поля. В нашем примере результатом этой операции будет &lt;strong&gt;[96 x 3025]&lt;/strong&gt;, что дает выходные данные скалярного произведения каждого фильтра в каждом месте.&lt;code&gt;np.dot(W_row, X_col)&lt;/code&gt;
- В конечном итоге результат должен быть возвращен к его надлежащему выходному размеру &lt;strong&gt;[55x55x96]&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;У этого подхода есть недостаток, заключающийся в том, что он может использовать много памяти, так как некоторые значения во входном объеме многократно реплицируются в . Тем не менее, преимущество заключается в том, что существует множество очень эффективных реализаций матричного умножения, которыми мы можем воспользоваться (например, в широко используемом &lt;a href="http://www.netlib.org/blas/"&gt;BLAS&lt;/a&gt; &lt;em&gt;API&lt;/em&gt;). Более того, та же идея &lt;em&gt;im2col&lt;/em&gt; может быть повторно использована для выполнения операции объединения, о которой мы поговорим далее.&lt;code&gt;X_col&lt;/code&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Обратное распространение&lt;/strong&gt;. Обратный проход для операции свертки (как для данных, так и для весов) также является сверткой (но с пространственно перевернутыми фильтрами). Это легко вывести в одномерном случае с помощью примера с игрушкой (пока не раскрывается).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Свертка 1х1&lt;/strong&gt;. В качестве отступления, в нескольких работах используются свертки &lt;strong&gt;1x1&lt;/strong&gt;, впервые исследованные &lt;a href="http://arxiv.org/abs/1312.4400"&gt;Network in Network&lt;/a&gt;. Некоторые люди поначалу путаются, видя свертки &lt;strong&gt;1x1&lt;/strong&gt;, особенно когда они исходят из фона обработки сигналов. Обычно сигналы двумерны, поэтому свертки 1x1 не имеют смысла (это просто поточечное масштабирование). Однако в &lt;em&gt;ConvNet&lt;/em&gt; это не так, потому что необходимо помнить, что мы работаем с трехмерными объемами и что фильтры всегда распространяются на всю глубину входного объема. Например, если входные данные равны &lt;strong&gt;[32x32x3]&lt;/strong&gt;, то выполнение сверток &lt;em&gt;1x1&lt;/em&gt; фактически будет выполнением трехмерных скалярных произведений (поскольку глубина входных данных равна &lt;strong&gt;3&lt;/strong&gt; каналам).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Расширенные извилины&lt;/strong&gt;. Недавняя разработка (&lt;a href="https://arxiv.org/abs/1511.07122"&gt;см., например, статью Фишера Ю. и Владлена Колтуна&lt;/a&gt;) заключается в введении еще одного гиперпараметра в слой &lt;em&gt;CONV&lt;/em&gt;, называемого &lt;em&gt;дилатацией&lt;/em&gt;. До сих пор мы обсуждали только непрерывные фильтры &lt;em&gt;CONV&lt;/em&gt;. Тем не менее, можно иметь фильтры, которые имеют промежутки между каждой клеткой, называемые расширением. Например, в одном измерении фильтр размера &lt;strong&gt;3&lt;/strong&gt; будет вычислять на входных данных следующее: . Это расширение до &lt;strong&gt;0&lt;/strong&gt;. Для расширения &lt;strong&gt;1&lt;/strong&gt; фильтр вместо этого будет вычислять ; Другими словами, между заявками есть разрыв в &lt;strong&gt;1&lt;/strong&gt;. Это может быть очень полезно в некоторых настройках для использования в сочетании с фильтрами &lt;strong&gt;0-расширения&lt;/strong&gt;, поскольку это позволяет объединять пространственную информацию по входным данным гораздо более агрессивно с меньшим количеством слоев. Например, если вы наложите два слоя &lt;em&gt;CONV&lt;/em&gt; &lt;strong&gt;3x3&lt;/strong&gt; друг на друга, то вы можете убедить себя, что нейроны на втором слое являются функцией участка входного сигнала &lt;strong&gt;5x5&lt;/strong&gt; (мы бы сказали, что &lt;em&gt;эффективное рецептивное поле&lt;/em&gt; этих нейронов равно &lt;strong&gt;5x5&lt;/strong&gt;). Если мы будем использовать расширенные извилины, то это эффективное рецептивное поле будет расти гораздо быстрее.&lt;code&gt;w&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;w[0]*x[0] + w[1]*x[1] + w[2]*x[2]w[0]*x[0] + w[1]*x[2] + w[2]*x[4]&lt;/code&gt;  &lt;/p&gt;
&lt;h4&gt;Слой пула&lt;/h4&gt;
&lt;p&gt;В архитектуре &lt;em&gt;ConvNet&lt;/em&gt; обычно периодически вставляется слой Pooling между последовательными слоями &lt;em&gt;Conv&lt;/em&gt;. Его функция состоит в том, чтобы постепенно уменьшать пространственный размер представления для уменьшения количества параметров и вычислений в сети и, следовательно, также контролировать переобучение. Слой пулинга работает независимо на каждом срезе глубины входных данных и изменяет его пространственный размер с помощью операции &lt;strong&gt;MAX&lt;/strong&gt;. Наиболее распространенной формой является пулинговый слой с фильтрами размером &lt;strong&gt;2x2&lt;/strong&gt;, применяемыми с шагом &lt;strong&gt;2&lt;/strong&gt; вниздискретизации каждого глубинного среза на входе на &lt;strong&gt;2&lt;/strong&gt; по ширине и высоте, отбрасывая &lt;strong&gt;75%&lt;/strong&gt; активаций. Каждая операция &lt;strong&gt;MAX&lt;/strong&gt; в этом случае будет принимать максимум более 4 чисел (маленькая область &lt;strong&gt;2x2&lt;/strong&gt; в некотором глубинном срезе). Размер глубины остается неизменным. В более общем смысле, пуловый слой:
- Принимает объем любого размера &lt;strong&gt;W1×H1×D1&lt;/strong&gt;
- Требуется два гиперпараметра:
    - их пространственная протяженность &lt;strong&gt;F&lt;/strong&gt;,
    - Шаг вперед &lt;strong&gt;S&lt;/strong&gt;,
- Производит объем большого размера &lt;strong&gt;\(W_2 \times H_2 \times D_2\)&lt;/strong&gt; где:
    - &lt;strong&gt;\(W_2 = (W_1 - F)/S + 1\)&lt;/strong&gt;
    - &lt;strong&gt;\(H_2 = (H_1 - F)/S + 1\)&lt;/strong&gt;
    - &lt;strong&gt;\(D_2 = D_1\)&lt;/strong&gt;
- Вводит нулевые параметры, так как вычисляет фиксированную функцию входных данных
- Для слоев &lt;em&gt;Pooling&lt;/em&gt; заполнение входных данных не является обычным способом с использованием нулевого отступа.  &lt;/p&gt;
&lt;p&gt;Стоит отметить, что на практике встречаются только два распространенных варианта максимального слоя пула: Слой пулинга с &lt;strong&gt;F=3,S=2&lt;/strong&gt; (также называемое перекрывающимся пулом) и чаще &lt;strong&gt;F=2,S=2&lt;/strong&gt;. Объединение размеров с большими рецептивными полями слишком разрушительно.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Общий пул&lt;/strong&gt;. В дополнение к максимальному объединению, единицы объединения могут выполнять и другие функции, такие как &lt;em&gt;усредненное объединение&lt;/em&gt; или даже &lt;em&gt;объединение по L2-норме&lt;/em&gt;. Исторически часто использовалось среднее объединение, но в последнее время оно вышло из моды по сравнению с операцией максимального объединения, которая, как было показано, работает лучше на практике.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/cnn/pool.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/cnn/maxpool.jpeg"&gt;&lt;br&gt;
Пулинг слоя понижает дискретизацию объема пространственно, независимо в каждом глубинном срезе входного объема. &lt;strong&gt;Сверху&lt;/strong&gt;: В этом примере входной объем размера &lt;strong&gt;[224x224x64]&lt;/strong&gt; объединяется с фильтром размера 2, шаг 2 в выходной объем размера &lt;strong&gt;[112x112x64]&lt;/strong&gt;. Обратите внимание, что глубина объема сохраняется. &lt;strong&gt;Снизу&lt;/strong&gt;: Наиболее распространенной операцией понижения дискретизации является max, что приводит к &lt;strong&gt;максимальному объединению&lt;/strong&gt;, показанному здесь с шагом &lt;strong&gt;2&lt;/strong&gt;. То есть, каждый макс берется над 4 числами (маленький квадратик &lt;strong&gt;2х2&lt;/strong&gt;).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Обратное распространение&lt;/strong&gt;. Вспомните из главы об обратном распространении, что обратный проход для операции &lt;strong&gt;max(x, y)&lt;/strong&gt; имеет простую интерпретацию как только маршрутизацию градиента на вход, который имел наибольшее значение в прямом проходе. Следовательно, во время прямого прохождения пулового слоя обычно отслеживают индекс максимальной активации (иногда также называемый &lt;em&gt;переключателями&lt;/em&gt;), чтобы градиентная маршрутизация была эффективной во время обратного распространения.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Избавление от пулов&lt;/strong&gt;. Многим людям не нравится операция по объединению, и они думают, что мы можем обойтись без нее. Например, &lt;a href="http://arxiv.org/abs/1412.6806"&gt;Pursuit for Simplicity: The All Convolutional Net&lt;/a&gt; предлагает отказаться от пулового слоя в пользу архитектуры, состоящей только из повторяющихся слоев &lt;em&gt;CONV&lt;/em&gt;. Чтобы уменьшить размер представления, они предлагают время от времени использовать больший шаг в слое &lt;em&gt;CONV&lt;/em&gt;. Также было обнаружено, что отказ от слоев пула важен для обучения хороших генеративных моделей, таких как вариационные автоэнкодеры (&lt;em&gt;VAE&lt;/em&gt;) или генеративно-состязательные сети (&lt;em&gt;GAN&lt;/em&gt;). Вполне вероятно, что в будущих архитектурах будет очень мало или вообще не будет слоев пула.  &lt;/p&gt;
&lt;h4&gt;Слой нормализации&lt;/h4&gt;
&lt;p&gt;Многие типы уровней нормализации были предложены для использования в архитектурах &lt;em&gt;ConvNet&lt;/em&gt;, иногда с намерением реализовать схемы торможения, наблюдаемые в биологическом мозге. Однако с тех пор эти слои вышли из моды, потому что на практике их вклад был минимальным, если вообще был. О различных типах нормализации см. обсуждение в &lt;a href="http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)"&gt;API библиотеки cuda-convnet&lt;/a&gt; Алекса Крижевского.  &lt;/p&gt;
&lt;h4&gt;Полносвязный слой&lt;/h4&gt;
&lt;p&gt;Нейроны в полностью связном слое имеют полные связи со всеми активациями в предыдущем слое, как это видно в обычных нейронных сетях. Таким образом, их активации могут быть вычислены с помощью умножения матриц с последующим смещением смещения. Дополнительные сведения см. в разделе &lt;em&gt;«Нейронные сети»&lt;/em&gt; примечаний.  &lt;/p&gt;
&lt;h4&gt;Преобразование слоев FC в слои CONV&lt;/h4&gt;
&lt;p&gt;Стоит отметить, что единственное различие между слоями &lt;em&gt;FC&lt;/em&gt; и &lt;em&gt;CONV&lt;/em&gt; заключается в том, что нейроны в слое &lt;em&gt;CONV&lt;/em&gt; связаны только с локальной областью на входе, и что многие нейроны в объеме &lt;em&gt;CONV&lt;/em&gt; имеют общие параметры. Тем не менее, нейроны в обоих слоях по-прежнему вычисляют точечные произведения, поэтому их функциональная форма идентична. Таким образом, оказывается, что можно преобразовывать между слоями &lt;em&gt;FC&lt;/em&gt; и &lt;em&gt;CONV&lt;/em&gt;:
- Для любого слоя &lt;em&gt;CONV&lt;/em&gt; существует слой &lt;em&gt;FC&lt;/em&gt;, реализующий ту же прямую функцию. Матрица весов будет большой матрицей, которая в основном равна нулю, за исключением некоторых блоков (из-за локальной связности), где веса во многих блоках равны (из-за совместного использования параметров).
- И наоборот, любой слой &lt;em&gt;FC&lt;/em&gt; может быть преобразован в слой &lt;em&gt;CONV&lt;/em&gt;. Например, слой &lt;em&gt;FC&lt;/em&gt; с &lt;strong&gt;K=4096&lt;/strong&gt;, то есть с учетом некоторого входного объема размера &lt;strong&gt;7×7×512&lt;/strong&gt; может быть эквивалентно выражен в виде слоя CONV с помощью &lt;strong&gt;F=7,P=0,S=1,K=4096&lt;/strong&gt;. Другими словами, мы устанавливаем размер фильтра точно равным размеру входного объема, и, следовательно, на выходе будет просто &lt;strong&gt;1×1×4096&lt;/strong&gt; так как только один столбец глубины «помещается» поперек входного объема, давая тот же результат, что и исходный слой FC.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Конверсия FC-&amp;gt;CONV&lt;/strong&gt;. Из этих двух преобразований возможность преобразования слоя FC в слой CONV особенно полезна на практике. Рассмотрим архитектуру ConvNet, которая берет изображение размером &lt;strong&gt;224x224x3&lt;/strong&gt;, а затем использует ряд слоев &lt;em&gt;CONV&lt;/em&gt; и слоев &lt;em&gt;POOL&lt;/em&gt; для уменьшения объема активации до размера &lt;strong&gt;7x7x512&lt;/strong&gt; (в архитектуре &lt;em&gt;AlexNet&lt;/em&gt;, которую мы увидим позже, это делается с помощью &lt;strong&gt;5&lt;/strong&gt; слоев пула, которые каждый раз уменьшают пространственную дискретизацию входных данных в два раза). Получаем итоговый пространственный размер &lt;strong&gt;224/2/2/2/2/2 = 7&lt;/strong&gt;). После этого &lt;em&gt;AlexNet&lt;/em&gt; использует два слоя &lt;em&gt;FC&lt;/em&gt; размера &lt;strong&gt;4096&lt;/strong&gt; и, наконец, последний слой &lt;em&gt;FC&lt;/em&gt; с &lt;strong&gt;1000&lt;/strong&gt; нейронами, которые вычисляют баллы класса. Мы можем преобразовать каждый из этих трех слоев &lt;em&gt;FC&lt;/em&gt; в слои &lt;em&gt;CONV&lt;/em&gt;, как описано выше:
- Замените первый слой &lt;em&gt;FC&lt;/em&gt;, который смотрит на объем &lt;strong&gt;[7x7x512]&lt;/strong&gt;, на слой &lt;em&gt;CONV&lt;/em&gt;, использующий размер фильтра &lt;strong&gt;F=7&lt;/strong&gt;, дающий выходной объем &lt;strong&gt;[1x1x4096]&lt;/strong&gt;.
- Замените второй слой FC на слой &lt;em&gt;CONV&lt;/em&gt;, использующий размер фильтра &lt;strong&gt;F=1&lt;/strong&gt;, дающий выходной объем &lt;strong&gt;[1x1x4096]&lt;/strong&gt;
- Замените последний слой &lt;em&gt;FC&lt;/em&gt; аналогичным образом, на &lt;strong&gt;F=1&lt;/strong&gt;, выдающий итоговый вывод &lt;strong&gt;[1x1x1000]&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;Каждое из этих преобразований на практике может включать в себя манипуляции (например, изменение формы) матрицей весов &lt;strong&gt;W&lt;/strong&gt; в каждом слое &lt;em&gt;FC&lt;/em&gt; в фильтры слоя &lt;em&gt;CONV&lt;/em&gt;. Оказывается, что это преобразование позволяет нам очень эффективно «скользить» по исходной &lt;em&gt;ConvNet&lt;/em&gt; через множество пространственных положений в более крупном изображении за один проход вперед.  &lt;/p&gt;
&lt;p&gt;Например, если образ &lt;strong&gt;224x224&lt;/strong&gt; дает объем размера &lt;strong&gt;[7x7x512]&lt;/strong&gt; - т.е. уменьшение на &lt;strong&gt;32&lt;/strong&gt;, то пересылка образа размера &lt;strong&gt;384x384&lt;/strong&gt; через преобразованную архитектуру даст эквивалентный объем в размере &lt;strong&gt;[12x12x512]&lt;/strong&gt;, так как &lt;em&gt;384/32 = 12&lt;/em&gt;. Последующие &lt;strong&gt;3&lt;/strong&gt; слоя &lt;em&gt;CONV&lt;/em&gt;, которые мы только что преобразовали из слоев FC, теперь дадут окончательный объем размера &lt;strong&gt;[6x6x1000]&lt;/strong&gt;, поскольку &lt;strong&gt;(12 - 7)/1 + 1 = 6&lt;/strong&gt;. Обратите внимание, что вместо одного вектора оценок классов размера &lt;strong&gt;[1x1x1000]&lt;/strong&gt; мы теперь получаем целый массив оценок классов &lt;strong&gt;6x6&lt;/strong&gt; на изображении &lt;strong&gt;384x384&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Независимая оценка исходной ConvNet (со слоями FC) по кадрам 224x224 изображения 384x384 с шагом 32 пикселя дает результат, идентичный однократной пересылке преобразованной ConvNet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Естественно, пересылка преобразованной &lt;em&gt;ConvNet&lt;/em&gt; за один раз гораздо эффективнее, чем итерация исходной ConvNet по всем этим &lt;strong&gt;36&lt;/strong&gt; местоположениям, поскольку &lt;strong&gt;36&lt;/strong&gt; оценок используют общие вычисления. Этот трюк часто используется на практике для повышения производительности, когда, например, обычно изменяют размер изображения, чтобы сделать его больше, используют преобразованный &lt;em&gt;ConvNet&lt;/em&gt; для оценки оценок класса во многих пространственных положениях, а затем усредняют баллы класса.  &lt;/p&gt;
&lt;p&gt;Наконец, что, если мы хотим эффективно применить исходную &lt;em&gt;ConvNet&lt;/em&gt; поверх изображения, но с шагом меньше &lt;strong&gt;32&lt;/strong&gt; пикселей? Мы могли бы добиться этого с помощью нескольких передач вперед. Например, обратите внимание, что если бы мы хотели использовать шаг в &lt;strong&gt;16&lt;/strong&gt; пикселей, мы могли бы сделать это, объединив объемы, полученные при пересылке преобразованной &lt;em&gt;ConvNet&lt;/em&gt; дважды: сначала над исходным изображением, а затем над изображением, но с пространственным сдвигом изображения на &lt;strong&gt;16&lt;/strong&gt; пикселей как по ширине, так и по высоте.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Блокнот IPython по &lt;a href="https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb"&gt;сетевой хирургии&lt;/a&gt; показывает, как выполнить преобразование на практике, в коде (с использованием &lt;em&gt;Caffe&lt;/em&gt;)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Архитектуры ConvNet&lt;/h2&gt;
&lt;p&gt;Мы видели, что сверточные сети обычно состоят только из трех типов слоев: &lt;em&gt;CONV&lt;/em&gt;, &lt;em&gt;POOL&lt;/em&gt; (мы предполагаем &lt;em&gt;Max&lt;/em&gt; &lt;em&gt;pool&lt;/em&gt;, если не указано иное) и &lt;em&gt;FC&lt;/em&gt; (сокращение от &lt;em&gt;fully&lt;/em&gt; &lt;em&gt;connected&lt;/em&gt;). Мы также явно напишем функцию активации &lt;em&gt;RELU&lt;/em&gt; в виде слоя, который применяет элементную нелинейность. В этом разделе мы обсудим, как они обычно складываются в целые &lt;em&gt;ConvNet&lt;/em&gt;.  &lt;/p&gt;
&lt;h3&gt;Узоры слоев&lt;/h3&gt;
&lt;p&gt;Наиболее распространенная форма архитектуры &lt;em&gt;ConvNet&lt;/em&gt; состоит из нескольких слоев &lt;em&gt;CONV&lt;/em&gt;-&lt;em&gt;RELU&lt;/em&gt;, затем за ними следуют слои &lt;em&gt;POOL&lt;/em&gt; и повторяет этот шаблон до тех пор, пока изображение не будет объединено в пространстве до небольшого размера. В какой-то момент часто происходит переход к полносвязным слоям. Последний полносвязный слой содержит выходные данные, такие как баллы класса. Другими словами, наиболее распространенная архитектура &lt;em&gt;ConvNet&lt;/em&gt; следует шаблону:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;где &lt;code&gt;*&lt;/code&gt; указывает на повторение, а &lt;code&gt;POOL?&lt;/code&gt; указывает на необязательный слой пула. Более того, &lt;code&gt;N &amp;gt;= 0&lt;/code&gt; (и обычно  &lt;code&gt;N &amp;lt;= 3&lt;/code&gt; &lt;code&gt;M &amp;gt;= 0&lt;/code&gt; &lt;code&gt;K &amp;gt;= 0&lt;/code&gt;), , (и обычно &lt;code&gt;K &amp;lt; 3&lt;/code&gt; ). Например, вот некоторые распространенные архитектуры ConvNet, которые следуют этому шаблону:&lt;br&gt;
- &lt;code&gt;INPUT -&amp;gt; FC&lt;/code&gt; реализует линейный классификатор. Здесь &lt;code&gt;N = M = K = 0&lt;/code&gt;
- &lt;code&gt;INPUT -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; FC&lt;/code&gt;
- &lt;code&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; POOL]*2 -&amp;gt; FC -&amp;gt; RELU -&amp;gt; FC&lt;/code&gt;. Здесь мы видим, что между каждым слоем POOL есть один слой CONV.
 -&lt;code&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; POOL]*3 -&amp;gt; [FC -&amp;gt; RELU]*2 -&amp;gt; FC&lt;/code&gt;. Здесь мы видим два слоя &lt;em&gt;CONV&lt;/em&gt;, расположенных перед каждым слоем &lt;em&gt;POOL&lt;/em&gt;. Как правило, это хорошая идея для более крупных и глубоких сетей, поскольку несколько слоев &lt;em&gt;CONV&lt;/em&gt; могут привести к более сложным характеристикам входного объема перед деструктивной операцией объединения.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Отдайте предпочтение стеку небольших фильтров CONV одному большому слою рецептивного поля CONV&lt;/em&gt;. Предположим, что вы накладываете три слоя &lt;em&gt;CONV&lt;/em&gt; &lt;strong&gt;3x3&lt;/strong&gt; друг на друга (конечно, с нелинейностями между ними). При таком расположении каждый нейрон на первом слое &lt;em&gt;CONV&lt;/em&gt; имеет представление входного объема &lt;strong&gt;3x3&lt;/strong&gt;. Нейрон на втором слое &lt;em&gt;CONV&lt;/em&gt; имеет представление &lt;strong&gt;3x3&lt;/strong&gt; первого слоя &lt;em&gt;CONV&lt;/em&gt; и, следовательно, представление входного объема &lt;strong&gt;5x5&lt;/strong&gt;. Аналогично, нейрон на третьем слое &lt;em&gt;CONV&lt;/em&gt; имеет представление &lt;strong&gt;3x3&lt;/strong&gt; на 2-й слой &lt;em&gt;CONV&lt;/em&gt; и, следовательно, на входной объем &lt;strong&gt;7x7&lt;/strong&gt;. Предположим, что вместо этих трех слоев &lt;strong&gt;3x3&lt;/strong&gt; &lt;em&gt;CONV&lt;/em&gt; мы хотим использовать только один слой &lt;em&gt;CONV&lt;/em&gt; с рецептивными полями &lt;strong&gt;7x7&lt;/strong&gt;. Эти нейроны будут иметь размер рецептивного поля входного объема, идентичный в пространственном масштабе (&lt;strong&gt;7x7&lt;/strong&gt;), но с некоторыми недостатками.
- Во-первых, нейроны будут вычислять линейную функцию над входными данными, в то время как три стека слоев &lt;em&gt;CONV&lt;/em&gt; содержат нелинейности, которые делают их особенности более выразительными. 
- Во-вторых, если мы предположим, что все тома имеют &lt;strong&gt;C&lt;/strong&gt; каналов, то можно видеть, что один слой &lt;strong&gt;7x7&lt;/strong&gt; &lt;em&gt;CONV&lt;/em&gt; будет содержать &lt;strong&gt;\(C \times (7 \times 7 \times C) = 49 C^2\)&lt;/strong&gt; параметры, в то время как три слоя &lt;strong&gt;3x3&lt;/strong&gt; &lt;em&gt;CONV&lt;/em&gt; будут содержать только &lt;strong&gt;\(3 \times (C \times (3 \times 3 \times C)) = 27 C^2\)&lt;/strong&gt; параметры. Интуитивно понятно, что наложение слоев &lt;em&gt;CONV&lt;/em&gt; с маленькими фильтрами в отличие от одного слоя &lt;em&gt;CONV&lt;/em&gt; с большими фильтрами позволяет нам выразить более мощные функции входных данных с меньшим количеством параметров. В качестве практического недостатка нам может потребоваться больше памяти для хранения всех результатов промежуточного слоя &lt;em&gt;CONV&lt;/em&gt;, если мы планируем использовать обратное распространение.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Недавние уходы&lt;/strong&gt;. Следует отметить, что традиционная парадигма линейного списка слоев в последнее время была поставлена под сомнение в архитектурах &lt;em&gt;Google Inception&lt;/em&gt;, а также в современных (современных) &lt;em&gt;Residual Networks&lt;/em&gt; от &lt;em&gt;Microsoft Research Asia&lt;/em&gt;. Оба они (см. подробности ниже в разделе тематических исследований) имеют более сложные и разные структуры подключения.&lt;/p&gt;
&lt;p&gt;__На практике: используйте то, что лучше всего работает на &lt;em&gt;ImageNet__&lt;/em&gt;. Если вы чувствуете некоторую усталость, думая об архитектурных решениях, вам будет приятно узнать, что в &lt;strong&gt;90%&lt;/strong&gt; или более приложений вам не нужно беспокоиться об этом. Я предпочитаю резюмировать этот момент как &lt;em&gt;«не будьте героем»&lt;/em&gt; : вместо того, чтобы создавать свою собственную архитектуру для решения проблемы, вы должны посмотреть, какая архитектура в настоящее время лучше всего работает на &lt;em&gt;ImageNet&lt;/em&gt;, загрузить предварительно обученную модель и настроить ее на основе ваших данных. В редких случаях приходится обучать &lt;em&gt;ConvNet&lt;/em&gt; с нуля или проектировать его с нуля. Я также говорил об этом в &lt;a href="https://www.youtube.com/watch?v=u6aEYuemt0M"&gt;школе Deep Learning&lt;/a&gt;.  &lt;/p&gt;
&lt;h3&gt;Шаблоны для определения размеров слоев&lt;/h3&gt;
&lt;p&gt;До сих пор мы опускали упоминания об общих гиперпараметрах, используемых на каждом из уровней ConvNet. Сначала мы изложим общие эмпирические правила для определения размеров архитектур, а затем будем следовать правилам, обсуждая нотацию:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Входной слой&lt;/strong&gt; (содержащий изображение) должен быть кратен &lt;strong&gt;2&lt;/strong&gt; много раз. К распространенным номерам относятся 32 (например, CIFAR-10), &lt;strong&gt;64&lt;/strong&gt;, &lt;strong&gt;96&lt;/strong&gt; (например, &lt;strong&gt;STL-10&lt;/strong&gt;) или &lt;strong&gt;224&lt;/strong&gt; (например, общие &lt;em&gt;ImageNet&lt;/em&gt; &lt;em&gt;ConvNet&lt;/em&gt;), &lt;strong&gt;384&lt;/strong&gt; и &lt;strong&gt;512&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Для конвальных слоев&lt;/strong&gt; следует использовать небольшие фильтры (например, &lt;strong&gt;3x3&lt;/strong&gt; или максимум &lt;strong&gt;5x5&lt;/strong&gt;), с шагом &lt;strong&gt;S=1&lt;/strong&gt; и, что особенно важно, заполнение входного объема нулями таким образом, чтобы слой conv не изменял пространственные размеры входных данных. То есть, когда &lt;strong&gt;F=3&lt;/strong&gt;, то с помощью &lt;strong&gt;P=1&lt;/strong&gt; сохранит исходный размер входных данных. Когда &lt;strong&gt;F=5&lt;/strong&gt;, &lt;strong&gt;P=2&lt;/strong&gt;. Для генерала &lt;strong&gt;F&lt;/strong&gt;, видно, что &lt;strong&gt;P=(F−1)/2&lt;/strong&gt; cохраняет размер ввода. Если вам нужно использовать фильтры большего размера (например, &lt;strong&gt;7x7&lt;/strong&gt; или около того), это обычно можно увидеть только на самом первом выпуклом слое, который смотрит на входное изображение.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Слои пула&lt;/strong&gt; отвечают за понижение дискретизации пространственных размеров входных данных. Наиболее распространенной настройкой является использование max-pooling с рецептивными полями &lt;strong&gt;2x2&lt;/strong&gt; (т.е. &lt;strong&gt;F=2&lt;/strong&gt;), и с шагом &lt;strong&gt;2&lt;/strong&gt; (т.е. &lt;strong&gt;S=2&lt;/strong&gt;). Обратите внимание, что при этом отбрасывается ровно &lt;strong&gt;75%&lt;/strong&gt; активаций входного объема (из-за уменьшения дискретизации на &lt;strong&gt;2&lt;/strong&gt; раза как по ширине, так и по высоте). Другой, чуть менее распространенный вариант — использование рецептивных полей &lt;strong&gt;3x3&lt;/strong&gt; с шагом &lt;strong&gt;2&lt;/strong&gt;, но это делает «подгонку» более сложной (например, слой &lt;strong&gt;32x32x3&lt;/strong&gt; потребует нулевого заполнения для использования с максимальным объединением полей с рецептивным полем &lt;strong&gt;3x3&lt;/strong&gt; и шагом &lt;strong&gt;2&lt;/strong&gt;). Очень редко можно увидеть, что размеры рецептивных полей для максимального пула больше &lt;strong&gt;3&lt;/strong&gt;, потому что в этом случае пулинг слишком потерян и агрессивен. Обычно это приводит к ухудшению производительности.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Уменьшение головной боли при уменьшении размера&lt;/strong&gt;. Представленная выше схема радует тем, что все слои &lt;em&gt;CONV&lt;/em&gt; сохраняют пространственный размер входных данных, в то время как только слои &lt;em&gt;POOL&lt;/em&gt; отвечают за пространственное понижение объемов. В альтернативной схеме, где мы используем шаги больше &lt;strong&gt;1&lt;/strong&gt; или не обнуляем входные данные в слоях &lt;em&gt;CONV&lt;/em&gt;, нам пришлось бы очень тщательно отслеживать входные объемы по всей архитектуре &lt;em&gt;CNN&lt;/em&gt; и убедиться, что все шаги и фильтры  &lt;em&gt;«работают»&lt;/em&gt; , и что архитектура &lt;em&gt;ConvNet&lt;/em&gt; хорошо и симметрично связана.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Зачем использовать strace of 1 в CONV?&lt;/strong&gt; Меньшие шаги лучше работают на практике. Кроме того, как уже упоминалось, шаг &lt;strong&gt;1&lt;/strong&gt; позволяет нам оставить всю пространственную понижение дискретизации слоям &lt;em&gt;POOL&lt;/em&gt;, при этом слои &lt;em&gt;CONV&lt;/em&gt; преобразуют только входной объем по глубине.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Зачем использовать набивку?&lt;/strong&gt; В дополнение к вышеупомянутому преимуществу сохранения постоянных пространственных размеров после &lt;em&gt;CONV&lt;/em&gt;, это фактически повышает производительность. Если бы слои &lt;em&gt;CONV&lt;/em&gt; не обнуляли входные данные, а выполняли только корректные свертки, то размер объемов уменьшался бы на небольшую величину после каждого &lt;em&gt;CONV&lt;/em&gt;, а информация на границах &lt;em&gt;«смывалась»&lt;/em&gt; бы слишком быстро.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Компрометация из-за ограничений памяти&lt;/strong&gt;. В некоторых случаях (особенно на ранних этапах архитектуры ConvNet) объем памяти может очень быстро увеличиваться с помощью эмпирических правил, представленных выше. Например, фильтрация изображения размером &lt;strong&gt;224x224x3&lt;/strong&gt; с тремя слоями &lt;em&gt;CONV&lt;/em&gt; &lt;strong&gt;3x3&lt;/strong&gt; с &lt;strong&gt;64&lt;/strong&gt; фильтрами каждый и отступом &lt;strong&gt;1&lt;/strong&gt; создаст три объема активации размером &lt;strong&gt;[224x224x64]&lt;/strong&gt;. Это составляет в общей сложности около &lt;strong&gt;10&lt;/strong&gt; миллионов активаций, или 72 МБ памяти (на изображение, как для активаций, так и для градиентов). Поскольку графические процессоры часто имеют узкие места из-за памяти, может потребоваться пойти на компромисс. На практике люди предпочитают идти на компромисс только на первом уровне &lt;em&gt;CONV&lt;/em&gt; сети. Например, одним из компромиссов может быть использование первого слоя &lt;em&gt;CONV&lt;/em&gt; с размерами фильтра &lt;strong&gt;7x7&lt;/strong&gt; и шагом &lt;strong&gt;2&lt;/strong&gt; (как в сети ZF). В качестве другого примера, &lt;em&gt;AlexNet&lt;/em&gt; использует размеры фильтров &lt;strong&gt;11x11&lt;/strong&gt; и &lt;em&gt;stride 4&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Тематические исследования&lt;/h3&gt;
&lt;p&gt;В области сверточных сетей существует несколько архитектур, которые имеют название. Наиболее распространенными являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LeNet&lt;/strong&gt;. Первые успешные приложения сверточных сетей были разработаны Яном Лекуном в 1990-х годах. Из них наиболее известной является архитектура &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"&gt;LeNet&lt;/a&gt;, которая использовалась для чтения почтовых индексов, цифр и т. д.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;. Первой работой, которая популяризировала сверточные сети в компьютерном зрении, стала &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"&gt;сеть AlexNet&lt;/a&gt;, разработанная Алексом Крижевским, Ильей Суцкевером и Джеффом Хинтоном. В 2012 году AlexNet был представлен на &lt;a href="http://www.image-net.org/challenges/LSVRC/2014/"&gt;конкурс ImageNet ILSVRC&lt;/a&gt; и значительно превзошел занявшего второе место (ошибка в топ-5 &lt;strong&gt;16%&lt;/strong&gt; по сравнению с ошибкой в &lt;strong&gt;26%&lt;/strong&gt;, занявшей второе место). Сеть имела очень похожую архитектуру на &lt;em&gt;LeNet&lt;/em&gt;, но была глубже, больше и включала сверточные слои, наложенные друг на друга (ранее было обычным делом иметь только один слой &lt;em&gt;CONV&lt;/em&gt;, за которым всегда следовал слой &lt;em&gt;POOL&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZF Net&lt;/strong&gt;. Победителем &lt;em&gt;ILSVRC 2013&lt;/em&gt; стала сверточная сеть от Мэтью Зейлера и Роба Фергуса. Она стала известна как &lt;a href="http://arxiv.org/abs/1311.2901"&gt;ZFNet&lt;/a&gt; (сокращение от Zeiler &amp;amp; Fergus Net). Это было усовершенствование AlexNet за счет настройки гиперпараметров архитектуры, в частности, за счет увеличения размера средних сверточных слоев и уменьшения размера шага и фильтра на первом слое.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GoogLeNet&lt;/strong&gt;. Победителем &lt;em&gt;ILSVRC 2014&lt;/em&gt; стала сверточная сеть от &lt;a href="http://arxiv.org/abs/1409.4842"&gt;Сегеди и др.&lt;/a&gt; от Google. Ее основным вкладом стала разработка модуля Inception, который значительно сократил количество параметров в сети (&lt;strong&gt;4M&lt;/strong&gt;, по сравнению с &lt;em&gt;AlexNet&lt;/em&gt; с &lt;strong&gt;60M&lt;/strong&gt;). Кроме того, в этой статье используется &lt;em&gt;Average Pooling&lt;/em&gt; вместо &lt;em&gt;Fully Connected layers&lt;/em&gt; в верхней части &lt;em&gt;ConvNet&lt;/em&gt;, что устраняет большое количество параметров, которые не имеют большого значения. Существует также несколько последующих версий &lt;em&gt;GoogLeNet&lt;/em&gt;, последняя из которых &lt;a href="http://arxiv.org/abs/1602.07261"&gt;Inception-v4&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VGGNet&lt;/strong&gt;. Второе место на &lt;em&gt;ILSVRC 2014&lt;/em&gt; заняла сеть Карена Симоняна и Эндрю Зиссермана, которая стала известна как &lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"&gt;VGGNet&lt;/a&gt;. Его основной вклад заключался в том, что он показал, что глубина сети является критически важным компонентом для хорошей производительности. Их окончательная лучшая сеть содержит 16 слоев &lt;em&gt;CONV&lt;/em&gt;/&lt;em&gt;FC&lt;/em&gt; и, что привлекательно, отличается чрезвычайно однородной архитектурой, которая выполняет только свертки 3x3 и пул &lt;strong&gt;2x2&lt;/strong&gt; от начала до конца. Их &lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"&gt;предварительно обученная модель&lt;/a&gt; доступна для использования в &lt;em&gt;Caffe&lt;/em&gt; по принципу «подключи и работай». Недостатком &lt;em&gt;VGGNet&lt;/em&gt; является то, что он дороже в оценке и использует гораздо больше памяти и параметров (&lt;em&gt;140M&lt;/em&gt;). Большинство этих параметров находятся в первом полностью связанном слое, и с тех пор было обнаружено, что эти слои &lt;em&gt;FC&lt;/em&gt; могут быть удалены без снижения производительности, что значительно сокращает количество необходимых параметров.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ResNet&lt;/strong&gt;.. &lt;a href="http://arxiv.org/abs/1512.03385"&gt;Остаточная сеть&lt;/a&gt;, разработанная Каим Хе и др., стала победителем &lt;em&gt;ILSVRC 2015&lt;/em&gt;. Она оснащена &lt;em&gt;специальными соединениями для пропуска&lt;/em&gt; и интенсивным использованием &lt;a href="http://arxiv.org/abs/1502.03167"&gt;пакетной нормализации&lt;/a&gt;. В архитектуре также отсутствуют полностью связанные слои в конце сети. Читателю также предлагается ознакомиться с презентацией Кайминга (&lt;a href="https://www.youtube.com/watch?v=1PGLj-uKT1w"&gt;видео&lt;/a&gt;, &lt;a href="http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"&gt;слайды&lt;/a&gt;) и некоторыми &lt;a href="https://github.com/gcr/torch-residual-networks"&gt;недавними экспериментами&lt;/a&gt;, которые воспроизводят эти сети в Torch. В настоящее время ResNet являются самыми современными моделями сверточных нейронных сетей и являются выбором по умолчанию для использования &lt;em&gt;ConvNet&lt;/em&gt; на практике (по состоянию на &lt;em&gt;10 мая 2016 года&lt;/em&gt;). В частности, см. более поздние разработки, которые корректируют исходную архитектуру, из &lt;a href="https://arxiv.org/abs/1603.05027"&gt;книги Каим Хе и др. IСопоставления идентификационных данных в глубоких остаточных сетях&lt;/a&gt; (опубликована &lt;em&gt;в марте 2016 г.&lt;/em&gt;).  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;VGGNet в деталях&lt;/strong&gt;. Давайте разберем &lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"&gt;VGGNet&lt;/a&gt; более подробно в качестве тематического исследования. Вся сеть &lt;em&gt;VGGNet&lt;/em&gt; состоит из слоев &lt;em&gt;CONV&lt;/em&gt;, которые выполняют свертки &lt;strong&gt;3x3&lt;/strong&gt; со &lt;em&gt;stride 1 и pad 1&lt;/em&gt;, и из слоев &lt;em&gt;POOL&lt;/em&gt;, которые выполняют &lt;strong&gt;2x2&lt;/strong&gt; &lt;em&gt;max&lt;/em&gt; &lt;em&gt;pooling&lt;/em&gt; с &lt;em&gt;stride&lt;/em&gt; &lt;em&gt;2&lt;/em&gt; (и без отступа). Мы можем записывать размер представления на каждом шаге обработки и отслеживать как размер представления, так и общее количество весов:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters  
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Как и в случае со сверточными сетями, обратите внимание, что большая часть памяти (а также вычислительного времени) используется в ранних слоях &lt;em&gt;CONV&lt;/em&gt;, а большинство параметров — в последних слоях &lt;em&gt;FC&lt;/em&gt;. В данном конкретном случае первый слой &lt;em&gt;FC&lt;/em&gt; содержит 100 м грузов из общего числа 140 м.&lt;/p&gt;
&lt;h3&gt;Вычислительные соображения&lt;/h3&gt;
&lt;p&gt;Самым большим узким местом, о котором следует знать при создании архитектур ConvNet, является узкое место памяти. Многие современные графические процессоры имеют ограничение в &lt;strong&gt;3/4/6 ГБ&lt;/strong&gt; памяти, а лучшие графические процессоры имеют около &lt;strong&gt;12 ГБ&lt;/strong&gt; памяти. Существует три основных источника памяти, которые необходимо отслеживать:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Из промежуточных размеров объемов: Это исходное количество &lt;strong&gt;активаций&lt;/strong&gt; на каждом уровне ConvNet, а также их градиенты (одинакового размера). Как правило, большая часть активаций происходит на более ранних уровнях &lt;em&gt;ConvNet&lt;/em&gt; (т.е. на первых уровнях &lt;em&gt;Conv&lt;/em&gt;). Они сохраняются, потому что они нужны для обратного распространения, но умная реализация, которая запускает &lt;em&gt;ConvNet&lt;/em&gt; только во время тестирования, в принципе могла бы значительно сократить это, сохраняя только текущие активации на любом уровне и отбрасывая предыдущие активации на уровнях ниже.&lt;/li&gt;
&lt;li&gt;Из размеров параметров: Это числа, которые содержат &lt;strong&gt;параметры&lt;/strong&gt; сети, их градиенты во время обратного распространения и, как правило, также кэш шагов, если оптимизация выполняется с использованием momentum, &lt;em&gt;Adagrad&lt;/em&gt; или &lt;em&gt;RMSProp&lt;/em&gt;. Следовательно, память для хранения только вектора параметров обычно должна быть умножена по крайней мере на &lt;strong&gt;3&lt;/strong&gt; или около того.&lt;/li&gt;
&lt;li&gt;Каждая реализация &lt;em&gt;ConvNet&lt;/em&gt; должна поддерживать &lt;strong&gt;различную&lt;/strong&gt; память, такую как пакеты данных изображений, возможно, их дополненные версии и т.д.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;После того, как вы получили приблизительную оценку общего количества значений (для активаций, градиентов и разного), это число следует преобразовать в размер в ГБ. Возьмите количество значений, умножьте на 4, чтобы получить исходное количество байтов (поскольку каждая плавающая точка равна 4 байтам, или, возможно, на 8 для двойной точности), а затем разделите на 1024 несколько раз, чтобы получить объем памяти в КБ, МБ и, наконец, в ГБ. Если ваша сеть не подходит, обычной эвристикой для «подгонки» является уменьшение размера пакета, поскольку большая часть памяти обычно потребляется активациями.  &lt;/p&gt;
&lt;h2&gt;Дополнительные материалы&lt;/h2&gt;
&lt;p&gt;Дополнительные ресурсы, связанные с реализацией:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/soumith/convnet-benchmarks"&gt;Бенчмарки Soumith для производительности CONV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html"&gt;Демонстрация ConvNetJS CIFAR-10&lt;/a&gt; позволяет экспериментировать с архитектурами ConvNet и видеть результаты и вычисления в режиме реального времени, в браузере.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://caffe.berkeleyvision.org/"&gt;Caffe&lt;/a&gt;, одна из популярных библиотек ConvNet.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://torch.ch/blog/2016/02/04/resnets.html"&gt;Современные ResNet в Torch7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/architecture/</guid><pubDate>Wed, 12 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Обучение нейронных сетей 2</title><link>https://mldl.ru/posts/convnets-4/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Обучение нейронных сетей&lt;/h2&gt;
&lt;p&gt;Содержание:
- &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Генерация некоторых данных&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Обучение линейного классификатора Softmax&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Инициализируйте параметры&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Подсчитайте баллы за класс&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Вычислите потери&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Вычисление аналитического градиента с обратным распространением&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Выполнение обновления параметров&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Сведение всего этого воедино: обучение классификатора Softmax&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Обучение нейронной сети&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Краткая сводка&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-4/"&gt;Дополнительные материалы&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;В этом разделе мы рассмотрим полную реализацию игрушечной нейронной сети в двух измерениях. Сначала мы реализуем простой линейный классификатор, а затем расширим код до двухслойной нейронной сети. Как мы увидим, это расширение на удивление простое, и требуется внести совсем немного изменений.  &lt;/p&gt;
&lt;h2&gt;Генерация некоторых данных&lt;/h2&gt;
&lt;p&gt;Давайте создадим набор данных для классификации, который нелегко разделить на линейные классы. Наш любимый пример — набор данных «спираль», который можно создать следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;per&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dimensionality&lt;/span&gt;
&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;each&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;row&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;single&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'uint8'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;radius&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c_&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;r*np.sin(t), r*np.cos(t)&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;
&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lets&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;visualize&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;:, 0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;:, 1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Spectral&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/eg/spiral_raw.png"&gt;  &lt;/p&gt;
&lt;p&gt;Данные игрушечной спирали состоят из трёх классов (синий, красный, жёлтый), которые нельзя разделить линейно.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Обычно мы хотим предварительно обработать набор данных, чтобы среднее значение каждого признака было равно нулю, а стандартное отклонение — единице, но в данном случае признаки уже находятся в диапазоне от &lt;strong&gt;-1 до 1&lt;/strong&gt;, поэтому мы пропускаем этот шаг.  &lt;/p&gt;
&lt;h2&gt;Обучение линейного классификатора Softmax&lt;/h2&gt;
&lt;h3&gt;Инициализируйте параметры&lt;/h3&gt;
&lt;p&gt;Давайте сначала обучим классификатор &lt;strong&gt;Softmax&lt;/strong&gt; на этом наборе данных для классификации. Как мы видели в предыдущих разделах, классификатор &lt;strong&gt;Softmax&lt;/strong&gt; имеет линейную функцию оценки и использует функцию потерь кросс-энтропии. Параметры линейного классификатора состоят из весовой матрицы &lt;code&gt;W&lt;/code&gt; и вектора смещения &lt;code&gt;b&lt;/code&gt; для каждого класса. Давайте сначала инициализируем эти параметры случайными числами:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; initialize parameters randomly
W = 0.01 * np.random.randn(D,K)
b = np.zeros((1,K))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Напомним, что &lt;code&gt;D = 2&lt;/code&gt; — это размерность, а &lt;code&gt;K = 3&lt;/code&gt; — количество классов.  &lt;/p&gt;
&lt;h3&gt;Подсчитайте баллы за класс&lt;/h3&gt;
&lt;p&gt;Поскольку это линейный классификатор, мы можем очень просто вычислить оценки для всех классов параллельно с помощью одного умножения матриц:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;scores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;linear&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;classifier&lt;/span&gt;
&lt;span class="nv"&gt;scores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;dot&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;W&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;b&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;В этом примере у нас есть &lt;strong&gt;300 двумерных точек&lt;/strong&gt;, поэтому после этого умножения массив &lt;code&gt;scores&lt;/code&gt; будет иметь размер &lt;strong&gt;[300 x 3]&lt;/strong&gt;, где каждая строка содержит баллы за классы, соответствующие трём классам (синий, красный, жёлтый).&lt;/p&gt;
&lt;h3&gt;Вычислите потери&lt;/h3&gt;
&lt;p&gt;Второй ключевой компонент, который нам нужен, — это функция потерь, представляющая собой дифференцируемую целевую функцию, которая количественно оценивает наше недовольство вычисленными оценками классов. Интуитивно понятно, что мы хотим, чтобы правильный класс имел более высокую оценку, чем другие классы. В этом случае потери должны быть низкими, а в противном случае — высокими. Существует множество способов количественно оценить эту интуитивную догадку, но в этом примере мы будем использовать потери перекрёстной энтропии, которые связаны с классификатором Softmax. Напомним, что если &lt;strong&gt;f&lt;/strong&gt; — это массив оценок классов для одного примера (например, массив из трёх чисел), тогда классификатор &lt;strong&gt;Softmax&lt;/strong&gt; вычисляет потерю для этого примера следующим образом:  &lt;/p&gt;
&lt;p&gt;$$
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)
$$  &lt;/p&gt;
&lt;p&gt;Мы можем видеть, что классификатор &lt;strong&gt;Softmax&lt;/strong&gt; интерпретирует каждый элемент &lt;strong&gt;f&lt;/strong&gt;. В качестве входных данных используются (ненормализованные) логарифмические вероятности трёх классов. Мы возводим их в степень, чтобы получить (&lt;em&gt;ненормализованные&lt;/em&gt;) вероятности, а затем нормализуем их, чтобы получить вероятности. Таким образом, выражение внутри логарифма — это нормализованная вероятность правильного класса. Обратите внимание на то, как работает это выражение: эта величина всегда находится в диапазоне от &lt;strong&gt;0 до 1&lt;/strong&gt;. Когда вероятность правильного класса очень мала (&lt;strong&gt;близка к 0&lt;/strong&gt;), потери будут стремиться к (положительной) бесконечности. И наоборот, когда вероятность правильного класса приближается к &lt;strong&gt;1&lt;/strong&gt;, потери приближаются к нулю, потому что &lt;strong&gt;log(1)=0&lt;/strong&gt;. Следовательно, выражение для &lt;strong&gt;\(L_i\)&lt;/strong&gt;. Вероятность правильного класса низкая, когда она высока, и очень высокая, когда она низка.   &lt;/p&gt;
&lt;p&gt;Напомним также, что полная потеря классификатора &lt;strong&gt;Softmax&lt;/strong&gt; определяется как средняя потеря кросс-энтропии по обучающим примерам и регуляризация:  &lt;/p&gt;
&lt;p&gt;$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }&lt;em k_l="k,l"&gt;\text{потеря данных} + \underbrace{ \frac{1}{2} \lambda \sum_k\sum_l W&lt;/em&gt; \\
$$  }^2 }_\text{потеря регуляризации&lt;/p&gt;
&lt;p&gt;Учитывая массив &lt;code&gt;scores&lt;/code&gt; значений, которые мы вычислили выше, мы можем вычислить потери. Во-первых, способ получения вероятностей прост:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;num_examples = X.shape[0]
&lt;span class="gh"&gt;#&lt;/span&gt; get unnormalized probabilities
exp_scores = np.exp(scores)
&lt;span class="gh"&gt;#&lt;/span&gt; normalize them for each example
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Теперь у нас есть массив &lt;code&gt;probs&lt;/code&gt; размером [300 x 3], где каждая строка содержит вероятности классов. В частности, поскольку мы их нормализовали, сумма значений в каждой строке равна единице. Теперь мы можем запросить логарифмические вероятности, присвоенные правильным классам в каждом примере:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;orrect_logprobs = -np.log(probs[range(num_examples),y])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Массив &lt;code&gt;correct_logprobs&lt;/code&gt; — это одномерный массив, содержащий только вероятности, присвоенные правильным классам для каждого примера. &lt;strong&gt;Полная потеря&lt;/strong&gt; — это среднее значение этих логарифмических вероятностей и потери от регуляризации:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;loss:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cross&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;
&lt;span class="n"&gt;data_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_logprobs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt;
&lt;span class="n"&gt;reg_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kt"&gt;reg&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;reg_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;В этом коде сила регуляризации &lt;strong&gt;λ&lt;/strong&gt; хранится внутри &lt;code&gt;reg&lt;/code&gt;. Коэффициент удобства &lt;code&gt;0.5&lt;/code&gt; умножения регуляризации станет ясен через секунду. Оценка этого вначале (со случайными параметрами) может дать нам &lt;code&gt;loss = 1.1&lt;/code&gt;, что и есть &lt;code&gt;-np.log(1.0/3)&lt;/code&gt;, поскольку при небольших начальных случайных весах все вероятности, присвоенные всем классам, составляют около одной трети. Теперь мы хотим сделать потери как можно более низкими, используя &lt;code&gt;loss = 0&lt;/code&gt; в качестве абсолютной нижней границы. Но чем меньше потери, тем выше вероятности, присвоенные правильным классам для всех примеров.  &lt;/p&gt;
&lt;h3&gt;Вычисление аналитического градиента с обратным распространением&lt;/h3&gt;
&lt;p&gt;У нас есть способ оценки потерь, и теперь нам нужно их минимизировать. Мы сделаем это с помощью градиентного спуска. То есть мы начнём со случайных параметров (как показано выше) и вычислим градиент функции потерь по отношению к параметрам, чтобы знать, как изменить параметры для уменьшения потерь. Давайте введём промежуточную переменную &lt;strong&gt;p&lt;/strong&gt;, который представляет собой вектор (&lt;em&gt;нормализованных&lt;/em&gt;) вероятностей. Потери для одного примера составляют:  &lt;/p&gt;
&lt;p&gt;$$
p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)
$$  &lt;/p&gt;
&lt;p&gt;Теперь мы хотим понять, как вычисляются баллы внутри &lt;strong&gt;f&lt;/strong&gt; следует изменить, чтобы уменьшить потери &lt;strong&gt;\(L_i\)&lt;/strong&gt;, что этот пример соответствует общей цели. Другими словами, мы хотим вычислить градиент &lt;strong&gt;\( \partial L_i / \partial f_k \)&lt;/strong&gt;. Потеря __\(L_i\)__вычисляется из &lt;strong&gt;p&lt;/strong&gt;, что , в свою очередь , зависит от &lt;strong&gt;f&lt;/strong&gt;. Читателю будет интересно использовать правило дифференцирования сложной функции для вычисления градиента, но в итоге всё оказывается очень простым и понятным, после того как многое сокращается:  &lt;/p&gt;
&lt;p&gt;$$
\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)
$$  &lt;/p&gt;
&lt;p&gt;Обратите внимание, насколько элегантно и просто выглядит это выражение. Предположим, что вычисленные нами вероятности были &lt;code&gt;p = [0.2, 0.3, 0.5]&lt;/code&gt; и что правильным классом был средний (&lt;strong&gt;с вероятностью 0,3&lt;/strong&gt;). Согласно этому выводу, градиент оценок будет равен &lt;code&gt;df = [0.2, -0.7, 0.5]&lt;/code&gt;. Вспомнив, что означает интерпретация градиента, мы видим, что этот результат вполне интуитивен: увеличение первого или последнего элемента вектора оценок &lt;strong&gt;f&lt;/strong&gt; (оценок неверных классов) приводит к &lt;em&gt;увеличению&lt;/em&gt; потерь (из-за положительных значений &lt;strong&gt;+0,2 и +0,5&lt;/strong&gt;) — а увеличение потерь плохо, как и ожидалось. Однако увеличение оценки правильного класса отрицательно влияет на потери. Градиент &lt;strong&gt;-0,7&lt;/strong&gt; говорит нам о том, что увеличение оценки правильного класса приведёт к уменьшению потерь &lt;strong&gt;\(L_i\)&lt;/strong&gt;, что имеет смысл.  &lt;/p&gt;
&lt;p&gt;Всё это сводится к следующему коду. Напомним, что &lt;code&gt;probs&lt;/code&gt; хранит вероятности всех классов (в виде строк) для каждого примера. Чтобы получить градиент оценок, который мы называем &lt;code&gt;dscores&lt;/code&gt;, мы поступаем следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;dscores = probs
dscores[range(num_examples),y] -= 1
dscores /= num_examples
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Наконец, у нас есть &lt;code&gt;scores = np.dot(X, W) + b&lt;/code&gt; и, вооружившись градиентом &lt;code&gt;scores&lt;/code&gt; (хранящимся в &lt;em&gt;dscores&lt;/em&gt;), мы можем выполнить обратное распространение ошибки в &lt;code&gt;W&lt;/code&gt; и &lt;code&gt;b&lt;/code&gt;:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;dW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;keepdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;reg&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;don&lt;/span&gt;&lt;span class="p"&gt;'&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;forget&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Здесь мы видим, что мы выполнили обратное преобразование с помощью операции умножения матриц, а также добавили вклад от регуляризации. Обратите внимание, что градиент регуляризации имеет очень простую форму &lt;code&gt;reg*W&lt;/code&gt;, поскольку мы использовали константу &lt;code&gt;0.5&lt;/code&gt; для вклада в потери (т. е. &lt;strong&gt;\(\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda w\)&lt;/strong&gt; ). Это распространенный удобный прием, который упрощает выражение градиента.  &lt;/p&gt;
&lt;h3&gt;Выполнение обновления параметров&lt;/h3&gt;
&lt;p&gt;Теперь, когда мы вычислили градиент, мы знаем, как каждый параметр влияет на функцию потерь. Теперь мы обновим параметры в направлении &lt;em&gt;отрицательного&lt;/em&gt; градиента, чтобы &lt;em&gt;уменьшить&lt;/em&gt; потери:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; perform a parameter update
W += -step_size &lt;span class="gs"&gt;* dW&lt;/span&gt;
&lt;span class="gs"&gt;b += -step_size *&lt;/span&gt; db
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Сведение всего этого воедино: обучение классификатора Softmax&lt;/h3&gt;
&lt;p&gt;Если собрать всё это воедино, получится полный код для обучения классификатора &lt;strong&gt;Softmax&lt;/strong&gt; с помощью градиентного спуска:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Train&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Classifier&lt;/span&gt;

&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;randomly&lt;/span&gt;
&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;some&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hyperparameters&lt;/span&gt;
&lt;span class="n"&gt;step_size&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1e-0&lt;/span&gt;
&lt;span class="kt"&gt;reg&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;strength&lt;/span&gt;

&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;descent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loop&lt;/span&gt;
&lt;span class="n"&gt;num_examples&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mh"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;exp_scores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exp_scores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;keepdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;loss:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cross&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;correct_logprobs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;data_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_logprobs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;reg_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kt"&gt;reg&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data_loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;reg_loss&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"iteration %d: loss %f"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;backpropate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dscores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;keepdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;dW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;reg&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;perform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;parameter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;step_size&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dW&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;step_size&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="err"&gt;```&lt;/span&gt;


&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;При&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;выполнении&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;этой&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;операции&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;выводятся&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;выходные&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;данные&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;```&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.096956&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.917265&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;20&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.851503&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;30&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.822336&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;40&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.807586&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;50&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.799448&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;60&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.794681&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;70&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.791764&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;80&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.789920&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;90&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.788726&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;100&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.787938&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;110&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.787409&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;120&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.787049&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;130&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786803&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;140&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786633&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;150&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786514&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;160&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786431&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;170&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786373&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;180&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786331&lt;/span&gt;
&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;190&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.786302&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Мы видим, что после примерно &lt;strong&gt;190&lt;/strong&gt; итераций мы приблизились к чему-то. Мы можем оценить точность обучающего набора данных:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; evaluate training set accuracy
scores = np.dot(X, W) + b
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Это выводит &lt;strong&gt;49%&lt;/strong&gt;. Не очень хорошо, но и неудивительно, учитывая, что набор данных составлен таким образом, что он не является линейно разделимым. Мы также можем построить границы принятых решений:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/eg/spiral_linear.png"&gt;&lt;br&gt;
Линейный классификатор не может изучить набор данных &lt;em&gt;toy spiral&lt;/em&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Обучение нейронной сети&lt;/h2&gt;
&lt;p&gt;Очевидно, что линейный классификатор не подходит для этого набора данных, и мы хотели бы использовать нейронную сеть. Для этих игрушечных данных будет достаточно одного дополнительного скрытого слоя. Теперь нам понадобятся два набора весовых коэффициентов и смещений (&lt;em&gt;для первого и второго слоев&lt;/em&gt;):  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; initialize parameters randomly
h = 100 # size of hidden layer
W = 0.01 &lt;span class="gs"&gt;* np.random.randn(D,h)&lt;/span&gt;
&lt;span class="gs"&gt;b = np.zeros((1,h))&lt;/span&gt;
&lt;span class="gs"&gt;W2 = 0.01 *&lt;/span&gt; np.random.randn(h,K)
b2 = np.zeros((1,K))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Прямой проход для подсчета очков теперь меняет форму:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; evaluate class scores with a 2-layer Neural Network
hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
scores = np.dot(hidden_layer, W2) + b2
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что единственное отличие от предыдущего варианта — это одна дополнительная строка кода, в которой мы сначала вычисляем представление скрытого слоя, а затем баллы на основе этого скрытого слоя. Важно отметить, что мы также добавили нелинейность, которая в данном случае представляет собой простую функцию &lt;strong&gt;ReLU&lt;/strong&gt;, устанавливающую пороговое значение активации скрытого слоя на нуле.  &lt;/p&gt;
&lt;p&gt;Всё остальное остаётся прежним. Мы вычисляем потери на основе оценок точно так же, как и раньше, и получаем градиент для оценок &lt;code&gt;dscores&lt;/code&gt; точно так же, как и раньше. Однако способ обратного распространения этого градиента на параметры модели, конечно, меняется. Сначала давайте выполним обратное распространение для второго слоя нейронной сети. Это выглядит так же, как и код для классификатора &lt;strong&gt;Softmax&lt;/strong&gt;, за исключением того, что мы заменяем &lt;code&gt;X&lt;/code&gt; (исходные данные) на переменную &lt;code&gt;hidden_layer&lt;/code&gt;):  &lt;/p&gt;
&lt;p&gt;```# backpropate the gradient to the parameters&lt;/p&gt;
&lt;h2&gt;first backprop into parameters W2 and b2&lt;/h2&gt;
&lt;p&gt;dW2 = np.dot(hidden_layer.T, dscores)
db2 = np.sum(dscores, axis=0, keepdims=True)&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;Однако&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;отличие&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;от&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;предыдущего&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;случая&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ещё&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;не&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;закончили&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;потому&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;что&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`hidden_layer`&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сама&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;является&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;функцией&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;других&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;параметров&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;и&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;данных&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Нам&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;нужно&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;продолжить&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обратное&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;распространение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ошибки&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;через&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;эту&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;переменную&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Её&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;можно&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вычислить&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;следующим&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;образом&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;dhidden = np.dot(dscores, W2.T)&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;Теперь у нас есть градиент на выходе скрытого слоя. Далее нам нужно выполнить обратное распространение ошибки для нелинейности **ReLU**. Это оказывается простым, потому что **ReLU** при обратном распространении ошибки фактически является переключателем. Поскольку **r=max(0,x)**, у нас есть это **dr/dx=1(x&amp;gt;0)**. В сочетании с правилом дифференцирования по частям мы видим, что блок **ReLU** пропускает градиент без изменений, если его входные данные больше 0, но &lt;span class="ge"&gt;_отменяет_&lt;/span&gt; его, если входные данные меньше нуля во время прямого прохода. Следовательно, мы можем выполнить обратное распространение ошибки для **ReLU** следующим образом:  
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;backprop the ReLU non-linearity&lt;/h2&gt;
&lt;p&gt;dhidden[hidden_layer &amp;lt;= 0] = 0&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;И теперь мы, наконец, переходим к первому слою весов и смещений:  
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;finally into W,b&lt;/h2&gt;
&lt;p&gt;dW = np.dot(X.T, dhidden)
db = np.sum(dhidden, axis=0, keepdims=True)&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;**Готово!** У нас есть градиенты &lt;span class="sb"&gt;`dW,db,dW2,db2`&lt;/span&gt; и мы можем выполнить обновление параметров. Всё остальное остаётся без изменений. Полный код выглядит очень похоже:  
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;initialize parameters randomly&lt;/h2&gt;
&lt;p&gt;h = 100 # size of hidden layer
W = 0.01 * np.random.randn(D,h)
b = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,K)
b2 = np.zeros((1,K))&lt;/p&gt;
&lt;h2&gt;some hyperparameters&lt;/h2&gt;
&lt;p&gt;step_size = 1e-0
reg = 1e-3 # regularization strength&lt;/p&gt;
&lt;h2&gt;gradient descent loop&lt;/h2&gt;
&lt;p&gt;num_examples = X.shape[0]
for i in range(10000):&lt;/p&gt;
&lt;p&gt;# evaluate class scores, [N x K]
  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
  scores = np.dot(hidden_layer, W2) + b2&lt;/p&gt;
&lt;p&gt;# compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]&lt;/p&gt;
&lt;p&gt;# compute the loss: average cross-entropy loss and regularization
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5&lt;em&gt;reg&lt;/em&gt;np.sum(W&lt;em&gt;W) + 0.5&lt;/em&gt;reg&lt;em&gt;np.sum(W2&lt;/em&gt;W2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)&lt;/p&gt;
&lt;p&gt;# compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples&lt;/p&gt;
&lt;p&gt;# backpropate the gradient to the parameters
  # first backprop into parameters W2 and b2
  dW2 = np.dot(hidden_layer.T, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=True)
  # next backprop into hidden layer
  dhidden = np.dot(dscores, W2.T)
  # backprop the ReLU non-linearity
  dhidden[hidden_layer &amp;lt;= 0] = 0
  # finally into W,b
  dW = np.dot(X.T, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=True)&lt;/p&gt;
&lt;p&gt;# add regularization gradient contribution
  dW2 += reg * W2
  dW += reg * W&lt;/p&gt;
&lt;p&gt;# perform a parameter update
  W += -step_size * dW
  b += -step_size * db
  W2 += -step_size * dW2
  b2 += -step_size * db2
  ```&lt;/p&gt;
&lt;p&gt;Это печатает:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;iteration 0: loss 1.098744
iteration 1000: loss 0.294946
iteration 2000: loss 0.259301
iteration 3000: loss 0.248310
iteration 4000: loss 0.246170
iteration 5000: loss 0.245649
iteration 6000: loss 0.245491
iteration 7000: loss 0.245400
iteration 8000: loss 0.245335
iteration 9000: loss 0.245292
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Точность обучения теперь равна:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; evaluate training set accuracy
hidden_layer = np.maximum(0, np.dot(X, W) + b)
scores = np.dot(hidden_layer, W2) + b2
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Что выводит &lt;strong&gt;98%&lt;/strong&gt;!. Мы также можем визуализировать границы решений:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/eg/spiral_net.png"&gt;&lt;br&gt;
Классификатор нейронной сети сжимает набор данных &lt;em&gt;spiral&lt;/em&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Краткие сведения&lt;/h2&gt;
&lt;p&gt;Мы работали с игрушечным 2D-набором данных и обучали как линейную сеть, так и двухслойную нейронную сеть. Мы увидели, что переход от линейного классификатора к нейронной сети требует очень мало изменений в коде. Функция оценки меняет свою форму (разница в 1 строке кода), а обратное распространение ошибки меняет свою форму (нам нужно выполнить ещё один цикл обратного распространения ошибки через скрытый слой к первому слою сети).  &lt;/p&gt;
&lt;h2&gt;Дополнительные материалы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Возможно, вам захочется взглянуть на этот код &lt;strong&gt;IPython Notebook&lt;/strong&gt; &lt;a href="http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html"&gt;отображаемый в формате HTML&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Или загрузите &lt;a href="http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.ipynb"&gt;файл ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/convnets-4/</guid><pubDate>Tue, 11 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Обучение нейронных сетей</title><link>https://mldl.ru/posts/convnets-3/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Обучение нейронных сетей&lt;/h2&gt;
&lt;p&gt;Содержание:
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Проверка градиента&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Проверки здравомыслия&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Присмотр за процессом обучения&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Функция потерь&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Точность поезда/вала&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Соотношение весов:обновлений &lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Распределение активации/градиента на слой&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Визуализация&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Обновление параметров&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Первый порядок (SGD), импульс, импульс Нестерова&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Отжиг темпов обучения&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Методы второго порядка&lt;/a&gt;
        - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Дополнительные материалы&lt;/a&gt; 
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Методы адаптивной скорости обучения для каждого параметра (Adagrad, RMSProp)&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Оптимизация гиперпараметров&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Оценка&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Модельные ансамбли&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Краткая сводка&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-3/"&gt;Дополнительные материалы&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Обучение&lt;/h2&gt;
&lt;p&gt;В предыдущих разделах мы рассмотрели статические части нейронных сетей: как мы можем настроить сетевую связность, данные и функцию потерь. Этот раздел посвящен динамике, или другими словами, процессу изучения параметров и нахождения хороших гиперпараметров.  &lt;/p&gt;
&lt;h2&gt;Проверка градиента&lt;/h2&gt;
&lt;p&gt;Теоретически выполнить проверку градиента так же просто, как сравнить аналитический градиент с числовым градиентом. На практике этот процесс гораздо более сложный и подвержен ошибкам. Вот несколько советов, рекомендаций и проблем, на которые следует обратить внимание:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Используйте формулу по центру&lt;/strong&gt;. Формула, которую вы, возможно, видели для приближения конечных разностей при вычислении численного градиента, выглядит следующим образом:&lt;/p&gt;
&lt;p&gt;$$
\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}
$$  &lt;/p&gt;
&lt;p&gt;где &lt;strong&gt;h&lt;/strong&gt; это очень небольшое число, на практике примерно &lt;strong&gt;1e-5&lt;/strong&gt; или около того. На практике оказывается, что гораздо лучше использовать формулу центрированной разности вида:  &lt;/p&gt;
&lt;p&gt;$$
\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}
$$  &lt;/p&gt;
&lt;p&gt;Для этого вам придется дважды оценить функцию потерь, чтобы проверить каждое измерение градиента (так что это примерно в 2 раза дороже), но аппроксимация градиента оказывается гораздо более точной. Чтобы убедиться в этом, можно использовать разложение Тейлора  &lt;strong&gt;\(f(x+h)\)&lt;/strong&gt; и  &lt;strong&gt;\(f(x-h)\)&lt;/strong&gt; и убедитесь, что первая формула содержит ошибку порядка &lt;strong&gt;O(h)&lt;/strong&gt;, в то время как вторая формула содержит только члены ошибки порядка &lt;strong&gt;\(O(h^2)\)&lt;/strong&gt; (&lt;em&gt;т.е. это приближение второго порядка&lt;/em&gt;).  &lt;/p&gt;
&lt;p&gt;Используйте относительную погрешность для сравнения. В чем особенности сравнения численного градиента &lt;strong&gt;\(f'_n\)&lt;/strong&gt; и аналитический градиент &lt;strong&gt;\(f'_a\)&lt;/strong&gt;? То есть, как мы узнаем, что они несовместимы? У вас может возникнуть соблазн отслеживать разницу &lt;strong&gt;\(\mid f'_a - f'_n \mid \)&lt;/strong&gt; или его квадрат и определите проверку градиента как неудачную, если эта разница превышает пороговое значение. Однако это проблематично. Для примера рассмотрим случай, когда их разница равна &lt;strong&gt;1e-4&lt;/strong&gt;. Это кажется очень подходящей разницей, если два градиента близки к &lt;strong&gt;1.0&lt;/strong&gt;, поэтому мы считаем, что два градиента совпадают. Но если бы оба градиента были порядка &lt;strong&gt;1e-5&lt;/strong&gt; или ниже, то мы бы считали &lt;strong&gt;1e-4&lt;/strong&gt; огромной разницей и, скорее всего, неудачей. Следовательно, всегда более уместно учитывать &lt;em&gt;относительную ошибку&lt;/em&gt;:  &lt;/p&gt;
&lt;p&gt;$$
\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}
$$  &lt;/p&gt;
&lt;p&gt;которая рассматривает отношение их разностей к отношению абсолютных значений обоих градиентов. Обратите внимание, что обычно формула относительной ошибки включает только один из двух членов (любой из них), но я предпочитаю увеличивать (или добавлять) оба, чтобы сделать его симметричным и предотвратить деление на ноль в случае, когда одно из двух равно нулю (&lt;em&gt;что часто случается, особенно с ReLU&lt;/em&gt;). Тем не менее, необходимо явно отслеживать случай, когда оба равны нулю, и пройти проверку градиента в этом крайнем случае. На практике:&lt;br&gt;
- Относительная погрешность &lt;strong&gt;&amp;gt; 1e-2&lt;/strong&gt; обычно означает, что градиент, вероятно, неправильный
- &lt;strong&gt;1e-2 &amp;gt;&lt;/strong&gt; относительная погрешность &lt;strong&gt;&amp;gt; 1e-4&lt;/strong&gt; должна заставить вас чувствовать себя некомфортно
- &lt;strong&gt;1e-4 &amp;gt;&lt;/strong&gt; относительная погрешность обычно приемлема для целей с изломами. Но если нет перегибов (например, использование нелинейностей &lt;strong&gt;tanh&lt;/strong&gt; и &lt;strong&gt;softmax&lt;/strong&gt;), то &lt;strong&gt;1e-4&lt;/strong&gt; слишком велико.
- &lt;strong&gt;1e-7&lt;/strong&gt; и меньше вы должны быть счастливы.  &lt;/p&gt;
&lt;p&gt;Также имейте в виду, что чем глубже сеть, тем выше будут относительные ошибки. Таким образом, если вы проверяете входные данные для 10-слойной сети, относительная ошибка &lt;strong&gt;1e-2&lt;/strong&gt; может быть нормальной, потому что ошибки накапливаются по мере прохождения. И наоборот, ошибка &lt;strong&gt;1e-2&lt;/strong&gt; для одной дифференцируемой функции, скорее всего, указывает на неправильный градиент.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Используйте двойную точность&lt;/strong&gt;. Распространенной ошибкой является использование плавающей точки одинарной точности для вычисления проверки градиента. Часто бывает так, что вы можете получить высокие относительные ошибки (до &lt;strong&gt;1e-2&lt;/strong&gt;) даже при правильной реализации градиента. По моему опыту, я иногда видел, как мои относительные ошибки резко уменьшались с &lt;strong&gt;1e-2&lt;/strong&gt; до &lt;strong&gt;1e-8&lt;/strong&gt; при переходе на двойную точность.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Оставайтесь в активном диапазоне плавающей запятой&lt;/strong&gt;. Хорошей идеей будет прочитать &lt;a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"&gt;статью «Что каждый специалист по информатике должен знать об арифметике с плавающей запятой»&lt;/a&gt;, так как это может развеять мифы об ошибках и позволить вам писать более тщательный код. Например, в нейронных сетях может быть распространена нормализация функции потерь по пакету.  &lt;/p&gt;
&lt;p&gt;Однако, если градиенты для каждой точки данных очень малы, то &lt;em&gt;дополнительное&lt;/em&gt; деление их на количество точек данных начинает давать очень маленькие числа, что, в свою очередь, приведет к большему количеству числовых проблем. Вот почему я предпочитаю всегда печатать исходный числовой/аналитический градиент и следить за тем, чтобы числа, которые вы сравниваете, не были слишком маленькими (например, примерно &lt;strong&gt;1e-10&lt;/strong&gt; и меньше по абсолютному значению вызывает беспокойство). Если это так, вы можете временно масштабировать функцию потерь на константу, чтобы привести их к "более хорошему" диапазону, где числа с плавающей запятой более плотные - в идеале порядка &lt;strong&gt;1,0&lt;/strong&gt;, где экспонента с плавающей запятой равна &lt;strong&gt;0&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Изгибы в достижении цели.&lt;/strong&gt; Одним из источников неточностей, о которых следует знать при проверке градиента, является проблема &lt;em&gt;изгибов&lt;/em&gt;. Изгибы относятся к недифференцируемым частям целевой функции, вводимым такими функциями, как &lt;em&gt;ReLU&lt;/em&gt;  &lt;strong&gt;(\(max(0,x)\))&lt;/strong&gt;
) или потеря &lt;strong&gt;SVM&lt;/strong&gt;, нейроны &lt;em&gt;Maxout&lt;/em&gt; и т.д. Рассмотрим градиентную проверку функции &lt;em&gt;ReLU&lt;/em&gt; по адресу &lt;strong&gt;\(x = -1e6\)&lt;/strong&gt;. С &lt;strong&gt;\(x &amp;lt; 0\)&lt;/strong&gt;, аналитический градиент в этой точке равен нулю. Однако числовой градиент внезапно вычислит ненулевой градиент, потому что &lt;strong&gt;\(f(x+h)\)&lt;/strong&gt; может пересечь излом (например, если &lt;strong&gt;\(h &amp;gt; 1e-6\)&lt;/strong&gt; ) и ввести ненулевой взнос. Вы можете подумать, что это патологический случай, но на самом деле этот случай может быть очень распространенным. Например, СВМ для CIFAR-10 содержит до 450 000 &lt;strong&gt;(\(max(0,x)\))&lt;/strong&gt; термины, потому что существует 50 000 примеров, и каждый пример дает 9 терминов для цели. Более того, нейронная сеть с классификатором SVM будет содержать гораздо больше изломов из-за &lt;em&gt;ReLU&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;Обратите внимание, что можно узнать, был ли пересечен излом при оценке убытка. Это можно сделать, отслеживая личности всех «победителей» в функции формы &lt;strong&gt;(\(max(0,x)\))&lt;/strong&gt;; То есть был &lt;strong&gt;x&lt;/strong&gt; или &lt;strong&gt;y&lt;/strong&gt; выше во время паса вперед. Если при оценке изменилась личность хотя бы одного победителя &lt;strong&gt;\(f(x+h)\)&lt;/strong&gt;, а в последствии &lt;strong&gt;\(f(x-h)\)&lt;/strong&gt;, то был пересечен излом и числовой градиент не будет точным.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Используйте только несколько точек данных&lt;/strong&gt;. Одним из решений вышеупомянутой проблемы перегибов является использование меньшего количества точек данных, поскольку функции потерь, которые содержат перегибы (например, из-за использования &lt;em&gt;ReLU&lt;/em&gt; или маржинальных потерь и т. д.), будут иметь меньше перегибов с меньшим количеством точек данных, поэтому вероятность того, что вы пересечете одну из них, при выполнении конечного другого приближения, снижается. Более того, если ваш &lt;em&gt;gradcheck&lt;/em&gt; всего на &lt;strong&gt;~2&lt;/strong&gt; или &lt;strong&gt;3&lt;/strong&gt; точки данных, то вы почти наверняка проверите весь пакет. Использование очень небольшого количества точек данных также делает проверку градиента быстрее и эффективнее.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Будьте осторожны с размером шага h&lt;/strong&gt;. Не обязательно минимальный размер &lt;strong&gt;h&lt;/strong&gt;- это хорошо, так как в таком случае есть шанс напороться на проблему численной точности. Иногда, при проверке корректности градиента &lt;strong&gt;h&lt;/strong&gt;, возможно, что значение &lt;strong&gt;1е-4&lt;/strong&gt; и &lt;strong&gt;1е-6&lt;/strong&gt; будет изменяться от абсолютно неверному при увеличении этого показателя. &lt;a href="http://en.wikipedia.org/wiki/Numerical_differentiation"&gt;Эта статья в Википедии&lt;/a&gt; содержит диаграмму, которая отображает значение &lt;strong&gt;h&lt;/strong&gt; по оси &lt;strong&gt;x&lt;/strong&gt; и числовую ошибку градиента по оси &lt;strong&gt;y&lt;/strong&gt;.    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Градусная проверка во время «характерного» режима работы&lt;/strong&gt;. Важно понимать, что проверка градиента выполняется в определенной (и обычно случайной), единственной точке в пространстве параметров. Даже если проверка градиента на этом этапе выполнена успешно, не сразу можно быть уверенным в том, что градиент правильно реализован глобально. Кроме того, случайная инициализация может быть не самой «характерной» точкой в пространстве параметров и фактически может привести к патологическим ситуациям, когда градиент кажется правильно реализованным, но на самом деле это не так. Например, &lt;em&gt;SVM&lt;/em&gt; с очень малой инициализацией веса присвоит почти ровно нулевые оценки всем точкам данных, а градиенты будут демонстрировать определенную закономерность во всех точках данных. Неправильная реализация градиента все равно может привести к появлению этого шаблона и не привести к более характерному режиму работы, в котором одни баллы больше других. Поэтому, чтобы быть в безопасности, лучше всего использовать короткое время &lt;strong&gt;прогорания&lt;/strong&gt;, в течение которого сеть может обучиться и выполнить градиентную проверку после того, как потери начнут снижаться. Опасность его выполнения на первой итерации заключается в том, что это может привести к патологическим пограничным случаям и замаскировать неправильную реализацию градиента.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Не позволяйте регуляризации перегружать данные&lt;/strong&gt;. Часто бывает так, что функция потерь является суммой потерь данных и потерь от регуляризации (например, штраф &lt;strong&gt;\(L_2\)&lt;/strong&gt; за веса). Одна из опасностей, о которой следует знать, заключается в том, что потеря регуляризации может превзойти потерю данных, и в этом случае градиенты будут в основном исходить от члена регуляризации (который обычно имеет гораздо более простое выражение градиента). Это может замаскировать неправильную реализацию градиента потери данных. Поэтому рекомендуется отключить регуляризацию и проверять сначала только потерю данных, а затем второй и независимый термин регуляризации. Одним из способов выполнения последнего является взлом кода, чтобы устранить вклад потери данных. Другой способ состоит в том, чтобы увеличить силу регуляризации, чтобы гарантировать, что ее эффектом не будет пренебрежение при проверке градиента, и что будет замечена неправильная реализация.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Не забудьте отключить выпадение/аугментации&lt;/strong&gt;. Выполняя градиентную проверку, не забывайте отключать любые недетерминированные эффекты в сети, такие как выпадение, случайные аугментации данных и т. д. В противном случае это может привести к огромным ошибкам при оценке численного градиента. Недостатком отключения этих эффектов является то, что вы не будете проверять их градиент (например, может случиться так, что выпадение не будет правильно распространено). Следовательно, лучшим решением может быть принудительное использование определенного случайного начального значения перед оценкой обоих &lt;strong&gt;\(f(x+h)\) и \(f(x-h)\)&lt;/strong&gt;, а также при оценке аналитического градиента.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Проверьте только несколько размеров&lt;/strong&gt;. На практике градиенты могут иметь размеры в миллион параметров. В этих случаях целесообразно проверить только некоторые размеры градиента и предположить, что другие являются правильными. &lt;strong&gt;Будьте внимательны&lt;/strong&gt;: Один из вопросов, с которым следует быть осторожным, заключается в том, чтобы убедиться, что градиент проверяет несколько размеров для каждого отдельного параметра. В некоторых приложениях люди объединяют параметры в один большой вектор параметров для удобства. В этих случаях, например, смещения могут занимать лишь небольшое количество параметров из всего вектора, поэтому важно не выбирать случайным образом, а учитывать это и проверять, что все параметры получают правильные градиенты.  &lt;/p&gt;
&lt;h2&gt;Перед изучением: советы и рекомендации по проверке здравомыслия&lt;/h2&gt;
&lt;p&gt;Вот несколько проверок здравого смысла, которые вы могли бы провести, прежде чем погрузиться в дорогостоящую оптимизацию:
- &lt;strong&gt;Ищите правильный проигрыш при случайном исполнении&lt;/strong&gt;. Убедитесь, что вы получаете ожидаемые потери при инициализации с небольшими параметрами. Лучше всего сначала проверить только потерю данных (поэтому установите интенсивность регуляризации равной нулю). Например, для CIFAR-10 с классификатором &lt;strong&gt;Softmax&lt;/strong&gt; мы ожидаем, что начальный убыток составит &lt;strong&gt;2,302&lt;/strong&gt;, потому что мы ожидаем диффузную вероятность &lt;strong&gt;0,1&lt;/strong&gt; для каждого класса (поскольку классов 10), а &lt;strong&gt;Softmax&lt;/strong&gt; убыток — это отрицательная логарифмическая вероятность правильного класса, таким образом: &lt;strong&gt;-ln(0,1) = 2,302&lt;/strong&gt;. Для &lt;em&gt;The Weston Watkins SVM&lt;/em&gt; мы ожидаем, что все желаемые маржи будут нарушены (поскольку все баллы примерно равны нулю), и, следовательно, ожидаем потери &lt;strong&gt;9&lt;/strong&gt; (поскольку маржа равна &lt;strong&gt;1&lt;/strong&gt; для каждого неправильного класса). Если вы не видите этих потерь, возможно, возникла проблема с инициализацией.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;В качестве второй проверки здравомыслия, увеличение силы регуляризации должно привести к увеличению потерь &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Переобучение крошечного подмножества данных&lt;/strong&gt;. И последнее, и самое важное, прежде чем обучаться на полном наборе данных, попытайтесь обучиться на крошечной части (например, на &lt;strong&gt;20&lt;/strong&gt; примерах) ваших данных и убедитесь, что вы можете достичь нулевой стоимости. Для этого эксперимента также лучше всего установить регуляризацию равной нулю, иначе это может помешать получению нулевой стоимости. Если вы не пройдете эту проверку на здравомыслие с небольшим набором данных, не стоит переходить к полному набору данных. Обратите внимание, что может случиться так, что вы можете переобучать очень маленький набор данных, но все равно иметь неправильную реализацию. Например, если признаки точек данных являются случайными из-за какой-либо ошибки, то можно перенаучить небольшой обучающий набор, но вы никогда не заметите обобщения при свертывании всего набора данных.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Присмотр за процессом обучения&lt;/h2&gt;
&lt;p&gt;Существует множество полезных величин, которые вы должны отслеживать во время обучения нейронной сети. Эти графики являются окном в процесс обучения и должны использоваться для получения интуиции о различных настройках гиперпараметров и о том, как их следует изменить для более эффективного обучения.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ось x&lt;/strong&gt; приведенных ниже графиков всегда указывается в единицах эпох, которые измеряют, сколько раз каждый пример был замечен во время обучения в ожидании (например, одна эпоха означает, что каждый пример был просмотрен один раз). Предпочтительнее отслеживать эпохи, а не итерации, так как количество итераций зависит от произвольной настройки размера пакета.  &lt;/p&gt;
&lt;h3&gt;Функция потерь&lt;/h3&gt;
&lt;p&gt;Первая величина, которую полезно отслеживать во время тренировки, — это потери, так как они оцениваются по отдельным партиям во время паса вперед. Ниже приведена мультяшная диаграмма, показывающая потери с течением времени, и особенно то, что форма может рассказать вам о скорости обучения:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/learningrates.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn3/loss.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Сверху&lt;/strong&gt;: Мультфильм, изображающий эффекты различных скоростей обучения. При низких темпах обучения улучшения будут линейными. С высокими темпами обучения они начнут выглядеть более экспоненциально. Более высокие темпы обучения будут уменьшать потери быстрее, но они застревают на худших значениях потерь (зеленая линия). Это связано с тем, что в оптимизации слишком много «энергии», а параметры хаотично колеблются, не в силах занять хорошее место в ландшафте оптимизации. &lt;strong&gt;Снизу&lt;/strong&gt;: Пример типичной функции потерь во времени при обучении небольшой сети на наборе данных CIFAR-10. Эта функция потерь выглядит разумной (она может указывать на слишком маленькую скорость обучения, основанную на скорости распада, но трудно сказать), а также указывает на то, что размер партии может быть слишком низким (поскольку стоимость слишком зашумлена).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Величина «покачивания» в потерях связана с размером партии. Когда размер партии равен &lt;strong&gt;1&lt;/strong&gt;, покачивание будет относительно большим. Если размер пакета равен полному набору данных, покачивание будет минимальным, так как каждое обновление градиента должно монотонно улучшать функцию потерь (если только скорость обучения не установлена слишком высокой).  &lt;/p&gt;
&lt;p&gt;Некоторые пользователи предпочитают строить графики своих функций потерь в области журнала. Поскольку прогресс в обучении обычно принимает экспоненциальную форму, график выглядит как чуть более интерпретируемая прямая линия, а не как хоккейная клюшка. Кроме того, если на одном и том же графике потерь построить несколько моделей с перекрестной проверкой, различия между ними становятся более очевидными.  &lt;/p&gt;
&lt;p&gt;Иногда функции проигрыша могут выглядеть забавно &lt;a href="http://lossfunctions.tumblr.com/"&gt;lossfunctions.tumblr.com.&lt;/a&gt;  &lt;/p&gt;
&lt;h3&gt;Точность поезда/вала&lt;/h3&gt;
&lt;p&gt;Вторая важная величина, которую необходимо отслеживать при обучении классификатора, — это точность валидации/обучения. Этот график может дать вам ценную информацию о количестве переобучения в вашей модели:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/accuracies.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Разрыв между точностью обучения и валидации указывает на степень переобучения. Два возможных случая показаны на схеме слева. Синяя кривая ошибок валидации показывает очень низкую точность валидации по сравнению с точностью обучения, что указывает на сильное переобучение (обратите внимание, что точность валидации может даже начать снижаться после какого-то момента). Когда вы видите это на практике, вы, вероятно, захотите увеличить регуляризацию (сильнее штраф в весе &lt;strong&gt;\(L_2\)&lt;/strong&gt;, больше отсева и т.д.) или собрать больше данных. Другой возможный случай — когда точность валидации достаточно хорошо отслеживает точность обучения. Этот случай указывает на то, что емкость вашей модели недостаточно высока: увеличьте модель, увеличив количество параметров.  &lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Соотношение весов:обновления&lt;/h3&gt;
&lt;p&gt;Последняя величина, которую вы, возможно, захотите отслеживать, — это отношение величин обновления к величине значений. &lt;strong&gt;Примечание&lt;/strong&gt;: обновления, а не исходные градиенты (например, в ванильном &lt;em&gt;sgd&lt;/em&gt; это будет градиент, умноженный на скорость обучения). Возможно, вы захотите оценить и отследить это соотношение для каждого набора параметров независимо. Грубая эвристика заключается в том, что это соотношение должно быть где-то в районе &lt;strong&gt;1e-3&lt;/strong&gt;. Если он ниже, то скорость обучения может быть слишком низкой. Если он выше, то, скорее всего, уровень обучения слишком высок. Вот конкретный пример:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;assume&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;parameter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;its&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;gradient&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dW&lt;/span&gt;
&lt;span class="nx"&gt;param_scale&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;W&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;dW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;simple&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;SGD&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;
&lt;span class="nx"&gt;update_scale&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;linalg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nx"&gt;W&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;actual&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;
&lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;update_scale&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;param_scale&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;want&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Вместо того, чтобы отслеживать минимальное или максимальное значение, некоторые люди предпочитают вычислять и отслеживать норму градиентов и их обновлений. Эти метрики обычно коррелируют и часто дают примерно одинаковые результаты.  &lt;/p&gt;
&lt;h3&gt;Распределение активации/градиента на слой&lt;/h3&gt;
&lt;p&gt;Неправильная инициализация может замедлить или даже полностью затормозить процесс обучения. К счастью, эту проблему можно диагностировать относительно легко. Одним из способов сделать это является построение гистограмм активации/градиента для всех слоев сети. Интуитивно понятно, что не очень хорошо видеть какие-либо странные распределения - например, с нейронами &lt;em&gt;tanh&lt;/em&gt; мы хотели бы видеть распределение активаций нейронов между полным диапазоном &lt;strong&gt;[-1,1]&lt;/strong&gt;, вместо того, чтобы видеть, как все нейроны выдают ноль, или все нейроны полностью насыщаются либо при &lt;strong&gt;-1&lt;/strong&gt;, либо при &lt;strong&gt;1&lt;/strong&gt;.  &lt;/p&gt;
&lt;h3&gt;Визуализации первого уровня&lt;/h3&gt;
&lt;p&gt;Наконец, при работе с пикселями изображения может быть полезно и приятно визуально отобразить объекты первого слоя:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/weights.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn3/cnnweights.jpg"&gt;&lt;br&gt;
Примеры визуализированных весов для первого слоя нейронной сети. &lt;strong&gt;Сверху&lt;/strong&gt;: Зашумленные функции указывают на то, что симптомом может быть неконвергентная сеть, неправильно установленная скорость обучения, очень низкий вес штрафа за регуляризацию. &lt;strong&gt;Снизу&lt;/strong&gt;: Красивые, гладкие, чистые и разнообразные черты лица являются хорошим признаком того, что тренировка идет хорошо.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Обновление параметров&lt;/h2&gt;
&lt;p&gt;После вычисления аналитического градиента с помощью обратного распространения градиенты используются для обновления параметров. Существует несколько подходов к выполнению обновления, о которых мы поговорим далее.  &lt;/p&gt;
&lt;p&gt;Отметим, что оптимизация для глубоких сетей в настоящее время является очень активным направлением исследований. В этом разделе мы выделим некоторые устоявшиеся и распространенные техники, которые вы можете увидеть на практике, кратко опишем их интуицию, но оставим подробный разбор за рамками занятия. Мы даем несколько дополнительных указаний для заинтересованного читателя.  &lt;/p&gt;
&lt;h3&gt;Первый порядок (SGD), импульс, импульс Нестерова&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ванильное обновление&lt;/strong&gt;. Простейшей формой обновления является изменение параметров в направлении отрицательного градиента (поскольку градиент указывает направление увеличения, но обычно мы хотим минимизировать функцию потерь). Предполагая вектор параметров и градиент, простейшее обновление имеет вид:&lt;code&gt;x&lt;/code&gt; &lt;code&gt;dx&lt;/code&gt;  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; Vanilla update
x += - learning_rate * dx
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;где гиперпараметр - фиксированная константа. При оценке на полном наборе данных и при достаточно низкой скорости обучения это гарантированно приведет к неотрицательному прогрессу в функции потерь.&lt;code&gt;learning_rate&lt;/code&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Обновление импульса&lt;/strong&gt; (&lt;em&gt;Momentum update&lt;/em&gt;) — еще один подход, который почти всегда имеет более высокую скорость сходимости в глубоких сетях. Это обновление может быть мотивировано с физической точки зрения задачи оптимизации. В частности, потери можно интерпретировать как высоту холмистой местности (и, следовательно, также как потенциальную энергию, так как &lt;strong&gt;U=mgh&lt;/strong&gt;. И поэтому &lt;strong&gt;\( U \propto h \)&lt;/strong&gt; ). Инициализация параметров случайными числами эквивалентна установке частицы с нулевой начальной скоростью в каком-либо месте. В этом случае процесс оптимизации можно рассматривать как эквивалент процесса моделирования вектора параметров (т.е. частицы) как движущейся по ландшафту.  &lt;/p&gt;
&lt;p&gt;Поскольку сила, действующая на частицу, связана с градиентом потенциальной энергии (т.е. &lt;strong&gt;F=−∇U&lt;/strong&gt; ), &lt;strong&gt;сила&lt;/strong&gt;, ощущаемая частицей, в точности является (&lt;em&gt;отрицательным&lt;/em&gt;) &lt;strong&gt;градиентом&lt;/strong&gt; функции потерь. Сверх того &lt;strong&gt;F=ma&lt;/strong&gt;. Таким образом, (&lt;em&gt;отрицательный&lt;/em&gt;) градиент с этой точки зрения пропорционален ускорению частицы. Обратите внимание, что это отличается от показанного выше обновления &lt;em&gt;SGD&lt;/em&gt;, где градиент напрямую интегрирует положение. Вместо этого физический взгляд предлагает обновление, в котором градиент только напрямую влияет на скорость, что, в свою очередь, влияет на положение:  &lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;h2&gt;Momentum update&lt;/h2&gt;
&lt;p&gt;v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;Здесь&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;видим&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;введение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;переменной&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`v`&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;которая&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;инициализируется&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;нулем&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;и&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;дополнительный&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;гиперпараметр&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n n-Quoted"&gt;`mu`&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;К&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сожалению&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;эта&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;переменная&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;оптимизации&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;называется&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;_импульсом_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ее&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;типичное&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;значение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;составляет&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;около&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;но&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ее&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;физическое&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;значение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;больше&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;соответствует&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;коэффициенту&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;трения&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;По&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сути&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;эта&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;переменная&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;гасит&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;скорость&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;и&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;снижает&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;кинетическую&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;энергию&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;системы&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;иначе&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;частица&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;никогда&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;бы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;не&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;остановилась&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;у&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;подножия&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;холма&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;При&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;перекрестной&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;проверке&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;этому&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;параметру&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обычно&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;присваиваются&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;такие&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;значения&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;как&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Подобно&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;графикам&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;отжига&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;для&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;темпов&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обучения&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;обсуждается&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ниже&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;оптимизация&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;иногда&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;может&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;немного&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;выиграть&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;от&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;графиков&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульса&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;где&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульс&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;увеличивается&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;на&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;более&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;поздних&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;этапах&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обучения&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Типичная&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;настройка&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;заключается&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;том&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;чтобы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;начать&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;с&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульса&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;около&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;и&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;отжечь&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;его&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;до&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;или&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;около&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;того&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;течение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;нескольких&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;эпох&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n n-Quoted"&gt;`v`&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`mu`&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;При&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обновлении&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Momentum&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вектор&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;параметра&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;будет&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;наращивать&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;скорость&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;любом&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;направлении&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;которое&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;имеет&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;постоянный&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="n"&gt;__Импульс&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Нестерова__&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Nesterov&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Momentum&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;–&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;это&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;немного&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;другая&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;версия&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обновления&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Momentum&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;которое&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;последнее&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;время&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;набирает&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;популярность&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Он&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обладает&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;более&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сильными&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;теоретическими&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;гарантиями&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сходимости&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;для&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;выпуклых&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;функций&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;и&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;на&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;практике&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;также&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;стабильно&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;работает&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;немного&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;лучше&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;стандартного&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульса&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="n"&gt;Основная&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;идея&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;метода&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Нестерова&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;заключается&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;том&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;что&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;когда&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;текущий&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вектор&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;параметров&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;находится&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;некотором&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;положении&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`x`&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;то&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;глядя&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;на&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;приведённое&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;выше&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;обновление&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульса&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;знаем&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;что&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;только&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульс&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;то&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;есть&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;без&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;учёта&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;второго&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;слагаемого&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;с&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиентом&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;должен&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сдвинуть&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вектор&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;параметров&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;на&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`mu * v`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Поэтому&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;если&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;собираемся&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вычислить&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;можем&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;рассматривать&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;будущее&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;приблизительное&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;положение&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`x + mu * v`&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;как&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;«забежание&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вперёд»&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;—&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;это&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;точка&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;окрестности&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;того&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;места&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;где&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вскоре&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;окажемся&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Следовательно&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;имеет&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;смысл&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вычислять&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`x + mu * v`&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вместо&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;«старой&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;устаревшей»&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;позиции&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`x`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;
&lt;span class="n"&gt;___&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="err"&gt;[]&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="n"&gt;cs231n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;assets&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nn3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nesterov&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpeg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="n"&gt;Нестеровский&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульс&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Вместо&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;того&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;чтобы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;оценивать&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;текущем&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;положении&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;красный&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;круг&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;знаем&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;что&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;наш&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульс&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;вот&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;вот&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;приведет&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;нас&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;к&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;кончику&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;зеленой&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;стрелки&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Таким&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;образом&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;с&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;помощью&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;импульса&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Нестерова&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;оцениваем&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;градиент&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;этой&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;«просматриваемой»&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;позиции&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;___&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="n"&gt;То&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;есть&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;в&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;немного&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;неудобной&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;нотации&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;мы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;хотели&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;бы&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;сделать&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;следующее&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n n-Quoted"&gt;`&lt;/span&gt;&lt;span class="n n-Quoted n-Quoted-Escape"&gt;``&lt;/span&gt;
&lt;span class="n n-Quoted"&gt;  x_ahead = x + mu * v&lt;/span&gt;
&lt;span class="n n-Quoted"&gt;# evaluate dx_ahead (the gradient at x_ahead instead of at x)&lt;/span&gt;
&lt;span class="n n-Quoted"&gt;v = mu * v - learning_rate * dx_ahead&lt;/span&gt;
&lt;span class="n n-Quoted"&gt;x += v&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Однако на практике люди предпочитают выражать обновление так, чтобы оно было максимально похоже на ванильный &lt;em&gt;SGD&lt;/em&gt; или на предыдущее импульсное обновление. Этого можно достичь, манипулируя приведенным выше обновлением с помощью переменной &lt;em&gt;transform&lt;/em&gt; , а затем выражая обновление в терминах вместо . То есть, вектор параметров, который мы на самом деле сохраняем, всегда является опережающей версией. Уравнения в терминах (но переименовывая его обратно в ) становятся такими:&lt;code&gt;x_ahead = x + mu * v&lt;/code&gt; &lt;code&gt;x_ahead&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;x_ahead&lt;/code&gt; &lt;code&gt;x&lt;/code&gt;  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;v_prev = v # back this up
v = mu &lt;span class="gs"&gt;* v - learning_rate *&lt;/span&gt; dx # velocity update stays the same
x += -mu &lt;span class="gs"&gt;* v_prev + (1 + mu) *&lt;/span&gt; v # position update changes form
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Мы рекомендуем эту дополнительную литературу, чтобы понять источник этих уравнений и математическую формулировку ускоренного импульса Нестерова (&lt;em&gt;NAG&lt;/em&gt;):  &lt;/p&gt;
&lt;h3&gt;Отжиг темпов обучения&lt;/h3&gt;
&lt;p&gt;При обучении глубоких сетей обычно полезно отжигать скорость обучения с течением времени. Хорошая интуиция заключается в том, что при высокой скорости обучения система содержит слишком много кинетической энергии, и вектор параметров хаотично скачет, не имея возможности оседать в более глубоких, но более узких частях функции потерь. Понять, когда нужно снижать скорость обучения, может быть непросто: снижайте ее медленно, и вы будете тратить впустую вычисления, хаотично прыгая с небольшим улучшением в течение длительного времени. Но если загнить слишком агрессивно, система охладится слишком быстро, не сумев достичь наилучшего положения. Существует три распространенных типа реализации снижения скорости обучения:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Шаг затухания&lt;/strong&gt;: Уменьшайте скорость обучения в несколько раз каждые несколько эпох. Типичными значениями могут быть снижение скорости обучения вдвое каждые &lt;strong&gt;5&lt;/strong&gt; эпох или на &lt;strong&gt;0,1&lt;/strong&gt; каждые &lt;strong&gt;20&lt;/strong&gt; эпох. Эти цифры в значительной степени зависят от типа задачи и модели. Одна из эвристик, которую вы можете увидеть на практике, заключается в том, чтобы наблюдать за ошибкой валидации во время обучения с фиксированной скоростью обучения и уменьшать скорость обучения на константу (например, &lt;strong&gt;0,5&lt;/strong&gt;) всякий раз, когда ошибка валидации перестает улучшаться.   &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Экспоненциальный затухание&lt;/strong&gt;. имеет математическую форму &lt;strong&gt;\(\alpha = \alpha_0 e^{-k t}\)&lt;/strong&gt;, где &lt;strong&gt;\(\alpha_0, k\)&lt;/strong&gt; являются гиперпараметрами и &lt;strong&gt;t&lt;/strong&gt; — номер итерации (но можно использовать и единицы измерения эпох).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распад на 1/т&lt;/strong&gt; имеет математический вид &lt;strong&gt;\(\alpha = \alpha_0 / (1 + k t )\&lt;/strong&gt;), где &lt;strong&gt;\(a_0, k\)&lt;/strong&gt; являются гиперпараметрами и &lt;strong&gt;t&lt;/strong&gt; — номер итерации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На практике мы обнаруживаем, что шаг затухания немного предпочтительнее, потому что связанные с ним гиперпараметры (доля затухания и время шага в единицах эпох) более интерпретируемы, чем гиперпараметр &lt;strong&gt;k&lt;/strong&gt;. Наконец, если вы можете позволить себе вычислительный бюджет, ошибитесь в сторону более медленного распада и тренируйтесь в течение более длительного времени.   &lt;/p&gt;
&lt;h3&gt;Методы второго порядка&lt;/h3&gt;
&lt;p&gt;Вторая, популярная группа методов оптимизации в контексте глубокого обучения основана &lt;a href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization"&gt;на методе Ньютона&lt;/a&gt;, который повторяет следующее обновление:  &lt;/p&gt;
&lt;p&gt;$$
x \leftarrow x - [H f(x)]^{-1} \nabla f(x)
$$  &lt;/p&gt;
&lt;p&gt;Здесь &lt;strong&gt;Hf(x)&lt;/strong&gt; — &lt;a href="http://en.wikipedia.org/wiki/Hessian_matrix"&gt;матрица Гессена&lt;/a&gt;, представляющая собой квадратную матрицу частных производных функции второго порядка. Термин &lt;strong&gt;∇f(x)&lt;/strong&gt;— вектор градиента, как показано в &lt;strong&gt;Gradient Descent&lt;/strong&gt;. Интуитивно &lt;em&gt;гессенский метод&lt;/em&gt; описывает локальную кривизну функции потерь, что позволяет нам выполнить более эффективное обновление. В частности, умножение на обратное гессенское значение приводит к тому, что оптимизация делает более агрессивные шаги в направлениях малой кривизны и более короткие шаги в направлениях крутой кривизны. Обратите внимание, что особенно важно, на отсутствие каких-либо гиперпараметров скорости обучения в формуле обновления, что сторонники этих методов называют большим преимуществом по сравнению с методами первого порядка.    &lt;/p&gt;
&lt;p&gt;Тем не менее, приведенное выше обновление непрактично для большинства приложений глубокого обучения, потому что вычисление (и инвертирование) гессена в его явной форме является очень дорогостоящим процессом как в пространстве, так и во времени. Например, нейронная сеть с одним миллионом параметров будет иметь гессенову матрицу размером [1 000 000 x 1 000 000], занимающую примерно 3725 гигабайт оперативной памяти. Следовательно, было разработано большое разнообразие квазиньютоновских методов, которые стремятся аппроксимировать обратный гессенский метод. Среди них наиболее популярным является &lt;a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS"&gt;L-BFGS&lt;/a&gt;, который использует информацию в градиентах с течением времени для неявного формирования аппроксимации (т.е. полная матрица никогда не вычисляется).  &lt;/p&gt;
&lt;p&gt;Тем не менее, даже после того, как мы устраним проблемы с памятью, большим недостатком наивного применения L-BFGS является то, что его приходится вычислять по всему обучающему набору, который может содержать миллионы примеров. В отличие от мини-партий SGD, заставить L-BFGS работать с мини-партиями сложнее и является активной областью исследований.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;На практике&lt;/strong&gt; в настоящее время не часто можно увидеть, чтобы &lt;strong&gt;L-BFGS&lt;/strong&gt; или аналогичные методы второго порядка применялись к крупномасштабному глубокому обучению и сверточным нейронным сетям. Вместо этого варианты &lt;em&gt;SGD&lt;/em&gt;, основанные на импульсе (Нестерова), более стандартны, потому что они проще и легче масштабируются.    &lt;/p&gt;
&lt;h4&gt;Дополнительные материалы:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://research.google.com/archive/large_deep_networks_nips2012.html"&gt;Large Scale Distributed Deep Networks&lt;/a&gt; — это статья от команды &lt;em&gt;Google Brain&lt;/em&gt;, в которой сравниваются варианты &lt;em&gt;L-BFGS&lt;/em&gt; и &lt;em&gt;SGD&lt;/em&gt; в крупномасштабной распределенной оптимизации.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1311.2115"&gt;Алгоритм SFO&lt;/a&gt; стремится объединить преимущества &lt;em&gt;SGD&lt;/em&gt; с преимуществами &lt;em&gt;L-BFGS&lt;/em&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Методы адаптивной скорости обучения для каждого параметра&lt;/h3&gt;
&lt;p&gt;Все предыдущие подходы, которые мы обсуждали до сих пор, манипулировали скоростью обучения глобально и одинаково по всем параметрам. &lt;strong&gt;Настройка скорости обучения&lt;/strong&gt; — дорогостоящий процесс, поэтому много работы было потрачено на разработку методов, которые могут адаптивно настраивать скорость обучения и даже делать это для каждого параметра. Многие из этих методов могут по-прежнему требовать других настроек гиперпараметров, но аргумент заключается в том, что они хорошо работают для более широкого диапазона значений гиперпараметров, чем скорость необработанного обучения. В этом разделе мы выделим некоторые распространенные адаптивные методы, с которыми вы можете столкнуться на практике:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adagrad&lt;/strong&gt; — это метод адаптивной скорости обучения, первоначально предложенный &lt;a href="http://jmlr.org/papers/v12/duchi11a.html"&gt;Дучи и др.&lt;/a&gt;  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что переменная имеет размер, равный размеру градиента, и отслеживает сумму квадратов градиентов по каждому параметру. Затем это используется для нормализации шага обновления параметров по элементам. Обратите внимание, что для весов, получающих высокие градиенты, эффективная скорость обучения будет снижена, в то время как для весов, получающих небольшие или нечастые обновления, эффективная скорость обучения будет увеличена. Забавно, но операция извлечения квадратного корня оказывается очень важной, и без нее алгоритм работает гораздо хуже. Сглаживание (обычно задается в диапазоне от &lt;strong&gt;1e-4&lt;/strong&gt; до &lt;strong&gt;1e-8&lt;/strong&gt;) позволяет избежать деления на ноль. Недостатком &lt;strong&gt;Adagrad&lt;/strong&gt; является то, что в случае глубокого обучения монотонный темп обучения обычно оказывается слишком агрессивным и прекращает обучение слишком рано.&lt;code&gt;cache&lt;/code&gt; &lt;code&gt;eps&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RMSprop&lt;/strong&gt; — это очень эффективный, но в настоящее время неопубликованный метод адаптивной скорости обучения. Забавно, что все, кто использует этот метод в своей работе, в настоящее время цитируют &lt;a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"&gt;слайд 29 лекции 6&lt;/a&gt; курса Джеффа Хинтона на &lt;em&gt;Coursera&lt;/em&gt;. Обновление &lt;strong&gt;RMSProp&lt;/strong&gt; очень просто корректирует метод &lt;strong&gt;Adagrad&lt;/strong&gt; в попытке снизить его агрессивную, монотонно снижающуюся скорость обучения. В частности, вместо этого он использует скользящее среднее квадратов градиентов, дающее:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;cache = decay_rate &lt;span class="gs"&gt;* cache + (1 - decay_rate) *&lt;/span&gt; dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Здесь находится гиперпараметр &lt;code&gt;decay_rate&lt;/code&gt;, типичные значения которого равны &lt;strong&gt;[0.9, 0.99, 0.999]&lt;/strong&gt;. Обратите внимание, что обновление &lt;code&gt;x+=&lt;/code&gt; идентично &lt;strong&gt;Adagrad&lt;/strong&gt;, но переменная&lt;code&gt;cache&lt;/code&gt; "учетка". Следовательно, &lt;strong&gt;RMSProp&lt;/strong&gt; по-прежнему модулирует скорость обучения каждого веса на основе величин его градиентов, что имеет положительный уравнительный эффект, но в отличие от &lt;strong&gt;Adagrad&lt;/strong&gt; обновления не становятся монотонно меньше.    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Адам&lt;/strong&gt;. &lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; — это недавно предложенное обновление, которое немного похоже на &lt;strong&gt;RMSProp&lt;/strong&gt; с импульсом. (&lt;em&gt;Упрощённое&lt;/em&gt;) обновление выглядит следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что обновление выглядит точно так же, как обновление &lt;strong&gt;RMSProp&lt;/strong&gt;, за исключением того, что вместо необработанного (и, возможно, зашумленного) вектора градиента &lt;code&gt;dx&lt;/code&gt; используется “сглаженная” версия градиента &lt;code&gt;m&lt;/code&gt;. Рекомендуемые значения в документе - &lt;code&gt;eps = 1e-8&lt;/code&gt;, &lt;code&gt;beta1 = 0,9&lt;/code&gt;, &lt;code&gt;beta2 = 0,999&lt;/code&gt;. На практике Adam в настоящее время рекомендуется использовать в качестве алгоритма по умолчанию и часто работает немного лучше, чем &lt;strong&gt;RMSProp&lt;/strong&gt;. Однако часто также стоит попробовать &lt;em&gt;SGD+Nesterov Momentum&lt;/em&gt; в качестве альтернативы. Полное обновление Adam также включает механизм &lt;em&gt;коррекции смещения&lt;/em&gt;, который компенсирует тот факт, что на первых нескольких временных шагах векторы &lt;code&gt;m,v&lt;/code&gt; инициализируются и, следовательно, смещаются на ноль, прежде чем они полностью “разогреются”. С механизмом &lt;em&gt;коррекции смещения&lt;/em&gt; обновление выглядит следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что обновление теперь является функцией итерации, а также других параметров. Мы отсылаем читателя к статье для получения подробной информации или к слайдам курса, где это подробно рассматривается.  &lt;/p&gt;
&lt;h3&gt;Дополнительные ссылки:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Модульные тесты для стохастической оптимизации предлагают серию тестов в качестве стандартизированного эталона для стохастической оптимизации.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/opt2.gif"&gt;   &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/opt1.gif"&gt;   &lt;/p&gt;
&lt;p&gt;Анимация, которая может помочь вашей интуиции о динамике процесса обучения. &lt;strong&gt;Сверху&lt;/strong&gt;: Контуры поверхности потерь и временная эволюция различных алгоритмов оптимизации. Обратите внимание на «чрезмерное» поведение методов, основанных на импульсе, из-за чего оптимизация выглядит как мяч, катящийся с горки.&lt;br&gt;
&lt;strong&gt;Снизу&lt;/strong&gt;: Визуализация седловой точки в ландшафте оптимизации, где кривизна по разным размерностям имеет разные знаки (одно измерение искривляется вверх, а другое вниз). Обратите внимание, что &lt;em&gt;SGD&lt;/em&gt; с трудом нарушает симметрию и застревает на вершине. И наоборот, такие алгоритмы, как &lt;strong&gt;RMSprop&lt;/strong&gt;, будут видеть очень низкие градиенты в направлении седла. Из-за знаменателя в обновлении &lt;strong&gt;RMSprop&lt;/strong&gt; это увеличит эффективную скорость обучения в этом направлении, что поможет &lt;strong&gt;RMSProp&lt;/strong&gt; двигаться дальше. Изображения предоставлены: &lt;a href="https://twitter.com/alecrad"&gt;Алек Рэдфорд&lt;/a&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Оптимизация гиперпараметров&lt;/h2&gt;
&lt;p&gt;Как мы уже видели, обучение нейронных сетей может включать в себя множество настроек гиперпараметров. К наиболее распространенным гиперпараметрам в контексте нейронных сетей относятся:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;начальная скорость обучения&lt;/li&gt;
&lt;li&gt;График снижения скорости обучения (например, постоянная затухания)&lt;/li&gt;
&lt;li&gt;регуляризация силы (&lt;em&gt;штраф &lt;/em&gt;&lt;em&gt;\(L_2\)&lt;/em&gt;&lt;em&gt;, сила отсева&lt;/em&gt;)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но, как мы видели, относительно менее чувствительных гиперпараметров гораздо больше, например, в попараметрических адаптивных методах обучения, настройке импульса и его расписания и т.д. В этом разделе мы опишем некоторые дополнительные советы и рекомендации по выполнению поиска гиперпараметров:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Реализация&lt;/strong&gt;. Более крупные нейронные сети обычно требуют много времени для обучения, поэтому выполнение поиска гиперпараметров может занять много дней/недель. Важно помнить об этом, так как это влияет на дизайн вашей кодовой базы. Одна из особенностей проекта заключается в том, чтобы иметь &lt;strong&gt;воркер/работника&lt;/strong&gt;, который постоянно отбирает случайные гиперпараметры и выполняет оптимизацию. Во время обучения сотрудник будет отслеживать производительность проверки после каждой эпохи и записывать контрольную точку модели (вместе с различной статистикой обучения, такой как потери с течением времени) в файл, предпочтительно в общей файловой системе. Полезно указывать производительность проверки непосредственно в имени файла, чтобы было легко проверить и отсортировать ход выполнения. Затем есть вторая программа, которую мы будем называть &lt;strong&gt;мастером&lt;/strong&gt;, которая запускает или убивает рабочих по всему вычислительному кластеру, а также может дополнительно проверять контрольные точки, написанные рабочими, и строить статистику их обучения и т. д.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Отдайте предпочтение одной свертке проверки перекрестной проверке&lt;/strong&gt;. В большинстве случаев один валидационный набор приличного размера существенно упрощает кодовую базу без необходимости перекрестной валидации с несколькими свертками. Вы можете услышать, как люди говорят, что они «перекрестно проверили» параметр, но часто предполагается, что они все еще использовали только один набор проверки.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Диапазоны гиперпараметров&lt;/strong&gt;. Поиск гиперпараметров в логарифмической шкале. Например, типичная выборка коэффициента обучения будет выглядеть следующим образом: &lt;code&gt;learning_rate = 10 ** uniform(-6, 1)&lt;/code&gt;. То есть мы генерируем случайное число из равномерного распределения, но затем возводим его в степень &lt;strong&gt;10&lt;/strong&gt;. Та же стратегия должна быть использована и для силы регуляризации. Интуитивно это объясняется тем, что скорость обучения и сила регуляризации оказывают мультипликативное влияние на динамику тренировки. Например, фиксированное изменение при добавлении &lt;strong&gt;0,01&lt;/strong&gt; к коэффициенту обучения оказывает огромное влияние на динамику, если коэффициент обучения равен &lt;strong&gt;0,001&lt;/strong&gt;, но почти не оказывает никакого влияния, если коэффициент обучения равен &lt;strong&gt;10&lt;/strong&gt;. Это связано с тем, что скорость обучения умножает вычисленный градиент в обновлении. Следовательно, гораздо естественнее рассматривать диапазон скорости обучения, умноженный или деленный на некоторую величину, чем диапазон скорости обучения, прибавленный или вычтенный на некоторую величину. Некоторые параметры (например, отсеивание) обычно ищутся в исходной шкале (например, &lt;code&gt;dropout = uniform(0,1)&lt;/code&gt;).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Отдайте предпочтение случайному поиску, а не поиску по сетке&lt;/strong&gt;. Как утверждают Бергстра и Бенджио в &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;книге «Случайный поиск для оптимизации гиперпараметров»&lt;/a&gt;, &lt;em&gt;«случайно выбранные испытания более эффективны для оптимизации гиперпараметров, чем испытания на сетке»&lt;/em&gt; . Как оказалось, это тоже обычно проще реализовать.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn3/gridsearchbad.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Основная иллюстрация из &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;книги «Случайный поиск для оптимизации гиперпараметров»&lt;/a&gt; Бергстры и Бенджио. Очень часто бывает так, что некоторые гиперпараметры имеют гораздо большее значение, чем другие (например, верхний гиперпараметр против левого на этом рисунке). Выполнение случайного поиска, а не поиска по сетке, позволяет гораздо точнее находить хорошие значения для важных.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Осторожнее с лучшими значениями на границе&lt;/strong&gt;. Иногда может случиться так, что вы ищете гиперпараметр (например, скорость обучения) в плохом диапазоне. Например, предположим, что мы используем &lt;code&gt;learning_rate = 10 ** uniform(-6, 1)&lt;/code&gt; . Как только мы получим результаты, важно еще раз проверить, что итоговая скорость обучения не находится на краю этого интервала, иначе вы можете пропустить более оптимальную настройку гиперпараметров за пределами интервала.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Этапируйте свой поиск от грубого к хорошему&lt;/strong&gt;. На практике может быть полезно сначала искать в грубых диапазонах (например, &lt;strong&gt;10 ** [-6, 1]&lt;/strong&gt;), а затем, в зависимости от того, где появляются наилучшие результаты, сужать диапазон. Кроме того, может быть полезно выполнить первоначальный грубый поиск во время обучения только за &lt;strong&gt;1&lt;/strong&gt; эпоху или даже меньше, потому что многие настройки гиперпараметров могут привести к тому, что модель вообще не будет обучаться или сразу же взорвется с бесконечными затратами. Второй этап может выполнять более узкий поиск с &lt;strong&gt;5&lt;/strong&gt; эпохами, а последний этап может выполнять детальный поиск в конечном диапазоне для гораздо большего количества эпох (например).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Байесовская оптимизация гиперпараметров&lt;/strong&gt; — это целая область исследований, посвященная созданию алгоритмов, которые пытаются более эффективно ориентироваться в пространстве гиперпараметров. Основная идея заключается в том, чтобы правильно сбалансировать компромисс между исследованием и эксплуатацией при запросе производительности при различных гиперпараметрах. На основе этих моделей также было разработано несколько библиотек, среди наиболее известных — &lt;a href="https://github.com/JasperSnoek/spearmint"&gt;Spearmint&lt;/a&gt;, &lt;a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/"&gt;SMAC&lt;/a&gt; и &lt;a href="http://jaberg.github.io/hyperopt/"&gt;Hyperopt&lt;/a&gt;. Тем не менее, в практических условиях с &lt;em&gt;ConvNet&lt;/em&gt; все еще относительно сложно превзойти случайный поиск в тщательно выбранных интервалах. Смотрите дополнительную дискуссию из окопов &lt;a href="http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html"&gt;здесь&lt;/a&gt;.  &lt;/p&gt;
&lt;h2&gt;Оценка&lt;/h2&gt;
&lt;h3&gt;Модельные ансамбли&lt;/h3&gt;
&lt;p&gt;На практике одним из надежных подходов к повышению производительности нейронных сетей на несколько процентов является обучение нескольких независимых моделей и усреднение их прогнозов во время тестирования. По мере увеличения числа моделей в ансамбле производительность обычно монотонно улучшается (хотя и с уменьшением отдачи). Более того, улучшения более значительны с большим разнообразием моделей в ансамбле. Существует несколько подходов к формированию ансамбля:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Одна и та же модель, разные инициализации&lt;/strong&gt;. Используйте перекрестную проверку для определения наилучших гиперпараметров, а затем обучите несколько моделей с лучшим набором гиперпараметров, но с разной случайной инициализацией. Опасность при таком подходе заключается в том, что сорт получается только за счет инициализации.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Лучшие модели, обнаруженные во время перекрестной проверки&lt;/strong&gt;. Используйте перекрестную проверку для определения наилучших гиперпараметров, а затем выберите несколько лучших (например, &lt;strong&gt;10&lt;/strong&gt;) моделей для формирования ансамбля. Это повышает разнообразие ансамбля, но чревато опасностью включения неоптимальных моделей. На практике это может быть проще выполнить, так как не требует дополнительного переобучения моделей после перекрестной проверки&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Разные контрольные точки одной модели&lt;/strong&gt;. Если обучение стоит очень дорого, то некоторые люди имеют ограниченный успех в прохождении различных контрольных точек одной сети с течением времени (например, после каждой эпохи) и использовании их для формирования ансамбля. Очевидно, что это страдает от некоторого недостатка разнообразия, но все же может работать достаточно хорошо на практике. Преимущество такого подхода в том, что он очень дешевый.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Бегущее среднее по параметрам во время тренировки&lt;/strong&gt;. Что касается последнего пункта, то дешевый способ почти всегда получить дополнительный процент или два производительности — это хранить в памяти вторую копию весовых коэффициентов сети, которая поддерживает экспоненциально уменьшающуюся сумму предыдущих весов во время обучения. Таким образом, вы усредняете состояние сети за последние несколько итераций. Вы обнаружите, что эта «сглаженная» версия весов за последние несколько шагов почти всегда приводит к лучшей ошибке проверки. Грубая интуиция, которую следует иметь в виду, заключается в том, что цель имеет форму чаши, и ваша сеть прыгает вокруг режима, поэтому среднее значение имеет больше шансов оказаться где-то ближе к режиму.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Одним из недостатков ансамблей моделей является то, что их оценка на тестовом примере занимает больше времени. Заинтересованного читателя может вдохновить недавняя работа Джеффа Хинтона «&lt;a href="https://www.youtube.com/watch?v=EK61htlw8hY"&gt;Темное знание&lt;/a&gt;», в которой идея состоит в том, чтобы «дистиллировать» хороший ансамбль обратно к одной модели, включив логарифмические правдоподобия ансамбля в модифицированную цель.  &lt;/p&gt;
&lt;h2&gt;Краткая сводка&lt;/h2&gt;
&lt;p&gt;Чтобы обучить нейронную сеть:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Градиент&lt;/strong&gt;: проверьте свою реализацию с помощью небольшого пакета данных и помните о подводных камнях.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;В качестве проверки здравого смысла&lt;/strong&gt; убедитесь, что ваши первоначальные потери разумны, и что вы можете достичь 100% точности обучения на очень небольшой части данных&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Во время обучения отслеживайте потери&lt;/strong&gt;, точность обучения/проверки, а если вы чувствуете себя более склонным, величину обновлений по отношению к значениям параметров (она должна быть &lt;strong&gt;~1e-3&lt;/strong&gt;), а при работе с &lt;em&gt;ConvNet&lt;/em&gt; — веса первого слоя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Рекомендуется использовать два обновления&lt;/strong&gt;: &lt;em&gt;SGD+Nesterov Momentum или Adam&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Снижайте скорость обучения в течение периода обучения&lt;/strong&gt;. Например, уменьшите вдвое скорость обучения после фиксированного количества эпох или всякий раз, когда точность проверки достигает максимума.&lt;/li&gt;
&lt;li&gt;Поиск хороших гиперпараметров с помощью случайного поиска (не поиска по сетке). Дифференцируйте поиск от грубого (широкие диапазоны гиперпараметров, обучение только для &lt;strong&gt;1-5&lt;/strong&gt; эпох) до тонкого (более узкие рейнджеры, обучение для гораздо большего количества эпох)&lt;/li&gt;
&lt;li&gt;Формируйте модельные ансамбли для дополнительной производительности  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Дополнительные материалы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf"&gt;SGD&lt;/a&gt; советы и рекомендации от Леона Ботту&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"&gt;Efficient BackProp&lt;/a&gt; (pdf) от Яна Лекуна&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1206.5533v2.pdf"&gt;Практические рекомендации по градиентному обучению глубокого Архитектура&lt;/a&gt; от Йошуа Бенджио&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/convnets-3/</guid><pubDate>Mon, 10 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Предобработка, инициализация весов, функции потерь</title><link>https://mldl.ru/posts/convnets-2/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Предобработка, инициализация весов, функции потерь&lt;/h2&gt;
&lt;p&gt;Содержание:
- &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Настройка данных и модели&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Предварительная обработка данных&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Инициализация веса&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Нормализация партии&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Регуляризация&lt;/a&gt; (L2/L1/Maxnorm/Dropout)
- &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Функции потерь&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-2/"&gt;Краткая сводка&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;Настройка данных и модели&lt;/h2&gt;
&lt;p&gt;В предыдущем разделе мы представили модель нейрона, который вычисляет скалярное произведение с помощью нелинейности, и нейронные сети, которые объединяют нейроны в слои. Вместе эти решения определяют новую форму &lt;strong&gt;функции оценки&lt;/strong&gt;, которую мы расширили по сравнению с простым линейным отображением, которое мы рассматривали в разделе «Линейная классификация». В частности, нейронная сеть выполняет последовательность линейных отображений с вложенными нелинейностями. В этом разделе мы обсудим дополнительные решения, касающиеся предварительной обработки данных, инициализации весов и функций потерь.  &lt;/p&gt;
&lt;h3&gt;Предварительная обработка данных&lt;/h3&gt;
&lt;p&gt;Существует три распространённые формы предварительной обработки данных в матрице данных &lt;code&gt;X&lt;/code&gt;, где мы будем предполагать, что &lt;code&gt;X&lt;/code&gt; имеет размер &lt;code&gt;[N x D]&lt;/code&gt; (&lt;code&gt;N&lt;/code&gt; — количество данных, &lt;code&gt;D&lt;/code&gt; — их размерность).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Вычитание среднего значения&lt;/strong&gt; — наиболее распространённая форма предварительной обработки. Она заключается в вычитании среднего значения для каждого отдельного признака в данных и имеет геометрическую интерпретацию центрирования облака данных вокруг начала координат по каждому измерению. В &lt;strong&gt;numpy&lt;/strong&gt; эта операция будет реализована следующим образом: (&lt;code&gt;X -= np.mean(X, axis = 0)&lt;/code&gt;. В случае с изображениями для удобства можно вычесть одно значение из всех пикселей (например, &lt;code&gt;X -= np.mean(X)&lt;/code&gt;), либо сделать это отдельно для трёх цветовых каналов.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Нормализация&lt;/strong&gt; — это приведение размерностей данных к примерно одинаковому масштабу. Существует два распространённых способа достижения такой нормализации:
- Один из них — разделить каждую размерность на её стандартное отклонение после центрирования по нулю: (&lt;code&gt;X /= np.std(X, axis = 0)&lt;/code&gt;). 
- Другой способ предварительной обработки — нормализовать каждую размерность так, чтобы минимальное и максимальное значения по каждой размерности составляли -1 и 1 соответственно. Имеет смысл применять эту предварительную обработку только в том случае, если у вас есть основания полагать, что разные входные параметры имеют разные масштабы (или единицы измерения), но они должны быть примерно одинаково важны для алгоритма обучения. В случае изображений относительные масштабы пикселей уже примерно одинаковы (и находятся в диапазоне от 0 до 255), поэтому нет необходимости выполнять этот дополнительный этап предварительной обработки.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn2/prepro1.jpeg"&gt;&lt;br&gt;
Общий конвейер предварительной обработки данных. &lt;strong&gt;Слева&lt;/strong&gt;: исходные данные, 2-мерные входные данные. &lt;strong&gt;В центре&lt;/strong&gt;: данные центрируются по нулевому значению путём вычитания среднего значения в каждом измерении. Облако данных теперь центрировано относительно начала координат. &lt;strong&gt;Справа&lt;/strong&gt;: каждое измерение дополнительно масштабируется с помощью стандартного отклонения. Красные линии указывают на границы данных — в центре они разной длины, а справа — одинаковой.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Метод главных компонент и отбеливание&lt;/strong&gt; — это ещё одна форма предварительной обработки. В этом процессе данные сначала центрируются, как описано выше. Затем мы можем вычислить ковариационную матрицу, которая показывает корреляционную структуру данных:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# Assume input data matrix X of size [N x D]&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# zero-center the data (important)&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# get the data covariance matrix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Элемент (i,j) ковариационной матрицы данных содержит ковариацию между i-м и j-м измерениями данных. В частности, диагональ этой матрицы содержит дисперсии. Кроме того, ковариационная матрица является симметричной и &lt;a href="http://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices"&gt;положительно определённой&lt;/a&gt;. Мы можем вычислить разложение ковариационной матрицы данных по методу сингулярного разложения:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;U,S,V = np.linalg.svd(cov)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;где столбцы &lt;code&gt;U&lt;/code&gt; являются собственными векторами, а &lt;code&gt;S&lt;/code&gt; — одномерным массивом сингулярных значений. Чтобы устранить корреляцию в данных, мы проецируем исходные (но центрированные по нулю) данные на собственный базис:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;Xrot = np.dot(X, U) # decorrelate the data
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание, что столбцы &lt;code&gt;U&lt;/code&gt; представляют собой набор ортогональных векторов (норма которых равна 1 и которые ортогональны друг другу), поэтому их можно рассматривать как базисные векторы. Таким образом, проекция соответствует повороту данных в &lt;code&gt;X&lt;/code&gt; таким образом, чтобы новые оси были собственными векторами. Если бы мы вычислили ковариационную матрицу &lt;code&gt;Xrot&lt;/code&gt;, то увидели бы, что теперь она диагональная. Преимущество &lt;code&gt;np.linalg.svd&lt;/code&gt; в том, что в возвращаемом значении &lt;code&gt;U&lt;/code&gt; столбцы собственных векторов отсортированы по собственным значениям. Мы можем использовать это для уменьшения размерности данных, используя только несколько главных собственных векторов и отбрасывая измерения, в которых данные не имеют дисперсии. Это также иногда называют &lt;a href="http://en.wikipedia.org/wiki/Principal_component_analysis"&gt;анализом главных компонент (PCA)&lt;/a&gt; для уменьшения размерности:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;После этой операции мы уменьшили исходный набор данных размером [N x D] до размера [N x 100], сохранив 100 измерений данных, которые содержат наибольшую дисперсию. Очень часто можно добиться очень хорошей производительности, обучая линейные классификаторы или нейронные сети на наборах данных, уменьшенных с помощью метода главных компонент, что позволяет сэкономить место и время.  &lt;/p&gt;
&lt;p&gt;Последнее преобразование, которое вы можете увидеть на практике, — это &lt;strong&gt;отбеливание&lt;/strong&gt;. Операция отбеливания преобразует данные в собственный базис и делит каждое измерение на собственное значение, чтобы нормализовать масштаб. Геометрическая интерпретация этого преобразования заключается в том, что если исходные данные представляют собой многомерную гауссову функцию, то отбелённые данные будут представлять собой гауссову функцию с нулевым средним значением и единичной ковариационной матрицей. Этот шаг будет выглядеть следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)  
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Предупреждение: усиливается шум&lt;/em&gt;. Обратите внимание, что мы добавляем &lt;strong&gt;1e-5&lt;/strong&gt; (или небольшую константу), чтобы предотвратить деление на ноль. Одним из недостатков этого преобразования является то, что оно может сильно усиливать шум в данных, поскольку растягивает все измерения (включая несущественные измерения с небольшой дисперсией, которые в основном являются шумом) до одинакового размера на входе. На практике это можно смягчить более сильным сглаживанием (т. е. увеличив &lt;strong&gt;1e-5&lt;/strong&gt; до большего числа).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn2/prepro2.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;PCA / Отбеливание. &lt;strong&gt;Слева&lt;/strong&gt;: оригинальная игрушка, 2-мерные входные данные. &lt;strong&gt;Посередине&lt;/strong&gt;: после выполнения PCA. Данные центрируются на нуле, а затем поворачиваются в собственный базис ковариационной матрицы данных. Это декоррелирует данные (ковариационная матрица становится диагональной). &lt;strong&gt;Справа&lt;/strong&gt;: каждое измерение дополнительно масштабируется по собственным значениям, преобразуя матрицу ковариации данных в единичную матрицу. Геометрически это соответствует растяжению и сжатию данных в изотропный гауссовский большой объект.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Мы также можем попытаться визуализировать эти преобразования с помощью изображений CIFAR-10. Обучающий набор CIFAR-10 имеет размер 50 000 x 3072, где каждое изображение растягивается в вектор-строку размером 3072. Затем мы можем вычислить ковариационную матрицу [3072 x 3072] и вычислить её разложение по методу сингулярного значения (что может быть относительно затратным). Как выглядят вычисленные собственные векторы визуально? Возможно, вам поможет изображение:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn2/cifar10pca.jpeg"&gt;&lt;br&gt;
&lt;strong&gt;Слева&lt;/strong&gt;: пример набора из 49 изображений. &lt;strong&gt;2-е слева&lt;/strong&gt;: 144 главных собственных вектора из 3072. Главные собственные векторы объясняют большую часть дисперсии данных, и мы видим, что они соответствуют более низким частотам на изображениях. &lt;strong&gt;2-е справа&lt;/strong&gt;: 49 изображений, уменьшенных с помощью метода главных компонент с использованием 144 показанных здесь собственных векторов. То есть вместо того, чтобы представлять каждое изображение в виде 3072-мерного вектора, где каждый элемент — это яркость конкретного пикселя в определённом месте и на определённом канале, каждое изображение выше представлено только 144-мерным вектором, где каждый элемент показывает, какая часть каждого собственного вектора составляет изображение. Чтобы увидеть, какая информация об изображении содержится в этих 144 числах, мы должны вернуться к «пиксельному» базису из 3072 чисел. Поскольку &lt;strong&gt;U&lt;/strong&gt; — это поворот, этого можно добиться, умножив на &lt;code&gt;U.transpose()[:144,:]&lt;/code&gt;, а затем, визуализировав полученные 3072 числа в виде изображения, вы можете заметить, что изображения немного размыты, что отражает тот факт, что верхние собственные векторы захватывают более низкие частоты. Однако большая часть информации всё равно сохраняется. &lt;strong&gt;Справа&lt;/strong&gt;: визуализация «белого» представления, в котором дисперсия по каждому из 144 измерений сжата до одинаковой длины. Здесь 144 «белых» числа возвращаются к пикселям изображения путём умножения на &lt;code&gt;U.transpose()[:144,:]&lt;/code&gt;. Более низкие частоты (на которые изначально приходилось больше всего дисперсии) теперь незначительны, а более высокие частоты (на которые изначально приходилось относительно мало дисперсии) становятся более выраженными.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;На практике&lt;/strong&gt;. Мы упоминаем метод главных компонент/отбеливание в этих заметках для полноты картины, но эти преобразования не используются в свёрточных сетях. Однако очень важно центрировать данные по нулевому значению, и часто также выполняется нормализация каждого пикселя.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Распространённая ошибка&lt;/strong&gt;. Важно отметить, что любая статистика предварительной обработки (например, среднее значение данных) должна рассчитываться только на обучающих данных, а затем применяться к проверочным/тестовым данным. Например, вычисление среднего значения и вычитание его из каждого изображения во всём наборе данных, а затем разделение данных на обучающие/проверочные/тестовые, было бы ошибкой. Вместо этого среднее значение должно рассчитываться только на обучающих данных, а затем вычитаться из всех разделов (обучающих/проверочных/тестовых).  &lt;/p&gt;
&lt;h3&gt;Инициализация веса&lt;/h3&gt;
&lt;p&gt;Мы рассмотрели, как построить архитектуру нейронной сети и как предварительно обработать данные. Прежде чем приступить к обучению сети, необходимо инициализировать её параметры.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ловушка: инициализация всех весов нулями&lt;/strong&gt;. Давайте начнём с того, чего делать не следует. Обратите внимание, что мы не знаем, каким должно быть конечное значение каждого веса в обученной сети, но при правильной нормализации данных разумно предположить, что примерно половина весов будет положительной, а половина — отрицательной. Тогда разумной идеей может быть установка всех начальных весов в нулевое значение, что, как мы ожидаем, будет «наилучшим предположением». Это оказалось ошибкой, потому что если каждый нейрон в сети вычисляет один и тот же результат, значит все они также будут вычислять одни и те же градиенты во время обратного распространения ошибки и подвергаться одним и тем же обновлениям параметров. Другими словами, если веса нейронов инициализированы одинаково, то между ними не будет асимметрии.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Небольшие случайные числа&lt;/strong&gt;. Поэтому мы по-прежнему хотим, чтобы весовые коэффициенты были очень близки к нулю, но, как мы уже говорили выше, не равнялись нулю. В качестве решения принято инициализировать весовые коэффициенты нейронов небольшими числами и называть это &lt;em&gt;нарушением симметрии&lt;/em&gt;. Идея заключается в том, что изначально все нейроны случайны и уникальны, поэтому они будут вычислять разные обновления и интегрироваться в сеть как её различные части. Реализация для одной весовой матрицы может выглядеть так: &lt;code&gt;W = 0.01* np.random.randn(D,H)&lt;/code&gt;, где &lt;code&gt;randn&lt;/code&gt; — выборки из гауссианы с нулевым средним и единичным стандартным отклонением. При такой формулировке вектор весов каждого нейрона инициализируется как случайный вектор, выбранный из многомерной гауссианы, поэтому нейроны ориентированы в случайном направлении во входном пространстве. Также можно использовать небольшие числа, выбранные из равномерного распределения, но на практике это, по-видимому, относительно мало влияет на конечную производительность.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Предупреждение&lt;/em&gt;: не обязательно, что меньшие числа будут работать лучше. Например, слой нейронной сети с очень маленькими весами во время обратного распространения ошибки будет вычислять очень маленькие градиенты для своих данных (поскольку этот градиент пропорционален значению весов). Это может значительно уменьшить «сигнал градиента», проходящий через сеть в обратном направлении, и стать проблемой для глубоких сетей.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Калибровка дисперсии с помощью 1/sqrt(n)&lt;/strong&gt;. Одна из проблем, связанных с вышеописанным предложением, заключается в том, что дисперсия выходных данных случайно инициализированного нейрона растёт с увеличением количества входных данных. Оказывается, мы можем нормализовать дисперсию выходных данных каждого нейрона до 1, масштабируя его вектор весов на квадратный корень из &lt;em&gt;количества входов&lt;/em&gt; (т. е. количества входных данных). То есть рекомендуемая эвристика заключается в инициализации вектора весов каждого нейрона следующим образом: &lt;code&gt;w = np.random.randn(n) / sqrt(n)&lt;/code&gt;, где &lt;code&gt;n&lt;/code&gt; — количество входных данных. Это гарантирует, что все нейроны в сети изначально имеют примерно одинаковое распределение выходных данных, и эмпирически улучшает скорость сходимости.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Схема вывода выглядит следующим образом&lt;/strong&gt;:Рассмотрим внутренний продукт &lt;strong&gt;\(s = \sum_i^n w_i x_i\)&lt;/strong&gt; между весами &lt;strong&gt;w&lt;/strong&gt; и входные данные &lt;strong&gt;x&lt;/strong&gt;, что даёт исходную активацию нейрона до нелинейности. Мы можем изучить дисперсию &lt;strong&gt;s&lt;/strong&gt;:  &lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\text{Var}(s) &amp;amp;= \text{Var}(\sum_i^n w_ix_i) \\
&amp;amp;= \sum_i^n \text{Var}(w_ix_i) \\
&amp;amp;= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + [E(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\
&amp;amp;= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\
&amp;amp;= \left( n \text{Var}(w) \right) \text{Var}(x)
\end{align}
$$  &lt;/p&gt;
&lt;p&gt;где на первых двух этапах мы использовали &lt;a href="http://en.wikipedia.org/wiki/Variance"&gt; свойства дисперсии&lt;/a&gt; . На третьем этапе мы предположили, что входные данные и веса имеют нулевое среднее значение, поэтому &lt;strong&gt;\(E[x_i] = E[w_i] = 0\)&lt;/strong&gt;. Обратите внимание, что в общем случае это не так: например, блоки ReLU будут иметь положительное среднее значение. На последнем этапе мы предположили, что все &lt;strong&gt;\(w_i, x_i\)&lt;/strong&gt; являются одинаково распределёнными. Из этого вывода следует, что если мы хотим &lt;strong&gt;s&lt;/strong&gt; иметь ту же дисперсию, что и все его входные данные &lt;strong&gt;x&lt;/strong&gt;, тогда во время инициализации мы должны убедиться, что дисперсия каждого веса &lt;strong&gt;w&lt;/strong&gt; является &lt;strong&gt;1/n&lt;/strong&gt;. Следовательно при учете &lt;strong&gt;\(\text{Var}(aX) = a^2\text{Var}(X)\)&lt;/strong&gt;) для случайной величины &lt;strong&gt;X&lt;/strong&gt; и скаляра &lt;strong&gt;a&lt;/strong&gt; говорит о необходимости взять единичный гауссовское распределение, а затем масштабировать его &lt;strong&gt;\(a = \sqrt{1/n}\)&lt;/strong&gt;, чтобы внести свой вклад в его дисперсию &lt;strong&gt;1/n&lt;/strong&gt;. Это дает инициализацию &lt;code&gt;w = np.random.randn(n) / sqrt(n)&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;Аналогичный анализ проводится в статье &lt;a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"&gt;«Понимание сложности обучения глубоких нейронных сетей прямого распространения»&lt;/a&gt; Глоро и др. В этой статье авторы в итоге рекомендуют инициализацию в виде &lt;strong&gt;Var(w)=2/(nin+nout)&lt;/strong&gt;, где &lt;strong&gt;\(n_in\), \(n_out)\&lt;/strong&gt; - это количество единиц в предыдущем слое и в следующем слое. Это основано на компромиссе и эквивалентном анализе градиентов обратного распространения. В более поздней статье на эту тему &lt;a href="http://arxiv-web3.library.cornell.edu/abs/1502.01852"&gt;«Глубокое изучение выпрямителей: превосходные результаты на уровне человека при классификации ImageNet»&lt;/a&gt; Хе и др. выводится инициализация специально для нейронов ReLU, и делается вывод, что дисперсия нейронов в сети должна быть &lt;strong&gt;2.0/n&lt;/strong&gt;. Это даёт инициализацию &lt;code&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/code&gt; и является текущей рекомендацией для использования на практике в конкретном случае нейронных сетей с нейронами ReLU.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Разреженная инициализация&lt;/strong&gt;. Другой способ решить проблему некалиброванных дисперсий — установить все весовые матрицы в нулевое значение, но для нарушения симметрии каждый нейрон случайным образом соединяется (с весами, выбранными из небольшого гауссовского распределения, как описано выше) с фиксированным количеством нейронов под ним. Типичное количество нейронов, с которыми можно соединиться, может составлять всего 10.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Инициализация смещений&lt;/strong&gt;. Можно и часто бывает нужно инициализировать смещения равными нулю, поскольку асимметрия устраняется с помощью небольших случайных чисел в весовых коэффициентах. Для нелинейностей ReLU некоторые предпочитают использовать небольшое постоянное значение, например &lt;strong&gt;0,01&lt;/strong&gt;, для всех смещений, потому что это гарантирует, что все блоки ReLU срабатывают в начале и, следовательно, получают и передают некоторый градиент. Однако неясно, обеспечивает ли это стабильное улучшение (на самом деле некоторые результаты указывают на то, что это ухудшает производительность), и чаще всего используется просто инициализация с нулевым смещением.  &lt;/p&gt;
&lt;p&gt;На практике в настоящее время рекомендуется использовать блоки ReLU и &lt;code&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/code&gt; в соответствии с &lt;a href="http://arxiv-web3.library.cornell.edu/abs/1502.01852"&gt;Хе и др.&lt;/a&gt;.  &lt;/p&gt;
&lt;h3&gt;Нормализация партии&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Нормализация с помощью пакетов&lt;/strong&gt;. Недавно разработанная Иоффе и Сегеди техника под названием &lt;a href="http://arxiv.org/abs/1502.03167"&gt;«Нормализация с помощью пакетов»&lt;/a&gt; избавляет от многих проблем, связанных с правильной инициализацией нейронных сетей, поскольку в начале обучения активации во всей сети принудительно распределяются по единичному гауссовскому распределению. Основное наблюдение заключается в том, что это возможно, потому что нормализация — это простая дифференцируемая операция. При реализации этой техники обычно вставляется слой &lt;strong&gt;BatchNorm&lt;/strong&gt; сразу после полносвязных слоёв (или свёрточных слоёв, как мы вскоре увидим) и перед нелинейностями. Мы не будем подробно останавливаться на этом методе, поскольку он хорошо описан в статье по ссылке, но отметим, что использование пакетной нормализации в нейронных сетях стало очень распространённой практикой. На практике сети, использующие пакетную нормализацию, значительно более устойчивы к неправильной инициализации. Кроме того, пакетную нормализацию можно интерпретировать как предварительную обработку на каждом слое сети, но интегрированную в саму сеть дифференцируемым образом. Здорово!  &lt;/p&gt;
&lt;h3&gt;Регуляризация&lt;/h3&gt;
&lt;p&gt;Существует несколько способов контроля возможностей нейронных сетей для предотвращения переобучения:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Регуляризация L2&lt;/strong&gt; — это, пожалуй, самая распространённая форма регуляризации. Её можно реализовать, штрафуя за квадраты значений всех параметров непосредственно в целевой функции. То есть для каждого веса &lt;strong&gt;w&lt;/strong&gt; в сети мы добавляем термин &lt;strong&gt;\(\frac{1}{2} \lambda w^2\)&lt;/strong&gt; к цели, где &lt;strong&gt;λ&lt;/strong&gt; является силой регуляризации. Обычно наблюдается фактор &lt;strong&gt;\(\frac{1}{2}\)&lt;/strong&gt; впереди, потому что тогда градиент этого члена по отношению к параметру &lt;strong&gt;w&lt;/strong&gt; это просто &lt;strong&gt;λw&lt;/strong&gt; вместо &lt;strong&gt;2λw&lt;/strong&gt;.Регуляризация &lt;strong&gt;\(L_2\)&lt;/strong&gt; интуитивно понятна: она сильно штрафует векторы с пиковыми значениями и отдаёт предпочтение векторам с размытыми значениями. Как мы обсуждали в разделе «Линейная классификация», из-за мультипликативных взаимодействий между весами и входными данными это позволяет сети использовать все входные данные понемногу, а не некоторые из них — по максимуму. Наконец, обратите внимание, что при обновлении параметров методом градиентного спуска использование регуляризации &lt;strong&gt;\(L_2\)&lt;/strong&gt; в конечном итоге означает, что каждый вес уменьшается линейно: &lt;code&gt;W += -lambda * W&lt;/code&gt; по направлению к нулю.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Регуляризация L1&lt;/strong&gt; — ещё одна относительно распространённая форма регуляризации, при которой для каждого веса &lt;strong&gt;w&lt;/strong&gt; мы добавляем термин &lt;strong&gt;λ∣w∣&lt;/strong&gt; к цели. Можно комбинировать регуляризацию &lt;strong&gt;\(L_1\)&lt;/strong&gt; с регуляризацией &lt;strong&gt;\(L_2\)&lt;/strong&gt;: &lt;strong&gt;\(\lambda_1 \mid w \mid + \lambda_2 w^2\)&lt;/strong&gt; (это называется &lt;a href="http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&amp;amp;%20Hastie.pdf"&gt;регуляризацией эластичной сети&lt;/a&gt;). Регуляризация &lt;strong&gt;\(L_1\)&lt;/strong&gt; обладает интригующим свойством: она приводит к тому, что весовые векторы во время оптимизации становятся разреженными (то есть очень близкими к нулю). Другими словами, нейроны с регуляризацией &lt;strong&gt;\(L_1\)&lt;/strong&gt; в конечном итоге используют только разреженное подмножество наиболее важных входных данных и становятся почти невосприимчивыми к «зашумлённым» входным данным. Для сравнения, конечные весовые векторы при регуляризации &lt;strong&gt;\(L_2\)&lt;/strong&gt; обычно представляют собой размытые, небольшие числа. На практике, если вас не интересует явный выбор признаков, можно ожидать, что регуляризация &lt;strong&gt;\(L_2\)&lt;/strong&gt; будет работать лучше, чем регуляризация &lt;strong&gt;\(L_1\)&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ограничения по максимальной норме&lt;/strong&gt;. Другой формой регуляризации является установление абсолютной верхней границы для величины вектора весов каждого нейрона и использование проецируемого градиентного спуска для обеспечения соблюдения ограничения. На практике это соответствует обычному обновлению параметров, а затем обеспечению соблюдения ограничения путём ограничения вектора весов &lt;strong&gt;\(\vec{w}\)&lt;/strong&gt; каждого нейрона, чтобы удовлетворить &lt;strong&gt;\(\Vert \vec{w} \Vert_2 &amp;lt; c\)&lt;/strong&gt;. Типичные значения &lt;strong&gt;c&lt;/strong&gt;. Они составляют порядка 3 или 4. Некоторые пользователи сообщают об улучшениях при использовании этой формы регуляризации. Одно из её привлекательных свойств заключается в том, что сеть не может «взрывообразно» расти, даже если скорость обучения установлена слишком высокой, потому что обновления всегда ограничены.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Выпадение&lt;/strong&gt; — чрезвычайно эффективный, простой и недавно представленный метод регуляризации, описанный Шриваставой и др. в &lt;a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"&gt;«Выпадении: простом способе предотвращения переобучения нейронных сетей»&lt;/a&gt; &lt;em&gt;(pdf)&lt;/em&gt;, который дополняет другие методы (&lt;strong&gt;\(L_1\)&lt;/strong&gt;, &lt;strong&gt;\(L_2\)&lt;/strong&gt;, &lt;em&gt;maxnorm&lt;/em&gt;). Во время обучения выпадение реализуется путём активации нейрона только с некоторой вероятностью &lt;strong&gt;p&lt;/strong&gt; (гиперпараметр), или в противном случае установив его равным нулю.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn2/dropout.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;Рисунок, взятый из &lt;a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"&gt;статьи о выпадении&lt;/a&gt;, иллюстрирует эту идею. Во время обучения выпадение можно интерпретировать как выборку нейронной сети из полной нейронной сети и обновление параметров выбранной сети только на основе входных данных. (Однако экспоненциальное количество возможных выбранных сетей не является независимым, поскольку они имеют общие параметры.) Во время тестирования выпадение не применяется, а интерпретируется как оценка усреднённого прогноза по экспоненциально большому ансамблю всех подсетей (подробнее об ансамблях в следующем разделе).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Выпадение в примере трёхслойной нейронной сети будет реализовано следующим образом:  &lt;/p&gt;
&lt;p&gt;```
 """ Vanilla Dropout: Not recommended implementation (see notes below) """&lt;/p&gt;
&lt;p&gt;p = 0.5 # probability of keeping a unit active. higher = less dropout&lt;/p&gt;
&lt;p&gt;def train_step(X):
  """ X contains the data """&lt;/p&gt;
&lt;p&gt;# forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(&lt;em&gt;H1.shape) &amp;lt; p # first dropout mask
  H1 &lt;/em&gt;= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(&lt;em&gt;H2.shape) &amp;lt; p # second dropout mask
  H2 &lt;/em&gt;= U2 # drop!
  out = np.dot(W3, H2) + b3&lt;/p&gt;
&lt;p&gt;# backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)&lt;/p&gt;
&lt;p&gt;def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
  ```  &lt;/p&gt;
&lt;p&gt;В приведённом выше коде внутри функции &lt;code&gt;train_step&lt;/code&gt; мы дважды применили отсев: на первом скрытом слое и на втором скрытом слое. Отсев также можно применить непосредственно на входном слое, в этом случае мы также создадим бинарную маску для входных данных &lt;code&gt;X&lt;/code&gt;. Обратный проход остаётся неизменным, но, конечно, должен учитывать созданные маски &lt;code&gt;U1,U2&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;Важно отметить, что в функции &lt;code&gt;predict&lt;/code&gt; мы больше не отбрасываем значения, а выполняем масштабирование выходных значений скрытого слоя с помощью &lt;strong&gt;p&lt;/strong&gt;.Это важно, потому что во время тестирования все нейроны видят все свои входные данные, поэтому мы хотим, чтобы выходные данные нейронов во время тестирования были идентичны ожидаемым выходным данным во время обучения. Например, в случае &lt;strong&gt;p=0.5&lt;/strong&gt;. Нейроны должны вдвое уменьшить свои выходные данные во время тестирования, чтобы получить те же выходные данные, что и во время обучения (в среднем). Чтобы понять это, рассмотрим выходные данные нейрона &lt;strong&gt;x&lt;/strong&gt; (до отбрасывания). При отбрасывании ожидаемый результат от этого нейрона станет &lt;strong&gt;px+(1−p)0&lt;/strong&gt;, потому что выходной сигнал нейрона с вероятностью будет равен нулю &lt;strong&gt;1−p&lt;/strong&gt;. Во время тестирования, когда мы поддерживаем постоянную активность нейрона, мы должны корректировать &lt;strong&gt;x→px&lt;/strong&gt;, чтобы сохранить ожидаемый результат. Также можно показать, что выполнение этого ослабления во время тестирования может быть связано с процессом перебора всех возможных бинарных масок (и, следовательно, всех экспоненциально большого количества подсетей) и вычисления их совокупного прогноза.  &lt;/p&gt;
&lt;p&gt;Нежелательным свойством представленной выше схемы является то, что мы должны масштабировать активации по &lt;strong&gt;p&lt;/strong&gt; во время тестирования. Поскольку производительность во время тестирования очень важна, всегда предпочтительнее использовать &lt;strong&gt;инвертированное отбрасывание&lt;/strong&gt;, при котором масштабирование выполняется во время обучения, а прямой проход во время тестирования остаётся нетронутым. Кроме того, это удобно тем, что код прогнозирования может оставаться нетронутым, если вы решите изменить место применения отбрасывания или отказаться от него. Инвертированное отбрасывание выглядит следующим образом:  &lt;/p&gt;
&lt;p&gt;```
 """ 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""&lt;/p&gt;
&lt;p&gt;p = 0.5 # probability of keeping a unit active. higher = less dropout&lt;/p&gt;
&lt;p&gt;def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(&lt;em&gt;H1.shape) &amp;lt; p) / p # first dropout mask. Notice /p!
  H1 &lt;/em&gt;= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(&lt;em&gt;H2.shape) &amp;lt; p) / p # second dropout mask. Notice /p!
  H2 &lt;/em&gt;= U2 # drop!
  out = np.dot(W3, H2) + b3&lt;/p&gt;
&lt;p&gt;# backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)&lt;/p&gt;
&lt;p&gt;def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
  ``` &lt;/p&gt;
&lt;p&gt;После первого появления метода отсева было проведено множество исследований, направленных на то, чтобы понять, в чём заключается его эффективность на практике и как он соотносится с другими методами регуляризации. Рекомендуем ознакомиться с дополнительной литературой для заинтересованных читателей:&lt;br&gt;
- &lt;a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"&gt;Статья о выпадении из курса&lt;/a&gt; Шриваставы и др., 2014.
- &lt;a href="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf"&gt;Выборочное обучение как адаптивная регуляризация&lt;/a&gt;: «мы показываем, что регуляризация с помощью вычеркивания эквивалентна регуляризации &lt;strong&gt;\(L_2\)&lt;/strong&gt; первого порядка, применяемой после масштабирования признаков с помощью оценки обратной диагональной информационной матрицы Фишера».  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Тема шума при прямом проходе&lt;/strong&gt;. Выпадение относится к более общей категории методов, которые вводят стохастическое поведение при прямом проходе сети. Во время тестирования шум усредняется &lt;em&gt;аналитически&lt;/em&gt; (как в случае с выпадением при умножении на &lt;strong&gt;p&lt;/strong&gt;) или &lt;em&gt;численно&lt;/em&gt; (например, с помощью выборки, выполняя несколько прямых проходов с разными случайными решениями, а затем усредняя их). Примером других исследований в этом направлении является &lt;a href="http://cs.nyu.edu/~wanli/dropc/"&gt;DropConnect&lt;/a&gt;, где во время прямого прохода случайный набор весовых коэффициентов устанавливается в ноль. В качестве предвосхищения отметим, что свёрточные нейронные сети также используют эту тему с помощью таких методов, как стохастическое объединение, дробное объединение и увеличение данных. Мы подробно рассмотрим эти методы позже.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Регуляризация смещения&lt;/strong&gt;. Как мы уже упоминали в разделе о линейной классификации, обычно не рекомендуется регуляризировать параметры смещения, поскольку они не взаимодействуют с данными посредством мультипликативных взаимодействий и, следовательно, не влияют на конечную цель. Однако в практических приложениях (при надлежащей предварительной обработке данных) регуляризация смещения редко приводит к значительному ухудшению производительности. Вероятно, это связано с тем, что по сравнению со всеми весовыми параметрами коэффициентов смещения очень мало, поэтому классификатор может «позволить себе» использовать коэффициенты смещения, если они нужны ему для уменьшения потерь данных.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Послойная регуляризация&lt;/strong&gt;. Не очень распространена регуляризация разных слоёв с разной степенью (за исключением, возможно, выходного слоя). В литературе опубликовано относительно мало результатов, связанных с этой идеей.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;На практике&lt;/strong&gt;: чаще всего используется единая глобальная сила регуляризации &lt;strong&gt;\(L_2\)&lt;/strong&gt;, которая проходит перекрестную проверку. Также часто применяется комбинация с отбрасыванием данных после всех слоев. Значение &lt;strong&gt;p=0.5&lt;/strong&gt; это разумное значение по умолчанию, но его можно настроить на основе данных проверки.  &lt;/p&gt;
&lt;h2&gt;Функции потерь&lt;/h2&gt;
&lt;p&gt;Мы обсудили часть целевой функции, отвечающую за регуляризацию, которую можно рассматривать как штраф за определённую меру сложности модели. Вторая часть целевой функции — это &lt;em&gt;потеря данных&lt;/em&gt;, которая в задаче обучения с учителем измеряет соответствие между прогнозом (например, оценками классов при классификации) и истинным значением. Потеря данных представляет собой среднее значение потерь данных для каждого отдельного примера. То есть, &lt;strong&gt;\(L = \frac{1}{N} \sum_i L_i\) where \(N\)&lt;/strong&gt;, где &lt;strong&gt;N&lt;/strong&gt; - количество обучающих данных. Давайте сократим &lt;strong&gt;\(f = f(x_i; W)\)&lt;/strong&gt; активация выходного слоя в нейронной сети. Существует несколько типов задач, которые вы можете решить на практике:
- &lt;strong&gt;Классификация&lt;/strong&gt; — это случай, который мы подробно обсуждали. Здесь мы предполагаем наличие набора примеров и одной правильной метки (из фиксированного набора) для каждого примера. Одной из двух наиболее часто встречающихся функций стоимости в этой задаче является SVM (&lt;em&gt;например, формулировка Уэстона Уоткинса&lt;/em&gt;):&lt;br&gt;
$$&lt;br&gt;
L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)
$$&lt;br&gt;
Как мы вкратце упомянули, некоторые люди сообщают о более высокой производительности при использовании квадратичной функции потерь (т. е. вместо &lt;strong&gt;\(\max(0, f_j - f_{y_i} + 1)^2\)&lt;/strong&gt;). Вторым распространенным выбором является &lt;strong&gt;классификатор Softmax&lt;/strong&gt;, который использует кросс-энтропийные потери:&lt;br&gt;
$$&lt;br&gt;
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)
$$   &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Проблема: большое количество классов&lt;/strong&gt;. Когда набор меток очень велик (например, слова в английском словаре или &lt;strong&gt;ImageNet&lt;/strong&gt;, содержащий 22 000 категорий), вычисление полной вероятности по &lt;strong&gt;методу Softmax&lt;/strong&gt; становится дорогостоящим. Для некоторых приложений популярны приближённые версии. Например, в задачах обработки естественного языка может быть полезно использовать &lt;em&gt;иерархический Softmax&lt;/em&gt; (см. одно из объяснений &lt;a href="http://arxiv.org/pdf/1310.4546.pdf"&gt;здесь&lt;/a&gt; (pdf)).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Иерархический Softmax&lt;/em&gt; раскладывает слова на метки в виде дерева. Затем каждая метка представляется в виде пути по дереву, и в каждом узле дерева обучается &lt;strong&gt;классификатор Softmax&lt;/strong&gt;, чтобы различать левую и правую ветви. Структура дерева сильно влияет на производительность и, как правило, зависит от задачи.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Классификация атрибутов&lt;/strong&gt;. Оба приведённых выше примера предполагают, что существует единственный правильный ответ &lt;strong&gt;\(y_i\)&lt;/strong&gt;. Но что , если &lt;strong&gt;\(y_i\)&lt;/strong&gt;- это бинарный вектор, в котором каждый пример может иметь или не иметь определённый атрибут, и где атрибуты не исключают друг друга? Например, изображения &lt;strong&gt;Вконтакте&lt;/strong&gt; можно рассматривать как помеченные определённым подмножеством хэштегов из большого набора всех хэштегов, и изображение может содержать несколько хэштегов. Разумным подходом в этом случае будет создание бинарного классификатора для каждого отдельного атрибута. Например, бинарный классификатор для каждой категории отдельно будет иметь вид:&lt;br&gt;
$$
L_i = \sum_j \max(0, 1 - y_{ij} f_j)
$$&lt;br&gt;
где сумма по всем категориям &lt;strong&gt;\(j\)&lt;/strong&gt; и &lt;strong&gt;\(y_{ij}\)&lt;/strong&gt; равно &lt;strong&gt;+1&lt;/strong&gt; или &lt;strong&gt;-1&lt;/strong&gt; в зависимости от того, помечен ли i-й пример j-м атрибутом, а вектор оценки &lt;strong&gt;\(f_j\)&lt;/strong&gt; будет положительным, если класс прогнозируется как присутствующий, и отрицательным в противном случае. Обратите внимание, что потери накапливаются, если положительный пример имеет оценку меньше &lt;strong&gt;+1&lt;/strong&gt; или если отрицательный пример имеет оценку больше &lt;strong&gt;-1&lt;/strong&gt;.&lt;br&gt;
Альтернативой этому подходу было бы обучение классификатора логистической регрессии для каждого атрибута по отдельности. Бинарный классификатор логистической регрессии имеет только два класса &lt;em&gt;(0, 1)&lt;/em&gt; и вычисляет вероятность класса &lt;strong&gt;1&lt;/strong&gt; следующим образом:&lt;br&gt;
$$
P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)
$$&lt;br&gt;
Поскольку сумма вероятностей классов &lt;em&gt;1 и 0&lt;/em&gt; равна единице, вероятность класса &lt;strong&gt;0&lt;/strong&gt; равна &lt;strong&gt;\(P(y = 0 \mid x; w, b) = 1 - P(y = 1 \mid x; w,b)\)&lt;/strong&gt;. Таким образом, пример классифицируется как положительный &lt;strong&gt;(y = 1)&lt;/strong&gt;, если &lt;strong&gt;\(\sigma (w^Tx + b) &amp;gt; 0.5\)&lt;/strong&gt;, или что эквивалентно , если оценка &lt;strong&gt;\(w^Tx +b &amp;gt; 0\)&lt;/strong&gt;. Затем функция потерь максимизирует эту вероятность. Вы можете убедиться, что это сводится к минимизации отрицательного логарифма правдоподобия:&lt;br&gt;
$$
L_i = -\sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))
$$&lt;br&gt;
где этикетки &lt;strong&gt;\(y_{ij}\)&lt;/strong&gt; считаются равными либо &lt;em&gt;1 (положительному), либо 0 (отрицательному)&lt;/em&gt;, и  &lt;strong&gt;\(\sigma(\cdot)\)&lt;/strong&gt;. Это сигмоидальная функция. Приведённое выше выражение может показаться пугающим, но градиент на &lt;strong&gt;f&lt;/strong&gt; на самом деле он чрезвычайно прост и интуитивно понятен: &lt;strong&gt;\(\partial{L_i} / \partial{f_j} = \sigma(f_j) - y_{ij}\)&lt;/strong&gt; (поскольку вы можете перепроверить себя, взяв производные).  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Регрессия&lt;/strong&gt; — это задача прогнозирования величин с действительными значениями, таких как цена дома или длина чего-либо на изображении. Для решения этой задачи обычно вычисляют потери между прогнозируемой величиной и истинным ответом, а затем измеряют норму &lt;strong&gt;\(L_2\)&lt;/strong&gt; в квадрате или норму &lt;strong&gt;\(L_1\)&lt;/strong&gt; разности. Норма &lt;strong&gt;\(L_2\)&lt;/strong&gt; в квадрате вычисляет потери для одного примера в виде:&lt;br&gt;
$$
L_i = \Vert f - y_i \Vert_2^2
$$&lt;br&gt;
Причина, по которой норма &lt;strong&gt;\(L_2\)&lt;/strong&gt; возводится в квадрат в целевой функции, заключается в том, что градиент становится намного проще, не меняя оптимальные параметры, поскольку возведение в квадрат — это монотонная операция. Норма &lt;strong&gt;\(L_1\)&lt;/strong&gt; вычисляется путём суммирования абсолютных значений по каждому измерению:&lt;br&gt;
$$
L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)&lt;em&gt;j \mid
$$&lt;br&gt;
_где сумма__ &lt;strong&gt;\(\sum_j\)&lt;/strong&gt; это сумма по всем параметрам желаемого прогноза, если прогнозируется более одной величины. Рассмотрим только j-й параметр i-го примера и обозначим разницу между истинным и прогнозируемым значением как &lt;strong&gt;\(\delta_{ij}\)&lt;/strong&gt;, градиент для этого измерения (т.е. &lt;strong&gt;\(\partial{L_i} / \partial{f_j}\))&lt;/strong&gt;) легко выводится как либо &lt;strong&gt;\(\delta_{ij}\)&lt;/strong&gt; с нормой &lt;strong&gt;\(L_2\)&lt;/strong&gt;, или &lt;strong&gt;\(sign(\delta_{ij})\)&lt;/strong&gt;.То есть градиент оценки будет либо прямо пропорционален разнице в ошибках, либо будет фиксированным и унаследует только знак разницы.&lt;br&gt;
_Предупреждение&lt;/em&gt;: важно отметить, что функцию потерь &lt;strong&gt;\(L_2\)&lt;/strong&gt; гораздо сложнее оптимизировать, чем более стабильную функцию потерь, такую как &lt;strong&gt;Softmax&lt;/strong&gt;.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Интуитивно понятно, что для этого требуется очень хрупкое и специфическое свойство сети, чтобы она выдавала ровно одно правильное значение для каждого входного сигнала (и его расширений). Обратите внимание, что это не относится к &lt;strong&gt;Softmax&lt;/strong&gt;, где точное значение каждого балла менее важно: важно только, чтобы их величины были соответствующими. Кроме того, функция потерь &lt;strong&gt;\(L_2\)&lt;/strong&gt; менее устойчива, поскольку выбросы могут приводить к огромным градиентам. Столкнувшись с проблемой регрессии, сначала подумайте, абсолютно ли недостаточно квантовать выходные данные по ячейкам. Например, если вы прогнозируете звездный рейтинг продукта, возможно, гораздо лучше использовать &lt;em&gt;5 независимых классификаторов&lt;/em&gt; для оценок в 1-5 звезд вместо потери регрессии. Классификация имеет дополнительное преимущество в том, что она может дать вам распределение по результатам регрессии, а не только по одному результату без указания его достоверности. Если вы уверены, что классификация не подходит, используйте &lt;strong&gt;\(L_2\)&lt;/strong&gt;, но будьте осторожны: например, &lt;strong&gt;\(L_2\)&lt;/strong&gt; более нестабилен, и применять отсев в сети (особенно на слое непосредственно перед потерей &lt;strong&gt;\(L_2\)&lt;/strong&gt;) — не лучшая идея.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Сталкиваясь с задачей регрессии, в первую очередь подумайте, действительно ли она необходима. Вместо этого по возможности дискретизируйте выходные данные и выполняйте классификацию по ним.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Структурированное прогнозирование&lt;/strong&gt;. Структурированные потери относятся к случаю, когда метками могут быть произвольные структуры, такие как графы, деревья или другие сложные объекты. Обычно также предполагается, что пространство структур очень велико и его нелегко перебрать. Основная идея структурированных потерь SVM заключается в том, чтобы требовать запас между правильной структурой и &lt;strong&gt;\(y_i\)&lt;/strong&gt; и набравшая наибольшее количество баллов неправильная структура. Эту проблему не принято решать как простую задачу неограниченной оптимизации с градиентным спуском. Вместо этого обычно разрабатываются специальные решатели, позволяющие воспользоваться конкретными упрощающими допущениями структурного пространства. Мы кратко упоминаем проблему, но считаем, что специфика выходит за рамки данного класса.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Краткая сводка&lt;/h2&gt;
&lt;p&gt;Подводя итог:
- Рекомендуется предварительно обработать данные, чтобы их среднее значение было равно нулю, и нормализовать их масштаб до &lt;strong&gt;[-1, 1]&lt;/strong&gt; по каждому признаку
- Инициализируйте веса, извлекая их из гауссова распределения со стандартным отклонением &lt;strong&gt;\(\sqrt{2/n}\), where \(n\)&lt;/strong&gt;, где &lt;strong&gt;n&lt;/strong&gt; - это количество входов в нейрон. Например, в NumPy: &lt;code&gt;w = np.random.randn(n) * sqrt(2.0/n)&lt;/code&gt;.
- Используйте регуляризацию &lt;strong&gt;\(L_2\)&lt;/strong&gt; и отсев (перевернутая версия)
- Используйте пакетную нормализацию
- Мы обсудили различные задачи, которые вы можете выполнять на практике, и наиболее распространённые функции потерь для каждой задачи  &lt;/p&gt;
&lt;p&gt;Теперь мы предварительно обработали данные, настроили и инициализировали модель. В следующем разделе мы рассмотрим процесс обучения и его динамику.&lt;/p&gt;</description><guid>https://mldl.ru/posts/convnets-2/</guid><pubDate>Sun, 09 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Сверточные сети. Введение</title><link>https://mldl.ru/posts/convnets-1/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Сверточные сети. Введение&lt;/h2&gt;
&lt;p&gt;Сожержание: 
- &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Краткое вступление без мозговых аналогий&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Моделирование одного нейрона&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Биологическая мотивация и связи&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Одиночный нейрон как линейный классификатор&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Часто используемые функции активации&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Архитектуры нейронных сетей&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Многоуровневая организация&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Пример вычисления с прямой связью&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Представительская власть&lt;/a&gt;
    + &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Настройка количества слоев и их размеров&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Краткие сведения&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/convnets-1/"&gt;Дополнительные ссылки&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;Краткое вступление&lt;/h2&gt;
&lt;p&gt;Можно представить нейронные сети, не прибегая к аналогам с мозгом. В разделе о линейной классификации мы вычисляли баллы для различных визуальных категорий по изображению с помощью формулы &lt;strong&gt;s=Wx&lt;/strong&gt;, где &lt;strong&gt;W&lt;/strong&gt; была матрицей и &lt;strong&gt;x&lt;/strong&gt; был вектор входных данных, содержащий все пиксельные данные изображения. В случае CIFAR-10 &lt;strong&gt;x&lt;/strong&gt; является вектором-столбцом &lt;strong&gt;[3072x1]&lt;/strong&gt;, и &lt;strong&gt;W&lt;/strong&gt;. Это матрица &lt;strong&gt;[10x3072]&lt;/strong&gt;, так что выходные данные представляют собой вектор из 10 оценок по классам.  &lt;/p&gt;
&lt;p&gt;Примерная нейронная сеть вместо этого вычисляла бы &lt;strong&gt;\( s = \( W_2 \max(0, W_1 x) \)&lt;/strong&gt;. Здесь, &lt;strong&gt;\(W_1\)&lt;/strong&gt; может быть, например, матрицей &lt;strong&gt;[100x3072]&lt;/strong&gt;, преобразующей изображение в 100-мерный промежуточный вектор. Функция &lt;strong&gt;\(max(0,-) \)&lt;/strong&gt; это нелинейность, котрая применяется поэлементно. Существует несколько вариантов нелинейности (которые мы рассмотрим ниже), но этот вариант является распространённым и просто приравнивает все значения ниже нуля к нулю. Наконец, матрица &lt;strong&gt;\(W_2\)&lt;/strong&gt; тогда будет иметь размер &lt;strong&gt;[10x100]&lt;/strong&gt;, так что мы снова получим 10 чисел, которые мы интерпретируем как оценки классов. Обратите внимание, что нелинейность имеет решающее значение с точки зрения вычислений — если бы мы её не использовали, то две матрицы можно было бы объединить в одну, и, следовательно, прогнозируемые оценки классов снова были бы линейной функцией входных данных. Нелинейность — это то, что даёт нам &lt;strong&gt;колебания&lt;/strong&gt;. Параметры &lt;strong&gt;W2,W1&lt;/strong&gt;. Они обучаются с помощью стохастического градиентного спуска, а их градиенты вычисляются с помощью правила дифференцирования (и обратного распространения ошибки).  &lt;/p&gt;
&lt;p&gt;Аналогично трехслойная нейронная сеть могла бы выглядеть следующим образом  &lt;strong&gt;\( s = W_3 \max(0, W_2 \max(0, W_1 x)) \)&lt;/strong&gt;, где все &lt;strong&gt;\(W_3, W_2, W_1\)&lt;/strong&gt;- это параметры, которые необходимо изучить. Размеры промежуточных скрытых векторов являются гиперпараметрами сети, и мы рассмотрим, как их можно задать позже. Теперь давайте посмотрим, как можно интерпретировать эти вычисления с точки зрения нейронов/сети.  &lt;/p&gt;
&lt;h2&gt;Моделирование одного нейрона&lt;/h2&gt;
&lt;p&gt;Изначально область нейронных сетей была в первую очередь ориентирована на моделирование биологических нейронных систем, но с тех пор она расширилась и стала заниматься разработкой и достижением хороших результатов в задачах машинного обучения. Тем не менее, мы начнём наше обсуждение с очень краткого и общего описания биологической системы, которая послужила источником вдохновения для значительной части этой области.  &lt;/p&gt;
&lt;h3&gt;Биологическая мотивация и связи&lt;/h3&gt;
&lt;p&gt;Основной вычислительной единицей мозга является &lt;strong&gt;нейрон&lt;/strong&gt;. В нервной системе человека насчитывается около 86 миллиардов нейронов, и они соединены примерно с &lt;strong&gt;10^14&lt;/strong&gt; — &lt;strong&gt;10^15&lt;/strong&gt; &lt;strong&gt;синапсами&lt;/strong&gt;. На схеме ниже показан схематичный рисунок биологического нейрона (сверху) и распространённая математическая модель (снизу). Каждый нейрон получает входные сигналы от своих &lt;strong&gt;дендритов&lt;/strong&gt; и выдаёт выходные сигналы по своему (единственному) &lt;strong&gt;аксону&lt;/strong&gt;. В конечном итоге аксон разветвляется и соединяется через синапсы с дендритами других нейронов. В вычислительной модели нейрона сигналы, которые проходят по аксонам (например, &lt;strong&gt;\(x_0\)&lt;/strong&gt; взаимодействуют мультипликативно (например, &lt;strong&gt;\(w_0 x_0\)&lt;/strong&gt; с дендритами другого нейрона в зависимости от силы синапса (например, &lt;strong&gt;\(w_0\)&lt;/strong&gt;. Идея заключается в том, что синаптические силы (веса &lt;strong&gt;w&lt;/strong&gt;) являются обучаемыми и контролируют силу влияния (и его направление: возбуждающее (положительный вес) или тормозящее (отрицательный вес) одного нейрона на другой. В базовой модели дендриты передают сигнал в тело клетки, где он суммируется. Если итоговая сумма превышает определённый порог, нейрон может &lt;em&gt;сработать&lt;/em&gt;, отправив импульс по своему аксону. В вычислительной модели мы предполагаем, что точное время срабатывания импульсов не имеет значения и что информация передаётся только частотой срабатывания. Основываясь на этой интерпретации, это &lt;em&gt;частотного кода&lt;/em&gt;, мы моделируем &lt;em&gt;частоту срабатывания&lt;/em&gt; нейрона с помощью &lt;strong&gt;функции активации f&lt;/strong&gt;, которая представляет собой частоту импульсов вдоль аксона. Исторически сложилось так, что в качестве функции активации часто используется &lt;strong&gt;сигмоидальная функция σ&lt;/strong&gt;, поскольку она принимает вещественные входные данные (силу сигнала после суммирования) и преобразует их в диапазон от 0 до 1. Подробнее об этих функциях активации мы поговорим далее в этом разделе.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neuron.png"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neuron_model.jpeg"&gt;&lt;br&gt;
  Карикатурное изображение биологического нейрона (&lt;strong&gt;сверху&lt;/strong&gt;) и его математическая модель (&lt;strong&gt;снизу&lt;/strong&gt;).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Пример кода для прямого распространения сигнала по одному нейрону может выглядеть следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Neuron&lt;/span&gt;(&lt;span class="n"&gt;object&lt;/span&gt;):
  &lt;span class="c1"&gt;# ... &lt;/span&gt;
  &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;forward&lt;/span&gt;(&lt;span class="nb"&gt;self&lt;/span&gt;, &lt;span class="n"&gt;inputs&lt;/span&gt;):
    &lt;span class="s"&gt;""" assume inputs and weights are 1-D numpy arrays and bias is a number """&lt;/span&gt;
    &lt;span class="n"&gt;cell_body_sum&lt;/span&gt; = &lt;span class="n"&gt;np&lt;/span&gt;.&lt;span class="nb"&gt;sum&lt;/span&gt;(&lt;span class="n"&gt;inputs&lt;/span&gt; * &lt;span class="nb"&gt;self&lt;/span&gt;.&lt;span class="n"&gt;weights&lt;/span&gt;) + &lt;span class="nb"&gt;self&lt;/span&gt;.&lt;span class="n"&gt;bias&lt;/span&gt;
    &lt;span class="n"&gt;firing_rate&lt;/span&gt; = &lt;span class="mf"&gt;1.0&lt;/span&gt; / (&lt;span class="mf"&gt;1.0&lt;/span&gt; + &lt;span class="n"&gt;math&lt;/span&gt;.&lt;span class="nb"&gt;exp&lt;/span&gt;(-&lt;span class="n"&gt;cell_body_sum&lt;/span&gt;)) &lt;span class="c1"&gt;# sigmoid activation function&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;firing_rate&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Другими словами, каждый нейрон выполняет скалярное произведение входных данных и своих весов, добавляет смещение и применяет нелинейность (или функцию активации), в данном случае сигмоидальную ** sigmoid \(\sigma(x) = 1/(1+e^{-x})\)**. Более подробно о различных функциях активации мы расскажем в конце этого раздела.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Грубая модель&lt;/strong&gt;. Важно подчеркнуть, что эта модель биологического нейрона является очень грубой: например, существует множество различных типов нейронов, каждый из которых обладает своими свойствами. Дендриты в биологических нейронах выполняют сложные нелинейные вычисления. Синапсы — это не просто один вес, это сложная нелинейная динамическая система. Известно, что точное время выходных импульсов во многих системах имеет большое значение, что позволяет предположить, что приближение кода скорости может не работать. Из-за всех этих и многих других упрощений будьте готовы к тому, что любой, кто разбирается в нейробиологии, будет возмущаться, если вы проведёте аналогию между нейронными сетями и реальным мозгом. Если вам интересно, ознакомьтесь с этим &lt;a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf"&gt;обзором&lt;/a&gt; (в формате pdf) или с этим &lt;a href="http://www.sciencedirect.com/science/article/pii/S0959438814000130"&gt;обзором&lt;/a&gt;, опубликованным недавно.  &lt;/p&gt;
&lt;h3&gt;Одиночный нейрон как линейный классификатор&lt;/h3&gt;
&lt;p&gt;Математическая форма прямого вычисления модели нейрона может показаться вам знакомой. Как мы видели на примере линейных классификаторов, нейрон может «любить» (активация близка к единице) или «не любить» (активация близка к нулю) определённые линейные области своего входного пространства. Следовательно, с помощью подходящей функции потерь на выходе нейрона мы можем превратить один нейрон в линейный классификатор:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Бинарный классификатор Softmax&lt;/strong&gt;. Например, мы можем интерпретировать &lt;strong&gt;\(\sigma(\sum_i * w_i * x_i + b)\ )&lt;/strong&gt;,как вероятность_ одного из классов &lt;strong&gt;\(P(y_i = 1 \mid x_i; w) \)&lt;/strong&gt;. Вероятность появления другого класса была бы равна &lt;strong&gt;\(P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) \)&lt;/strong&gt;, так как их сумма должна быть равна единице. С помощью этой интерпретации мы можем сформулировать функцию потерь перекрёстной энтропии, как мы видели в разделе «Линейная классификация», и оптимизация этой функции приведёт к созданию бинарного классификатора Softmax (также известного как &lt;em&gt;логистическая регрессия&lt;/em&gt;). Поскольку сигмоидальная функция принимает значения от 0 до 1, прогнозы этого классификатора основаны на том, превышает ли выходное значение нейрона &lt;strong&gt;0,5&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Бинарный классификатор SVM&lt;/strong&gt;. В качестве альтернативы мы могли бы добавить к выходу нейрона функцию потерь с максимальным зазором и обучить его как бинарную машину опорных векторов.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Интерпретация регуляризации&lt;/strong&gt;. В этом биологическом контексте потеря регуляризации в обоих случаях SVM/Softmax может быть интерпретирована как постепенное забывание, поскольку она приводит к уменьшению всех синаптических весов &lt;strong&gt;w&lt;/strong&gt; приближается к нулю после каждого обновления параметра.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Один нейрон можно использовать для реализации бинарного классификатора (например, бинарного классификатора Softmax или бинарного классификатора SVM)  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Часто используемые функции активации&lt;/h3&gt;
&lt;p&gt;Каждая функция активации (или &lt;em&gt;нелинейность&lt;/em&gt;) принимает одно число и выполняет над ним определённую фиксированную математическую операцию. На практике вы можете столкнуться с несколькими функциями активации: &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/sigmoid.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn1/tanh.jpeg"&gt;&lt;br&gt;
&lt;strong&gt;Сверху&lt;/strong&gt;: сигмоидальная нелинейность сжимает действительные числа до диапазона &lt;strong&gt;[0,1]&lt;/strong&gt;.&lt;br&gt;
&lt;strong&gt;Справа&lt;/strong&gt;: нелинейность tanh сжимает действительные числа до диапазона &lt;strong&gt;[-1,1]&lt;/strong&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Сигмоида&lt;/strong&gt;. Сигмоидальная нелинейность имеет математическую форму &lt;strong&gt;\(\sigma(x) = 1 / (1 + e^{-x})\)&lt;/strong&gt;. Она показана на изображении выше слева. Как упоминалось в предыдущем разделе, она принимает вещественное число и «сжимает» его до диапазона &lt;strong&gt;от 0 до 1&lt;/strong&gt;. В частности, большие отрицательные числа становятся равными &lt;strong&gt;0&lt;/strong&gt;, а большие положительные числа становятся равными &lt;strong&gt;1&lt;/strong&gt;. Сигмоидальная функция часто использовалась в прошлом, так как её можно интерпретировать как частоту срабатывания нейрона: от полного отсутствия срабатывания (&lt;strong&gt;0&lt;/strong&gt;) до полного срабатывания с предполагаемой максимальной частотой (&lt;strong&gt;1&lt;/strong&gt;). На практике сигмоидальная нелинейность в последнее время вышла из моды и используется редко. У неё есть два основных недостатка:&lt;br&gt;
- &lt;em&gt;Сигмоиды насыщаются и уничтожают градиенты&lt;/em&gt;. Очень нежелательное свойство сигмоидального нейрона заключается в том, что, когда активация нейрона насыщается на одном из концов &lt;strong&gt;0&lt;/strong&gt; или &lt;strong&gt;1&lt;/strong&gt;, градиент в этих областях почти равен нулю. Напомним, что во время обратного распространения ошибки этот (локальный) градиент будет умножен на градиент выхода этого нейрона для всей задачи. Поэтому, если локальный градиент очень мал, он фактически «уничтожит» градиент, и почти никакой сигнал не пройдёт через нейрон к его весам и рекурсивно к его данным. Кроме того, необходимо соблюдать особую осторожность при инициализации весов сигмоидальных нейронов, чтобы предотвратить перегрузку. Например, если начальные веса слишком велики, то большинство нейронов будут перегружены, и сеть едва ли будет обучаться.&lt;br&gt;
- &lt;em&gt;Сигмоидальные выходные данные не центрированы по нулю&lt;/em&gt;. Это нежелательно, так как нейроны на более поздних уровнях обработки в нейронной сети (подробнее об этом позже) будут получать данные, не центрированные по нулю. Это влияет на динамику во время градиентного спуска, потому что если данные, поступающие в нейрон, всегда положительные (например, &lt;strong&gt;x&amp;gt;0&lt;/strong&gt; поэлементно в &lt;strong&gt;\(f = w^Tx + b\)&lt;/strong&gt;)), тогда градиент по весам &lt;strong&gt;w&lt;/strong&gt; во время обратного распространения ошибки все значения станут либо положительными, либо отрицательными (в зависимости от градиента всего выражения &lt;strong&gt;f&lt;/strong&gt;). Это может привести к нежелательной зигзагообразной динамике в обновлении градиентов весовых коэффициентов. Однако обратите внимание, что после суммирования этих градиентов по пакету данных окончательное обновление весовых коэффициентов может иметь разные знаки, что несколько смягчает эту проблему. Таким образом, это неудобство, но его последствия менее серьёзны по сравнению с проблемой насыщенной активации, описанной выше.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tanh&lt;/strong&gt;. Нелинейность tanh показана на изображении выше снизу. Она сжимает вещественное число до диапазона &lt;strong&gt;[-1, 1]&lt;/strong&gt;. Как и в случае с сигмоидальным нейроном, его активация насыщается, но, в отличие от сигмоидального нейрона, его выходная величина смещена относительно нуля. Поэтому на практике &lt;em&gt;нелинейность tanh всегда предпочтительнее сигмоидальной нелинейности&lt;/em&gt;. Также обратите внимание, что нейрон tanh — это просто масштабированный сигмоидальный нейрон, из-за чего, в частности, верно следующее: &lt;strong&gt;\( \tanh(x) = 2 \sigma(2x) -1  \)&lt;/strong&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/relu.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn1/alexplot.jpeg"&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Сверху&lt;/strong&gt;: функция активации выпрямленной линейной единицы (&lt;em&gt;ReLU&lt;/em&gt;), которая равна нулю, когда &lt;strong&gt;x &amp;lt; 0&lt;/strong&gt;, а затем линейна с наклоном &lt;strong&gt;1&lt;/strong&gt;, когда &lt;strong&gt;x &amp;gt; 0&lt;/strong&gt;. &lt;br&gt;
&lt;strong&gt;Снизу&lt;/strong&gt;: график из статьи &lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"&gt;Крижевски и др.&lt;/a&gt; (pdf), показывающий 6-кратное улучшение сходимости с модулем ReLU по сравнению с модулем tanh.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;ReLU&lt;/strong&gt;. Выпрямленный линейный блок стал очень популярным в последние несколько лет. Он вычисляет функцию &lt;strong&gt;\(f(x) = \max(0, x)\)&lt;/strong&gt;. Другими словами, активация просто ограничивается нулём (см. изображение выше сверху). У использования &lt;em&gt;ReLU&lt;/em&gt; есть несколько плюсов и минусов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(+) Было обнаружено, что она значительно ускоряет (например, в 6 раз в &lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"&gt;Крижевски и др.&lt;/a&gt;) сходимость стохастического градиентного спуска по сравнению с сигмоидальными/тангенциальными функциями. Утверждается, что это связано с её линейной, ненасыщаемой формой.&lt;/li&gt;
&lt;li&gt;(+) По сравнению с нейронами tanh/сигмоидными нейронами, которые требуют дорогостоящих операций (экспоненциальных и т. д.), &lt;em&gt;ReLU&lt;/em&gt; можно реализовать, просто установив пороговое значение для матрицы активации равным нулю.&lt;/li&gt;
&lt;li&gt;(-) К сожалению, блоки &lt;em&gt;ReLU&lt;/em&gt; могут быть нестабильными во время обучения и могут &lt;em&gt;«умирать»&lt;/em&gt; . Например, большой градиент, проходящий через нейрон ReLU, может привести к обновлению весов таким образом, что нейрон больше никогда не активируется ни для одной точки данных. Если это произойдёт, то градиент, проходящий через блок, с этого момента будет равен нулю. То есть блоки &lt;em&gt;ReLU&lt;/em&gt; могут необратимо &lt;em&gt;«умирать»&lt;/em&gt; во время обучения, поскольку они могут быть отброшены от множества данных. Например, если скорость обучения установлена слишком высокой, вы можете обнаружить, что до &lt;strong&gt;40%&lt;/strong&gt; вашей сети могут быть &lt;em&gt;«мёртвыми»&lt;/em&gt; (то есть нейроны, которые никогда не активируются на протяжении всего набора обучающих данных). При правильной настройке скорости обучения эта проблема возникает реже.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Протекающий ReLU&lt;/strong&gt;. Протекающий &lt;em&gt;ReLU&lt;/em&gt; — это одна из попыток решить проблему &lt;em&gt;«умирающего&lt;/em&gt; &lt;em&gt;ReLU»&lt;/em&gt;. Вместо того чтобы быть равной нулю при &lt;strong&gt;x &amp;lt; 0&lt;/strong&gt;, функция просачивающегося &lt;em&gt;ReLU&lt;/em&gt; будет иметь небольшой положительный наклон (около &lt;strong&gt;0,01&lt;/strong&gt;). То есть функция вычисляет &lt;strong&gt;\(f(x) = \mathbb{1}(x &amp;lt; 0) (\alpha x) + \mathbb{1}(x&amp;gt;=0) (x) \) where \(\alpha\)&lt;/strong&gt;, где &lt;strong&gt;α&lt;/strong&gt;- это небольшая константа. Некоторые люди сообщают об успехах с использованием этой формы функции активации, но результаты не всегда стабильны. Наклон в отрицательной области также может быть параметром каждого нейрона, как в случае с нейронами &lt;em&gt;PReLU&lt;/em&gt;, представленными в работе &lt;a href="http://arxiv.org/abs/1502.01852"&gt;«Глубокое погружение в выпрямители»&lt;/a&gt; Кайминга Хэ и др., 2015. Однако в настоящее время неясно, насколько стабильны преимущества при выполнении разных задач.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maxout&lt;/strong&gt;. Были предложены другие типы устройств, которые не имеют функциональной формы &lt;strong&gt;\(f(w^Tx + b)\)&lt;/strong&gt;, где нелинейность применяется к скалярному произведению весов и данных. Одним из относительно популярных вариантов является нейрон Maxout (введённый недавно &lt;a href="https://arxiv.org/abs/1302.4389"&gt;Goodfellowи др.&lt;/a&gt;), который обобщает ReLU и его неидеальную версию. Нейрон Maxout вычисляет функцию &lt;strong&gt;\(\max(w_1^Tx+b_1, w_2^Tx + b_2)\)&lt;/strong&gt;. Обратите внимание, что и &lt;em&gt;ReLU&lt;/em&gt;, и &lt;em&gt;Leaky&lt;/em&gt; &lt;em&gt;ReLU&lt;/em&gt; являются частным случаем этой формы (например, для &lt;em&gt;ReLU&lt;/em&gt; мы имеем &lt;strong&gt;\(w_1, b_1 = 0\)&lt;/strong&gt;). Таким образом, нейрон Maxout обладает всеми преимуществами блока &lt;em&gt;ReLU&lt;/em&gt; (линейный режим работы, отсутствие насыщения) и не имеет его недостатков (умирающий &lt;em&gt;ReLU&lt;/em&gt;). Однако, в отличие от нейронов &lt;em&gt;ReLU&lt;/em&gt;, он удваивает количество параметров для каждого отдельного нейрона, что приводит к большому общему количеству параметров.  &lt;/p&gt;
&lt;p&gt;На этом мы завершаем обсуждение наиболее распространённых типов нейронов и их функций активации. В качестве последнего комментария: очень редко в одной сети сочетаются разные типы нейронов, хотя в этом нет принципиальных проблем.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: &lt;em&gt;«Какой тип нейронов мне следует использовать?»&lt;/em&gt; Используйте нелинейность &lt;em&gt;ReLU&lt;/em&gt;, будьте осторожны с темпами обучения и, возможно, отслеживайте долю «мёртвых» нейронов в сети. Если вас это беспокоит, попробуйте &lt;em&gt;Leaky&lt;/em&gt; &lt;em&gt;ReLU&lt;/em&gt; или Maxout. Никогда не используйте сигмоид. Попробуйте tanh, но будьте готовы к тому, что он будет работать хуже, чем &lt;em&gt;ReLU&lt;/em&gt;/&lt;em&gt;Maxout&lt;/em&gt;.  &lt;/p&gt;
&lt;h2&gt;Архитектуры нейронных сетей&lt;/h2&gt;
&lt;h3&gt;Многоуровневая организация&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Нейронные сети как нейроны в графах&lt;/strong&gt;. Нейронные сети моделируются как совокупности нейронов, соединённых в ациклический граф. Другими словами, выходные данные одних нейронов могут становиться входными данными для других нейронов. Циклы недопустимы, так как это привело бы к бесконечному циклу при прямом проходе сети. Вместо аморфных скоплений соединённых нейронов модели нейронных сетей часто состоят из отдельных слоёв нейронов. Для обычных нейронных сетей наиболее распространённым типом слоёв является &lt;strong&gt;слой с полной связью&lt;/strong&gt;, в котором нейроны между двумя соседними слоями полностью соединены попарно, но нейроны в пределах одного слоя не имеют общих связей. Ниже приведены два примера топологий нейронных сетей, в которых используется набор слоёв с полной связью:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neural_net.jpeg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/nn1/neural_net2.jpeg"&gt;&lt;br&gt;
&lt;strong&gt;Сверху&lt;/strong&gt;: двухслойная нейронная сеть (один скрытый слой из 4 нейронов (или единиц) и один выходной слой из 2 нейронов) с тремя входами. &lt;strong&gt;Снизу&lt;/strong&gt;: трёхслойная нейронная сеть с тремя входами, двумя скрытыми слоями по 4 нейрона в каждом и одним выходным слоем. Обратите внимание, что в обоих случаях между нейронами разных слоёв есть связи (синапсы), но не внутри слоя.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Соглашения об именовании&lt;/strong&gt;. Обратите внимание, что, когда мы говорим о N-слойной нейронной сети, мы не учитываем входной слой. Таким образом, однослойная нейронная сеть — это сеть без скрытых слоёв (входные данные напрямую преобразуются в выходные). В этом смысле иногда можно услышать, что логистическая регрессия или метод опорных векторов — это просто частный случай однослойных нейронных сетей. Вы также можете услышать, что эти сети называют &lt;em&gt;«искусственными нейронными сетями»&lt;/em&gt; (ИНС) или &lt;em&gt;«многослойными перцептронами»&lt;/em&gt; (МПП). Многим не нравятся аналогии между нейронными сетями и реальным мозгом, и они предпочитают называть нейроны &lt;em&gt;единицами&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Выходной слой&lt;/strong&gt;. В отличие от всех остальных слоёв нейронной сети, нейроны выходного слоя чаще всего не имеют функции активации (или можно считать, что у них линейная функция активации). Это связано с тем, что последний выходной слой обычно используется для представления оценок классов (например, при классификации), которые являются произвольными действительными числами, или для представления некоторой действительной цели (например, при регрессии).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Размер нейронных сетей&lt;/strong&gt;. Два показателя, которые обычно используются для измерения размера нейронных сетей, — это количество нейронов или, чаще, количество параметров. Рассмотрим две сети на рисунке выше:
- В первой сети (слева) &lt;strong&gt;4 + 2 = 6&lt;/strong&gt; нейронов (не считая входных данных), &lt;strong&gt;(3 x 4) + (4 x 2) = 20&lt;/strong&gt; весовых коэффициентов и &lt;strong&gt;4 + 2 = 6&lt;/strong&gt; смещений, всего 26 обучаемых параметров.
- Во второй сети (&lt;strong&gt;справа&lt;/strong&gt;) &lt;strong&gt;4 + 4 + 1 = 9&lt;/strong&gt; нейронов, &lt;strong&gt;(3 x 4) + (4 x 4) + (4 x 1) = 12 + 16 + 4 = 32&lt;/strong&gt; весовых коэффициента и &lt;strong&gt;4 + 4 + 1 = 9&lt;/strong&gt; смещений, всего &lt;strong&gt;41 обучаемый параметр&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;Для сравнения: современные свёрточные нейронные сети содержат порядка 100 миллионов параметров и обычно состоят примерно из 10–20 слоёв (отсюда &lt;em&gt;глубокое обучение&lt;/em&gt;). Однако, как мы увидим, количество &lt;em&gt;эффективных&lt;/em&gt; связей значительно больше из-за совместного использования параметров. Подробнее об этом в модуле «Свёрточные нейронные сети».  &lt;/p&gt;
&lt;h3&gt;Пример вычисления с прямой связью&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Повторное матричное умножение в сочетании с функцией активации&lt;/em&gt;. Одна из основных причин, по которой нейронные сети организованы в виде слоёв, заключается в том, что такая структура позволяет очень просто и эффективно оценивать нейронные сети с помощью матричных векторных операций. Если рассматривать трёхслойную нейронную сеть на приведённой выше схеме, то входными данными будет вектор &lt;strong&gt;[3x1]&lt;/strong&gt;. Все весовые коэффициенты для слоя можно хранить в одной матрице. Например, веса первого скрытого слоя &lt;code&gt;W1&lt;/code&gt; будут иметь размер &lt;strong&gt;[4x3]&lt;/strong&gt;, а смещения для всех нейронов будут находиться в векторе &lt;code&gt;b1&lt;/code&gt; размером &lt;strong&gt;[4x1]&lt;/strong&gt;. Здесь каждый нейрон имеет свои веса в строке &lt;code&gt;W1&lt;/code&gt;, поэтому умножение матрицы на вектор &lt;code&gt;np.dot(W1,x)&lt;/code&gt; вычисляет активации всех нейронов в этом слое. Аналогично, &lt;code&gt;W2&lt;/code&gt; будет матрицей &lt;strong&gt;[4x4]&lt;/strong&gt;, которая хранит связи второго скрытого слоя, а &lt;code&gt;W3&lt;/code&gt; — матрицей &lt;strong&gt;[1x4]&lt;/strong&gt; для последнего (выходного) слоя. Полный прямой проход этой трёхслойной нейронной сети — это просто три матричных умножения, объединённых с применением функции активации:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;pass&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;layer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;neural&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;network&lt;/span&gt;:
&lt;span class="nv"&gt;f&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;.&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;.&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;exp&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;activation&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;sigmoid&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="k"&gt;random&lt;/span&gt;.&lt;span class="nv"&gt;randn&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;random&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;input&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;vector&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;three&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;numbers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="nv"&gt;x1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;h1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;f&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;dot&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;W1&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;x&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;b1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;calculate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;first&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;hidden&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;layer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;activations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nv"&gt;x1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;h2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;f&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;dot&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;W2&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;h1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;b2&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;calculate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;second&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;hidden&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;layer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;activations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nv"&gt;x1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;out&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;dot&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;W3&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;h2&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;b3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;output&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;neuron&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nv"&gt;x1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;В приведённом выше коде &lt;code&gt;W1,W2,W3,b1,b2,b3&lt;/code&gt; — это обучаемые параметры сети. Обратите внимание, что вместо одного входного вектора-столбца переменная &lt;code&gt;x&lt;/code&gt; может содержать целую выборку обучающих данных (где каждый входной пример будет столбцом &lt;code&gt;x&lt;/code&gt;), и тогда все примеры будут эффективно обрабатываться параллельно. Обратите внимание, что последний слой нейронной сети обычно не имеет функции активации (например, он представляет собой (числовое) значение класса в задаче классификации).  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Прямой проход полносвязного слоя соответствует одному умножению матриц, за которым следует смещение и функция активации.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Представительская власть&lt;/h3&gt;
&lt;p&gt;Один из способов взглянуть на нейронные сети с полносвязными слоями заключается в том, что они определяют семейство функций, параметры которых задаются весовыми коэффициентами сети. Возникает естественный вопрос: какова репрезентативная мощность этого семейства функций? В частности, существуют ли функции, которые нельзя смоделировать с помощью нейронной сети?  &lt;/p&gt;
&lt;p&gt;Оказывается, что нейронные сети, содержащие хотя бы один скрытый слой, являются универсальными аппроксиматорами. То есть можно показать (например, см. &lt;a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf"&gt;«Аппроксимацию суперпозициями сигмоидальных функций»&lt;/a&gt; 1989 года (pdf) или это &lt;a href="http://neuralnetworksanddeeplearning.com/chap4.html"&gt;интуитивное объяснение&lt;/a&gt; Майкла Нильсена), что для любой непрерывной функции &lt;strong&gt;f(x)&lt;/strong&gt; и некоторые &lt;strong&gt;ϵ&amp;gt;0&lt;/strong&gt;, существует Нейронная сеть &lt;strong&gt;g(x)_ с одним скрытым слоем (с разумным выбором нелинейности, например, сигмоидальной) таким образом, что &lt;/strong&gt;∀x,∣f(x)−g(x)∣&amp;lt;ϵ__. Другими словами, нейронная сеть может аппроксимировать любую непрерывную функцию.  &lt;/p&gt;
&lt;p&gt;Если для аппроксимации любой функции достаточно одного скрытого слоя, зачем использовать больше слоёв и углубляться в детали? Ответ заключается в том, что тот факт, что двухслойная нейронная сеть является универсальным аппроксиматором, хоть и выглядит красиво с математической точки зрения, на практике является относительно слабым и бесполезным утверждением. В одномерном пространстве функция «сумма пиков индикаторов» &lt;strong&gt;\(g(x) = \sum_i c_i \mathbb{1}(a_i &amp;lt; x &amp;lt; b_i)\)&lt;/strong&gt;, где &lt;strong&gt;\(a,b,c\)&lt;/strong&gt;. Векторы параметров также являются универсальным аппроксиматором, но никто не предлагает использовать эту функциональную форму в машинном обучении. Нейронные сети хорошо работают на практике, потому что они компактно выражают красивые, плавные функции, которые хорошо согласуются со статистическими свойствами данных, с которыми мы сталкиваемся на практике, а также легко обучаются с помощью наших алгоритмов оптимизации (например, градиентного спуска). Точно так же тот факт, что более глубокие сети (с несколькими скрытыми слоями) могут работать лучше, чем сети с одним скрытым слоем, является эмпирическим наблюдением, несмотря на то, что их репрезентативная мощность одинакова.  &lt;/p&gt;
&lt;p&gt;Кстати, на практике часто бывает так, что 3-слойные нейронные сети превосходят 2-слойные, но ещё большее количество слоёв (4, 5, 6) редко приносит большую пользу. Это резко контрастирует с свёрточными сетями, где глубина оказалась чрезвычайно важным компонентом для хорошей системы распознавания (например, порядка 10 обучаемых слоёв). Один из аргументов в пользу этого наблюдения заключается в том, что изображения имеют иерархическую структуру (например, лица состоят из глаз, которые состоят из контуров и т. д.), поэтому несколько уровней обработки интуитивно понятны для этой области данных.  &lt;/p&gt;
&lt;p&gt;Полная история, конечно, гораздо сложнее и является предметом многочисленных недавних исследований. Если вас интересуют эти темы, мы рекомендуем вам прочитать:
 - &lt;a href="http://www.deeplearningbook.org/"&gt;Книга «Глубокое обучение»&lt;/a&gt; Бенджио, Гудфеллоу, Курвиля, в частности &lt;a href="http://www.deeplearningbook.org/contents/mlp.html"&gt;глава 6.4&lt;/a&gt;.
 - &lt;a href="http://arxiv.org/abs/1312.6184"&gt;Действительно ли Глубокие сети должны быть глубокими?&lt;/a&gt;
 - &lt;a href="http://arxiv.org/abs/1412.6550"&gt;ФитНеты: Советы для тонких глубоких Сеток&lt;/a&gt;  &lt;/p&gt;
&lt;h3&gt;Настройка количества слоев и их размеров&lt;/h3&gt;
&lt;p&gt;Как мы решаем, какую архитектуру использовать, когда сталкиваемся с практической задачей? Следует ли нам использовать несколько скрытых слоёв? Один скрытый слой? Два скрытых слоя? Насколько большим должен быть каждый слой? Во-первых, обратите внимание, что по мере увеличения размера и количества слоёв в нейронной сети &lt;strong&gt;ёмкость&lt;/strong&gt; сети увеличивается. То есть пространство представимых функций растёт, поскольку нейроны могут взаимодействовать для выражения множества различных функций. Например, предположим, что у нас есть задача бинарной классификации в двух измерениях. Мы могли бы обучить три отдельные нейронные сети, каждая из которых имеет один скрытый слой определённого размера, и получить следующие классификаторы:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/layer_sizes.jpeg"&gt;&lt;br&gt;
 Более крупные нейронные сети могут представлять более сложные функции. Данные показаны в виде кружков, окрашенных в соответствии с их классом, а под ними показаны области принятия решений обученной нейронной сетью. Вы можете поиграть с этими примерами в этой &lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html"&gt;демо-версии ConvNetsJS&lt;/a&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;На приведённой выше схеме мы видим, что нейронные сети с большим количеством нейронов могут выполнять более сложные функции. Однако это одновременно и благо (поскольку мы можем научиться классифицировать более сложные данные), и проклятие (поскольку легче переобучиться на обучающих данных). &lt;strong&gt;Переобучение&lt;/strong&gt; происходит, когда модель с высокой способностью к обучению подстраивается под шум в данных, а не под (предполагаемую) основную закономерность. Например, модель с 20 скрытыми нейронами подстраивается под все обучающие данные, но за счёт разделения пространства на множество непересекающихся красных и зелёных областей принятия решений. Модель с 3 скрытыми нейронами способна классифицировать данные только в общих чертах. Она моделирует данные как два сгустка и интерпретирует несколько красных точек внутри зелёного кластера как &lt;strong&gt;выбросы&lt;/strong&gt; (шум). На практике это может привести к лучшему &lt;strong&gt;обобщению&lt;/strong&gt; на тестовом наборе данных.  &lt;/p&gt;
&lt;p&gt;Исходя из нашего обсуждения выше, можно сделать вывод, что нейронные сети меньшего размера предпочтительнее, если данные недостаточно сложны, чтобы предотвратить переобучение. Однако это неверно — существует множество других предпочтительных способов предотвращения переобучения в нейронных сетях, которые мы обсудим позже (например, регуляризация &lt;strong&gt;\(L_2\)&lt;/strong&gt;, отсев, входной шум). На практике всегда лучше использовать эти методы для контроля переобучения, а не количество нейронов.  &lt;/p&gt;
&lt;p&gt;Тонкая причина этого заключается в том, что небольшие сети сложнее обучать с помощью локальных методов, таких как градиентный спуск: очевидно, что у их функций потерь относительно мало локальных минимумов, но оказывается, что многие из этих минимумов легче достигаются и являются плохими (то есть с высокими потерями). И наоборот, более крупные нейронные сети содержат значительно больше локальных минимумов, но эти минимумы оказываются гораздо лучше с точки зрения фактических потерь. Поскольку нейронные сети являются невыпуклыми, их свойства трудно изучать математически, но были предприняты некоторые попытки понять эти целевые функции, например, в недавней статье &lt;a href="http://arxiv.org/abs/1412.0233"&gt;«Поверхности потерь в многослойных сетях»&lt;/a&gt;. На практике вы обнаружите, что если вы обучаете небольшую сеть, то конечные потери могут сильно варьироваться — в некоторых случаях вам везёт, и вы сходитесь к хорошему результату, но в некоторых случаях вы застреваете в одном из плохих минимумов. С другой стороны, если вы обучите большую сеть, вы начнёте находить множество различных решений, но разброс в итоговых потерях будет намного меньше. Другими словами, все решения примерно одинаково хороши и в меньшей степени зависят от случайной инициализации.  &lt;/p&gt;
&lt;p&gt;Повторюсь, сила регуляризации — предпочтительный способ контроля переобучения нейронной сети. Мы можем рассмотреть результаты, полученные при трёх различных настройках:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/nn1/reg_strengths.jpeg"&gt;&lt;br&gt;
Влияние силы регуляризации: каждая из приведённых выше нейронных сетей имеет 20 скрытых нейронов, но изменение силы регуляризации делает области окончательного принятия решений более плавными при более высокой регуляризации. Вы можете поиграть с этими примерами в &lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html"&gt;демонстрационной версии ConvNetsJS&lt;/a&gt;.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Вывод заключается в том, что вам не следует использовать более мелкие сети, потому что вы боитесь переобучения. Вместо этого вам следует использовать настолько большую нейронную сеть, насколько позволяет ваш вычислительный бюджет, и применять другие методы регуляризации для контроля переобучения.  &lt;/p&gt;
&lt;h2&gt;Краткие сведения&lt;/h2&gt;
&lt;p&gt;Подводя итог:
- Мы представили очень грубую модель биологического &lt;strong&gt;нейрона&lt;/strong&gt;.
- Мы рассмотрели несколько типов &lt;strong&gt;функций активации&lt;/strong&gt;, которые используются на практике, и наиболее распространённым из них является ReLU.
- Мы представили &lt;strong&gt;нейронные сети&lt;/strong&gt;, в которых нейроны соединены &lt;strong&gt;полностью связанными слоями&lt;/strong&gt;, где нейроны в соседних слоях имеют полные парные связи, но нейроны внутри слоя не соединены.
- Мы увидели, что эта многоуровневая архитектура позволяет очень эффективно оценивать нейронные сети на основе матричных умножений, объединённых с применением функции активации.
- Мы увидели, что нейронные сети являются &lt;strong&gt;универсальными аппроксиматорами функций&lt;/strong&gt;, но мы также обсудили тот факт, что это свойство мало связано с их повсеместным использованием. Они используются потому, что делают определённые «правильные» предположения о функциональных формах функций, которые встречаются на практике.
- Мы обсудили тот факт, что более крупные сети всегда будут работать лучше, чем сети меньшего размера, но их более высокая пропускная способность должна соответствующим образом регулироваться с помощью более сильной регуляризации (например, более высокого затухания весов), иначе они могут переобучаться. В следующих разделах мы рассмотрим другие формы регуляризации (особенно отсев).  &lt;/p&gt;
&lt;h2&gt;Дополнительные ссылки&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.deeplearning.net/tutorial/mlp.html"&gt;deeplearning.net учебное пособие&lt;/a&gt; с Theano&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/"&gt;ConvNetJS&lt;/a&gt; демонстрации для интуиции&lt;/li&gt;
&lt;li&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Учебные пособия Майкла Нильсена&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/convnets-1/</guid><pubDate>Sat, 08 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Обратное распространение ошибки</title><link>https://mldl.ru/posts/backpropagation/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Обратное распространение ошибки&lt;/h2&gt;
&lt;p&gt;Содержание: 
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Введение&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Простые выражения, интерпретирующие градиент&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Составные выражения, цепное правило, обратное распространение&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Интуитивное понимание обратного распространения&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Модульность: Пример сигмовидной активации&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Бэкпроп на практике: Поэтапное вычисление&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Закономерности в обратном потоке&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Градиенты для векторизованных операций&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/backpropagation/"&gt;Краткая сводка&lt;/a&gt;  &lt;/p&gt;
&lt;h2&gt;Введение&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Мотивация&lt;/strong&gt;. В этом разделе мы углубимся в интуитивное понимание &lt;strong&gt;обратного распространения ошибки&lt;/strong&gt;, которое представляет собой способ вычисления градиентов выражений с помощью рекурсивного применения &lt;strong&gt;правила дифференцирования сложной функции&lt;/strong&gt;. Понимание этого процесса и его тонкостей крайне важно для эффективного проектирования, разработки и отладки нейронных сетей.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Постановка задачи&lt;/strong&gt;. Основная задача, рассматриваемая в этом разделе, заключается в следующем: нам дана некоторая функция &lt;strong&gt;f(x)&lt;/strong&gt;,где &lt;strong&gt;x&lt;/strong&gt; является вектором входных данных, и мы заинтересованы в вычислении градиента &lt;strong&gt;f&lt;/strong&gt; в &lt;strong&gt;x&lt;/strong&gt; (т.е. &lt;strong&gt;∇f(x)&lt;/strong&gt;).  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Мотивация&lt;/strong&gt;. Напомним, что основная причина, по которой мы интересуемся этой проблемой, заключается в том, что в конкретном случае нейронных сетей &lt;strong&gt;f&lt;/strong&gt; будет соответствовать функции потерь ( &lt;strong&gt;L&lt;/strong&gt; ) и входные данные &lt;strong&gt;x&lt;/strong&gt; будет состоять из обучающих данных и весовых коэффициентов нейронной сети. Например, в качестве функции потерь может использоваться функция потерь SVM, а в качестве входных данных — обучающие данные &lt;strong&gt;\((x_i,y_i), i=1 \ldots N\)&lt;/strong&gt;, а также веса и предубеждения &lt;strong&gt;W,b&lt;/strong&gt;. Обратите внимание, что (как это обычно бывает в машинном обучении) мы рассматриваем обучающие данные как заданные и фиксированные, а весовые коэффициенты — как переменные, которыми мы можем управлять. Следовательно, даже если мы можем легко использовать обратное распространение ошибки для вычисления градиента по входным примерам &lt;strong&gt;\(x_i\)&lt;/strong&gt;. На практике мы обычно вычисляем градиент только для параметров (например, &lt;strong&gt;W,b&lt;/strong&gt;), чтобы мы могли использовать его для обновления параметров. Однако, как мы увидим позже, градиент по &lt;strong&gt;\(x_i\)&lt;/strong&gt;, например, может быть полезен для визуализации и интерпретации того, что может делать нейронная сеть.  &lt;/p&gt;
&lt;p&gt;Если вы пришли на этот курс и вам удобно вычислять градиенты с помощью правила дифференцирования сложной функции, мы всё равно рекомендуем вам хотя бы бегло просмотреть этот раздел, поскольку в нём представлен редко встречающийся взгляд на обратное распространение ошибки как на обратный поток в схемах с вещественными значениями, и любые полученные вами знания могут пригодиться вам на протяжении всего курса.  &lt;/p&gt;
&lt;h2&gt;Простые выражения и интерпретация градиента&lt;/h2&gt;
&lt;p&gt;Давайте начнём с простого, чтобы разработать обозначения и соглашения для более сложных выражений. Рассмотрим простую функцию умножения двух чисел &lt;strong&gt;f(x,y)=xy&lt;/strong&gt;. Чтобы вычислить частную производную для любого из входных параметров, достаточно воспользоваться простым математическим расчётом:  &lt;/p&gt;
&lt;p&gt;$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
$$  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Интерпретация&lt;/strong&gt;. Помните, что показывают производные: они указывают на скорость изменения функции по отношению к переменной, окружающей бесконечно малую область вблизи определённой точки:  &lt;/p&gt;
&lt;p&gt;$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$  &lt;/p&gt;
&lt;p&gt;Технически примечательно, что знак деления в левой части, в отличие от знака деления в правой части, не является делением. Вместо этого эта запись указывает на то, что оператор &lt;strong&gt;\(  \frac{d}{dx} \)&lt;/strong&gt; применяется к функции &lt;strong&gt;f&lt;/strong&gt; и возвращает другую функцию (производную). Можно представить, что приведённое выше выражение означает, что &lt;strong&gt;h&lt;/strong&gt; очень мало, а значит функция хорошо аппроксимируется прямой линией, а производная — это её наклон. Другими словами, производная от каждой переменной показывает чувствительность всего выражения к её значению. Например, если &lt;strong&gt;x=4,y=−3&lt;/strong&gt; тогда &lt;strong&gt;f(x,y)=−12&lt;/strong&gt; и производная от &lt;strong&gt;\(x\) \(\frac{\partial f}{\partial x} = -3\)3&lt;/strong&gt;. Это говорит нам о том, что если мы увеличим значение этой переменной на небольшую величину, то всё выражение уменьшится (из-за отрицательного знака) в три раза. Это можно увидеть, если переставить слагаемые в приведённом выше уравнении ( &lt;strong&gt;\( f(x + h) = f(x) + h \frac{df(x)}{dx} \)&lt;/strong&gt; ). Аналогично, поскольку &lt;strong&gt;\(\frac{\partial f}{\partial y} = 4\)&lt;/strong&gt;, мы ожидаем, что увеличение стоимости &lt;strong&gt;y&lt;/strong&gt; на какую-то очень небольшую сумму &lt;strong&gt;h&lt;/strong&gt; также увеличит результат функции (из-за положительного знака) и &lt;strong&gt;4h&lt;/strong&gt;.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Производная по каждой переменной показывает, насколько чувствительно всё выражение к изменению её значения.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Как уже упоминалось, градиент &lt;strong&gt;∇f&lt;/strong&gt; является вектором частных производных, поэтому мы имеем, что &lt;strong&gt;\(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]\)&lt;/strong&gt;. Несмотря на то, что градиент технически является вектором, для простоты мы часто используем такие термины, как &lt;em&gt;«градиент по x»&lt;/em&gt;, вместо технически корректного выражения &lt;em&gt;«частная производная по x»&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;Мы также можем вывести производные для операции сложения:  &lt;/p&gt;
&lt;p&gt;$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$  &lt;/p&gt;
&lt;p&gt;то есть производная по обоим &lt;strong&gt;x,y&lt;/strong&gt; является единым, независимо от того, какими значения &lt;strong&gt;x,y&lt;/strong&gt; являются. Это имеет смысл, поскольку увеличение &lt;strong&gt;x или y&lt;/strong&gt; увеличило бы выпуск продукции &lt;strong&gt;f&lt;/strong&gt;. И скорость этого увеличения будет зависеть от фактических значений &lt;strong&gt;x,y&lt;/strong&gt; (в отличие от случая с умножением выше). Последняя функция, которую мы будем часто использовать в этом классе, — это операция &lt;em&gt;max&lt;/em&gt;:  &lt;/p&gt;
&lt;p&gt;$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x &amp;gt;= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y &amp;gt;= x)
$$  &lt;/p&gt;
&lt;p&gt;То есть (суб)градиент равен &lt;strong&gt;1&lt;/strong&gt; для большего входного значения и &lt;strong&gt;0&lt;/strong&gt; для другого входного значения. Интуитивно понятно, что если входные значения &lt;strong&gt;x=4,y=2&lt;/strong&gt;тогда максимальное значение равно &lt;strong&gt;4&lt;/strong&gt;, и функция не чувствительна к настройке &lt;strong&gt;y&lt;/strong&gt;. То есть, если бы мы увеличили его на крошечную величину &lt;strong&gt;h&lt;/strong&gt; функция бы продолжила выводить &lt;strong&gt;4&lt;/strong&gt;, и поэтому градиент равен нулю: эффекта нет. Конечно, если бы мы изменили &lt;strong&gt;y&lt;/strong&gt; на большую величину (например, больше &lt;strong&gt;2&lt;/strong&gt;), то значение &lt;strong&gt;f&lt;/strong&gt; изменилось бы, но производные ничего не говорят нам о влиянии таких больших изменений на входные данные функции. Они информативны только для крошечных, бесконечно малых изменений входных данных, как показано на &lt;strong&gt;\(\lim_{h \rightarrow 0}\)&lt;/strong&gt; в его определении.  &lt;/p&gt;
&lt;h2&gt;Составные выражения с правилом цепочки&lt;/h2&gt;
&lt;p&gt;Теперь давайте рассмотрим более сложные выражения, включающие несколько составных функций, например &lt;strong&gt;f(x,y,z)=(x+y)z&lt;/strong&gt;. Это выражение по-прежнему достаточно простое, чтобы дифференцировать его напрямую, но мы подойдём к нему с особой стороны, которая поможет понять принцип обратного распространения ошибки. В частности, обратите внимание, что это выражение можно разбить на два: &lt;strong&gt;q=x+y&lt;/strong&gt; и &lt;strong&gt;f=qz&lt;/strong&gt;. Более того, мы знаем, как вычислить производные обоих выражений по отдельности, как показано в предыдущем разделе. &lt;strong&gt;f&lt;/strong&gt; это просто умножение &lt;strong&gt;q&lt;/strong&gt; и &lt;strong&gt;z&lt;/strong&gt;, так что &lt;strong&gt;\(\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q\)&lt;/strong&gt;, и &lt;strong&gt;q&lt;/strong&gt; является добавлением &lt;strong&gt;x&lt;/strong&gt; и &lt;strong&gt;y&lt;/strong&gt;, итак &lt;strong&gt;\( \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1 \)&lt;/strong&gt;. Однако нам необязательно знать градиент для промежуточного значения &lt;strong&gt;q&lt;/strong&gt; - ценность &lt;strong&gt;\(\frac{\partial f}{\partial q}\)&lt;/strong&gt; нивелируется. Вместо этого нас, в конечном счете, интересует градиент &lt;strong&gt;f&lt;/strong&gt; в отношении его вклада &lt;strong&gt;x,y,z&lt;/strong&gt;. &lt;strong&gt;Правило цепочки&lt;/strong&gt; говорит нам о том, что правильный способ «объединить» эти выражения градиента в цепочку — это умножение. Например, &lt;strong&gt;\(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x} \)&lt;/strong&gt;. На практике это просто умножение двух чисел, обозначающих два градиента. Давайте рассмотрим это на примере:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; set some inputs
x = -2; y = 5; z = -4

&lt;span class="gh"&gt;#&lt;/span&gt; perform the forward pass
q = x + y # q becomes 3
f = q &lt;span class="gs"&gt;* z # f becomes -12&lt;/span&gt;

&lt;span class="gs"&gt;# perform the backward pass (backpropagation) in reverse order:&lt;/span&gt;
&lt;span class="gs"&gt;# first backprop through f = q *&lt;/span&gt; z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
dqdx = 1.0
dqdy = 1.0
&lt;span class="gh"&gt;#&lt;/span&gt; now backprop through q = x + y
dfdx = dfdq &lt;span class="gs"&gt;* dqdx  # The multiplication here is the chain rule!&lt;/span&gt;
&lt;span class="gs"&gt;dfdy = dfdq *&lt;/span&gt; dqdy  
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;У нас остаётся градиент в переменных &lt;code&gt;[dfdx,dfdy,dfdz]&lt;/code&gt;, который показывает чувствительность переменных &lt;code&gt;x,y,z&lt;/code&gt; к &lt;code&gt;f&lt;/code&gt;!. Это самый простой пример обратного распространения ошибки. Далее мы будем использовать более лаконичную запись, в которой отсутствует префикс &lt;code&gt;df&lt;/code&gt;! Например, мы будем просто писать &lt;code&gt;dq&lt;/code&gt; вместо &lt;code&gt;dfdq&lt;/code&gt; и всегда предполагать, что градиент вычисляется для конечного результата.  &lt;/p&gt;
&lt;p&gt;Это вычисление также можно хорошо визуализировать с помощью принципиальной схемы:  &lt;/p&gt;
&lt;hr&gt;
&lt;div class="fig figleft fighighlight"&gt;
&lt;svg style="max-width: 420px" viewbox="0 0 420 220"&gt;&lt;defs&gt;&lt;marker id="arrowhead" refx="6" refy="2" markerwidth="6" markerheight="4" orient="auto"&gt;&lt;path d="M 0,0 V 4 L6,2 Z"&gt;&lt;/path&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;line x1="40" y1="30" x2="110" y2="30" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="45" y="24" font-size="16" fill="green"&gt;-2&lt;/text&gt;&lt;text x="45" y="47" font-size="16" fill="red"&gt;-4&lt;/text&gt;&lt;text x="35" y="24" font-size="16" text-anchor="end" fill="black"&gt;x&lt;/text&gt;&lt;line x1="40" y1="100" x2="110" y2="100" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="45" y="94" font-size="16" fill="green"&gt;5&lt;/text&gt;&lt;text x="45" y="117" font-size="16" fill="red"&gt;-4&lt;/text&gt;&lt;text x="35" y="94" font-size="16" text-anchor="end" fill="black"&gt;y&lt;/text&gt;&lt;line x1="40" y1="170" x2="110" y2="170" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="45" y="164" font-size="16" fill="green"&gt;-4&lt;/text&gt;&lt;text x="45" y="187" font-size="16" fill="red"&gt;3&lt;/text&gt;&lt;text x="35" y="164" font-size="16" text-anchor="end" fill="black"&gt;z&lt;/text&gt;&lt;line x1="210" y1="65" x2="280" y2="65" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="215" y="59" font-size="16" fill="green"&gt;3&lt;/text&gt;&lt;text x="215" y="82" font-size="16" fill="red"&gt;-4&lt;/text&gt;&lt;text x="205" y="59" font-size="16" text-anchor="end" fill="black"&gt;q&lt;/text&gt;&lt;circle cx="170" cy="65" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="170" y="70" font-size="20" fill="black" text-anchor="middle"&gt;+&lt;/text&gt;&lt;line x1="110" y1="30" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="110" y1="100" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="190" y1="65" x2="210" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="380" y1="117" x2="450" y2="117" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="385" y="111" font-size="16" fill="green"&gt;-12&lt;/text&gt;&lt;text x="385" y="134" font-size="16" fill="red"&gt;1&lt;/text&gt;&lt;text x="375" y="111" font-size="16" text-anchor="end" fill="black"&gt;f&lt;/text&gt;&lt;circle cx="340" cy="117" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="340" y="127" font-size="20" fill="black" text-anchor="middle"&gt;*&lt;/text&gt;&lt;line x1="280" y1="65" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="110" y1="170" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="360" y1="117" x2="380" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;/svg&gt;

&lt;div class="figcaption"&gt;
&lt;/div&gt;
&lt;div style="clear:both;"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a name="intuitive"&gt;&lt;/a&gt; 
Реальная &lt;em&gt;"схема"&lt;/em&gt; слева показывает визуальное представление вычислений. &lt;strong&gt;Прямой проход&lt;/strong&gt; вычисляет значения от входных данных до выходных (показано зелёным цветом).&lt;strong&gt;Обратный проход&lt;/strong&gt;, затем выполняет обратное распространение ошибки, которое начинается с конца и рекурсивно применяет правило цепочки для вычисления градиентов (показано красным цветом) вплоть до входных данных схемы. Градиенты можно представить как текущие в обратном направлении по схеме.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Интуитивное понимание обратного распространения&lt;/h2&gt;
&lt;p&gt;Обратите внимание, что обратное распространение ошибки — это локальный процесс. Каждый элемент в схеме получает входные данные и может сразу вычислить две вещи:
 1. выходное значение 
 2. &lt;em&gt;локальный&lt;/em&gt; градиент выходного значения по отношению к входным данным.  &lt;/p&gt;
&lt;p&gt;Обратите внимание, что элементы могут делать это совершенно независимо, не зная ни о каких деталях всей схемы, в которую они встроены. Однако после завершения прямого прохода во время обратного распространения ошибки элемент в конечном итоге узнает о градиенте выходного значения по отношению к конечному результату всей схемы. Правило цепочки гласит, что функция должна взять этот градиент и умножить его на каждый градиент, который она обычно вычисляет для всех своих входных данных.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Это дополнительное умножение (для каждого входа) благодаря правилу цепочки может превратить один относительно бесполезный элемент в деталь сложной схемы, такой как целая нейронная сеть.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Давайте разберёмся, как это работает, на примере. Сложение получило на вход &lt;strong&gt;[-2, 5]&lt;/strong&gt; и выдало на выходе &lt;strong&gt;3&lt;/strong&gt;. Поскольку сложение вычисляет операцию сложения, его локальный градиент для обоих входных значений равен &lt;strong&gt;+1&lt;/strong&gt;. Остальная часть схемы вычислила итоговое значение, равное &lt;strong&gt;-12&lt;/strong&gt;. Во время обратного прохода, при котором правило цепочки рекурсивно применяется в обратном направлении, сложение (которое является входом для умножения) узнаёт, что градиент его выходного значения равен &lt;strong&gt;-4&lt;/strong&gt;. Если мы представим, что схема &lt;em&gt;«хочет»&lt;/em&gt; вывести более высокое значение (что может помочь с интуитивным пониманием), то мы можем представить, что схема &lt;em&gt;«хочет»&lt;/em&gt;, чтобы выходное значение логического элемента &lt;strong&gt;«и»&lt;/strong&gt; было ниже (из-за отрицательного знака) и с &lt;em&gt;силой&lt;/em&gt; &lt;strong&gt;4&lt;/strong&gt;. Чтобы продолжить рекурсию и вычислить градиент, логический элемент &lt;strong&gt;«и»&lt;/strong&gt; берёт этот градиент и умножает его на все локальные градиенты для своих входов (делая градиент для &lt;strong&gt;x&lt;/strong&gt; и &lt;strong&gt;y&lt;/strong&gt; равным &lt;strong&gt;1 * -4 = -4&lt;/strong&gt;). Обратите внимание, что это даёт желаемый эффект: если &lt;strong&gt;x, y&lt;/strong&gt; уменьшатся (в соответствии с их отрицательным градиентом), то выходное значение сумматора уменьшится, что, в свою очередь, приведёт к увеличению выходного значения умножителя.  &lt;/p&gt;
&lt;p&gt;Таким образом, обратное распространение ошибки можно представить как взаимодействие элементов (через сигнал градиента), которые сообщают друг другу, хотят ли они, чтобы их выходные данные увеличивались или уменьшались (и насколько сильно), чтобы итоговое значение было выше.  &lt;/p&gt;
&lt;h2&gt;Модульность: Пример сигмовидной активации&lt;/h2&gt;
&lt;p&gt;Введённые нами выше элементы являются относительно произвольными. В качестве элемента может выступать любая дифференцируемая функция, и мы можем объединять несколько элементов в один или разбивать функцию на несколько элементов, когда это удобно. Давайте рассмотрим другое выражение, которое иллюстрирует этот момент:  &lt;/p&gt;
&lt;p&gt;$$
f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
$$  &lt;/p&gt;
&lt;p&gt;как мы увидим позже на занятии, это выражение описывает двумерный нейрон (с входными данными &lt;strong&gt;x&lt;/strong&gt; и весами &lt;strong&gt;w&lt;/strong&gt;), который использует функцию &lt;em&gt;сигмоидальной активации&lt;/em&gt;. Но пока давайте рассматривать это очень просто как функцию, которая преобразует входные данные &lt;em&gt;w,x&lt;/em&gt; в одно число. Функция состоит из нескольких логических элементов. Помимо тех, что описаны выше (сложение, умножение, максимальное значение), есть ещё четыре:  &lt;/p&gt;
&lt;p&gt;$$
f(x) = \frac{1}{x} 
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = -1/x^2 
\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = 1 
\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = e^x
\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = a
$$  &lt;/p&gt;
&lt;p&gt;Где функции &lt;strong&gt;\(f_c, f_a\)&lt;/strong&gt; преобразуйте входные данные в константу, равную &lt;strong&gt;c&lt;/strong&gt; и масштабируйте входные данные на константу, равную &lt;strong&gt;a&lt;/strong&gt;, соответственно. Технически это частные случаи сложения и умножения, но мы вводим их как (новые) унарные операции, поскольку нам не нужны градиенты для констант &lt;strong&gt;c,a&lt;/strong&gt;. Тогда полная схема выглядит следующим образом:  &lt;/p&gt;
&lt;div class="fig figleft fighighlight"&gt;
&lt;svg style="max-width: 799px" viewbox="0 0 799 306"&gt;&lt;g transform="scale(0.8)"&gt;&lt;defs&gt;&lt;marker id="arrowhead" refx="6" refy="2" markerwidth="6" markerheight="4" orient="auto"&gt;&lt;path d="M 0,0 V 4 L6,2 Z"&gt;&lt;/path&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;line x1="50" y1="30" x2="90" y2="30" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="24" font-size="16" fill="green"&gt;2.00&lt;/text&gt;&lt;text x="55" y="47" font-size="16" fill="red"&gt;-0.20&lt;/text&gt;&lt;text x="45" y="24" font-size="16" text-anchor="end" fill="black"&gt;w0&lt;/text&gt;&lt;line x1="50" y1="100" x2="90" y2="100" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="94" font-size="16" fill="green"&gt;-1.00&lt;/text&gt;&lt;text x="55" y="117" font-size="16" fill="red"&gt;0.39&lt;/text&gt;&lt;text x="45" y="94" font-size="16" text-anchor="end" fill="black"&gt;x0&lt;/text&gt;&lt;line x1="50" y1="170" x2="90" y2="170" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="164" font-size="16" fill="green"&gt;-3.00&lt;/text&gt;&lt;text x="55" y="187" font-size="16" fill="red"&gt;-0.39&lt;/text&gt;&lt;text x="45" y="164" font-size="16" text-anchor="end" fill="black"&gt;w1&lt;/text&gt;&lt;line x1="50" y1="240" x2="90" y2="240" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="234" font-size="16" fill="green"&gt;-2.00&lt;/text&gt;&lt;text x="55" y="257" font-size="16" fill="red"&gt;-0.59&lt;/text&gt;&lt;text x="45" y="234" font-size="16" text-anchor="end" fill="black"&gt;x1&lt;/text&gt;&lt;line x1="50" y1="310" x2="90" y2="310" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="304" font-size="16" fill="green"&gt;-3.00&lt;/text&gt;&lt;text x="55" y="327" font-size="16" fill="red"&gt;0.20&lt;/text&gt;&lt;text x="45" y="304" font-size="16" text-anchor="end" fill="black"&gt;w2&lt;/text&gt;&lt;line x1="170" y1="65" x2="210" y2="65" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="175" y="59" font-size="16" fill="green"&gt;-2.00&lt;/text&gt;&lt;text x="175" y="82" font-size="16" fill="red"&gt;0.20&lt;/text&gt;&lt;circle cx="130" cy="65" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="130" y="75" font-size="20" fill="black" text-anchor="middle"&gt;*&lt;/text&gt;&lt;line x1="90" y1="30" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="90" y1="100" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="150" y1="65" x2="170" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="170" y1="205" x2="210" y2="205" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="175" y="199" font-size="16" fill="green"&gt;6.00&lt;/text&gt;&lt;text x="175" y="222" font-size="16" fill="red"&gt;0.20&lt;/text&gt;&lt;circle cx="130" cy="205" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="130" y="215" font-size="20" fill="black" text-anchor="middle"&gt;*&lt;/text&gt;&lt;line x1="90" y1="170" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="90" y1="240" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="150" y1="205" x2="170" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="290" y1="135" x2="330" y2="135" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="295" y="129" font-size="16" fill="green"&gt;4.00&lt;/text&gt;&lt;text x="295" y="152" font-size="16" fill="red"&gt;0.20&lt;/text&gt;&lt;circle cx="250" cy="135" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="250" y="140" font-size="20" fill="black" text-anchor="middle"&gt;+&lt;/text&gt;&lt;line x1="210" y1="65" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="210" y1="205" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="270" y1="135" x2="290" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="410" y1="222" x2="450" y2="222" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="415" y="216" font-size="16" fill="green"&gt;1.00&lt;/text&gt;&lt;text x="415" y="239" font-size="16" fill="red"&gt;0.20&lt;/text&gt;&lt;circle cx="370" cy="222" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="370" y="227" font-size="20" fill="black" text-anchor="middle"&gt;+&lt;/text&gt;&lt;line x1="330" y1="135" x2="350" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="90" y1="310" x2="350" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="390" y1="222" x2="410" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="530" y1="222" x2="570" y2="222" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="535" y="216" font-size="16" fill="green"&gt;-1.00&lt;/text&gt;&lt;text x="535" y="239" font-size="16" fill="red"&gt;-0.20&lt;/text&gt;&lt;circle cx="490" cy="222" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="490" y="227" font-size="20" fill="black" text-anchor="middle"&gt;*-1&lt;/text&gt;&lt;line x1="450" y1="222" x2="470" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="510" y1="222" x2="530" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="650" y1="222" x2="690" y2="222" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="655" y="216" font-size="16" fill="green"&gt;0.37&lt;/text&gt;&lt;text x="655" y="239" font-size="16" fill="red"&gt;-0.53&lt;/text&gt;&lt;circle cx="610" cy="222" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="610" y="227" font-size="20" fill="black" text-anchor="middle"&gt;exp&lt;/text&gt;&lt;line x1="570" y1="222" x2="590" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="630" y1="222" x2="650" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="770" y1="222" x2="810" y2="222" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="775" y="216" font-size="16" fill="green"&gt;1.37&lt;/text&gt;&lt;text x="775" y="239" font-size="16" fill="red"&gt;-0.53&lt;/text&gt;&lt;circle cx="730" cy="222" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="730" y="227" font-size="20" fill="black" text-anchor="middle"&gt;+1&lt;/text&gt;&lt;line x1="690" y1="222" x2="710" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="750" y1="222" x2="770" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="890" y1="222" x2="930" y2="222" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="895" y="216" font-size="16" fill="green"&gt;0.73&lt;/text&gt;&lt;text x="895" y="239" font-size="16" fill="red"&gt;1.00&lt;/text&gt;&lt;circle cx="850" cy="222" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="850" y="227" font-size="20" fill="black" text-anchor="middle"&gt;1/x&lt;/text&gt;&lt;line x1="810" y1="222" x2="830" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="870" y1="222" x2="890" y2="222" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;div class="figcaption"&gt;
  Пример схемы для двумерного нейрона с сигмоидальной функцией активации. Входные данные — [x0, x1], а (обучаемые) весовые коэффициенты нейрона — [w0, w1, w2]. Как мы увидим позже, нейрон вычисляет скалярное произведение входных данных, а затем его активация мягко сжимается сигмоидальной функцией до диапазона от 0 до 1.
&lt;/div&gt;
&lt;div style="clear:both;"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;В приведённом выше примере мы видим длинную цепочку вызовов функций, которые работают с результатом скалярного произведения &lt;strong&gt;w,x.&lt;/strong&gt; Функция, которую реализуют эти операции, называется &lt;em&gt;сигмоидальной функцией&lt;/em&gt; &lt;strong&gt;σ(x)&lt;/strong&gt;. Оказывается, производная сигмоидальной функции по входным данным упрощается, если выполнить дифференцирование (после забавной сложной части, где мы добавляем и вычитаем &lt;strong&gt;1&lt;/strong&gt; в числителе):  &lt;/p&gt;
&lt;p&gt;$$
\sigma(x) = \frac{1}{1+e^{-x}} \\
\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) 
= \left( 1 - \sigma(x) \right) \sigma(x)
$$  &lt;/p&gt;
&lt;p&gt;Как мы видим, градиент упрощается и становится на удивление простым. Например, сигмоидальное выражение получает на вход &lt;strong&gt;1,0&lt;/strong&gt; и вычисляет на выходе &lt;strong&gt;0,73&lt;/strong&gt; во время прямого прохода. Приведённый выше вывод показывает, что &lt;em&gt;локальный градиент&lt;/em&gt; будет &lt;strong&gt;равен (1 — 0,73) * 0,73 ~= 0,2&lt;/strong&gt;, как и в случае с предыдущей схемой (см. изображение выше), за исключением того, что в этом случае это будет сделано с помощью одного простого и эффективного выражения (и с меньшим количеством численных проблем). Таким образом, в любом реальном практическом применении было бы очень полезно объединить эти операции в один элемент управления. Давайте рассмотрим обратное распространение ошибки для этого нейрона в коде:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# assume some random weights and data&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# forward pass&lt;/span&gt;
&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# sigmoid function&lt;/span&gt;

&lt;span class="c1"&gt;# backward pass through the neuron (backpropagation)&lt;/span&gt;
&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# gradient on dot variable, using the sigmoid gradient derivation&lt;/span&gt;
&lt;span class="n"&gt;dx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# backprop into x&lt;/span&gt;
&lt;span class="n"&gt;dw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ddot&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# backprop into w&lt;/span&gt;
&lt;span class="c1"&gt;# we're done! we have the gradients on the inputs to the circuit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Совет по реализации: поэтапное обратное распространение ошибки&lt;/strong&gt;. Как показано в приведенном выше коде, на практике всегда полезно разбивать прямой проход на этапы, которые легко поддаются обратному распространению ошибки. Например, здесь мы создали промежуточную переменную &lt;code&gt;dot&lt;/code&gt;, которая содержит результат скалярного произведения &lt;code&gt;w&lt;/code&gt; и &lt;code&gt;x&lt;/code&gt;. Затем во время обратного прохода мы последовательно вычисляем (в обратном порядке) соответствующие переменные (например, &lt;code&gt;ddot&lt;/code&gt; и, в конечном итоге, &lt;code&gt;dw, dx&lt;/code&gt;), которые содержат градиенты этих переменных.  &lt;/p&gt;
&lt;p&gt;Смысл этого раздела в том, что детали того, как выполняется обратное распространение ошибки и какие части прямой функции мы считаем логическими элементами, — это вопрос удобства. Полезно знать, какие части выражения имеют простые локальные градиенты, чтобы их можно было объединить в цепочку с наименьшим количеством кода и усилий.  &lt;/p&gt;
&lt;h2&gt;Бэкпроп на практике: Поэтапное вычисление&lt;/h2&gt;
&lt;p&gt;Давайте рассмотрим это на другом примере. Предположим, что у нас есть функция следующего вида:  &lt;/p&gt;
&lt;p&gt;$$
f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}
$$  &lt;/p&gt;
&lt;p&gt;Для ясности: эта функция совершенно бесполезна, и неясно, зачем вам вообще понадобилось вычислять её градиент, за исключением того, что это хороший пример обратного распространения ошибки на практике. Очень важно подчеркнуть, что если бы вы начали вычислять производную по любому из &lt;strong&gt;x&lt;/strong&gt; или &lt;strong&gt;y&lt;/strong&gt;, то в результате вы получили бы очень большие и сложные выражения. Однако оказывается, что в этом нет необходимости, потому что нам не нужно записывать явную функцию, которая вычисляет градиент. Нам нужно только знать, как его вычислить. Вот как мы бы структурировали прямой проход для такого выражения:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;x = 3 # example values
y = -4

&lt;span class="gh"&gt;#&lt;/span&gt; forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Фух, к концу выражения мы вычислили прямой проход. Обратите внимание, что мы структурировали код таким образом, что он содержит несколько промежуточных переменных, каждая из которых представляет собой простое выражение, для которого мы уже знаем локальные градиенты. Поэтому вычислить обратный путь легко: мы пойдём в обратном направлении, и для каждой переменной на пути прямого прохода (&lt;code&gt;sigy, num, sigx, xpy, xpysqr, den, invden&lt;/code&gt;) у нас будет та же переменная, но начинающаяся с &lt;code&gt;d&lt;/code&gt;, которая будет содержать градиент выходного сигнала схемы по отношению к этой переменной. Кроме того, обратите внимание, что каждый элемент в нашем обратном распространении ошибки будет включать вычисление локального градиента этого выражения и объединение его с градиентом этого выражения путём умножения. Для каждой строки мы также указываем, к какой части прямого прохода она относится:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
&lt;span class="gh"&gt;#&lt;/span&gt; backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) &lt;span class="gs"&gt;* dinvden                                #(7)&lt;/span&gt;
&lt;span class="gs"&gt;# backprop den = sigx + xpysqr&lt;/span&gt;
&lt;span class="gs"&gt;dsigx = (1) *&lt;/span&gt; dden                                                #(6)
dxpysqr = (1) &lt;span class="gs"&gt;* dden                                              #(6)&lt;/span&gt;
&lt;span class="gs"&gt;# backprop xpysqr = xpy*&lt;/span&gt;*2
dxpy = (2 &lt;span class="gs"&gt;* xpy) *&lt;/span&gt; dxpysqr                                        #(5)
&lt;span class="gh"&gt;#&lt;/span&gt; backprop xpy = x + y
dx = (1) &lt;span class="gs"&gt;* dxpy                                                   #(4)&lt;/span&gt;
&lt;span class="gs"&gt;dy = (1) *&lt;/span&gt; dxpy                                                   #(4)
&lt;span class="gh"&gt;#&lt;/span&gt; backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) &lt;span class="gs"&gt;* sigx) *&lt;/span&gt; dsigx # Notice += !! See notes below  #(3)
&lt;span class="gh"&gt;#&lt;/span&gt; backprop num = x + sigy
dx += (1) &lt;span class="gs"&gt;* dnum                                                  #(2)&lt;/span&gt;
&lt;span class="gs"&gt;dsigy = (1) *&lt;/span&gt; dnum                                                #(2)
&lt;span class="gh"&gt;#&lt;/span&gt; backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) &lt;span class="gs"&gt;* sigy) *&lt;/span&gt; dsigy                                 #(1)
&lt;span class="gh"&gt;#&lt;/span&gt; done! phew
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Обратите внимание на несколько вещей:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Кэшируйте переменные прямого пути&lt;/strong&gt;. Для вычисления обратного пути очень полезно иметь некоторые переменные, которые использовались при прямом пути. На практике вы хотите структурировать свой код таким образом, чтобы кэшировать эти переменные и чтобы они были доступны во время обратного распространения. Если это слишком сложно, можно (но нерационально) пересчитать их.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Градиенты суммируются в точках разветвления&lt;/strong&gt;. В прямом выражении переменные &lt;strong&gt;x, y&lt;/strong&gt; встречаются несколько раз, поэтому при обратном распространении ошибки мы должны быть внимательны и использовать &lt;code&gt;+=&lt;/code&gt; вместо &lt;code&gt;=&lt;/code&gt; для накопления градиента по этим переменным (иначе мы перезапишем его). Это соответствует &lt;code&gt;правилу дифференцирования сложной функции&lt;/code&gt; в математическом анализе, которое гласит, что если переменная разветвляется на разные части схемы, то градиенты, которые возвращаются к ней, суммируются.  &lt;/p&gt;
&lt;h2&gt;Закономерности в обратном потоке&lt;/h2&gt;
&lt;p&gt;Интересно отметить, что во многих случаях обратный градиент можно интерпретировать интуитивно. Например, три наиболее часто используемых элемента в нейронных сетях (&lt;em&gt;сложение, умножение, максимальное значение&lt;/em&gt;) имеют очень простую интерпретацию с точки зрения того, как они действуют во время обратного распространения ошибки. Рассмотрим этот пример схемы:  &lt;/p&gt;
&lt;hr&gt;
&lt;div class="fig figleft fighighlight"&gt;
&lt;svg style="max-width: 460px" viewbox="0 0 460 290"&gt;&lt;g transform="scale(1)"&gt;&lt;defs&gt;&lt;marker id="arrowhead" refx="6" refy="2" markerwidth="6" markerheight="4" orient="auto"&gt;&lt;path d="M 0,0 V 4 L6,2 Z"&gt;&lt;/path&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;line x1="50" y1="30" x2="90" y2="30" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="24" font-size="16" fill="green"&gt;3.00&lt;/text&gt;&lt;text x="55" y="47" font-size="16" fill="red"&gt;-8.00&lt;/text&gt;&lt;text x="45" y="24" font-size="16" text-anchor="end" fill="black"&gt;x&lt;/text&gt;&lt;line x1="50" y1="100" x2="90" y2="100" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="94" font-size="16" fill="green"&gt;-4.00&lt;/text&gt;&lt;text x="55" y="117" font-size="16" fill="red"&gt;6.00&lt;/text&gt;&lt;text x="45" y="94" font-size="16" text-anchor="end" fill="black"&gt;y&lt;/text&gt;&lt;line x1="50" y1="170" x2="90" y2="170" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="164" font-size="16" fill="green"&gt;2.00&lt;/text&gt;&lt;text x="55" y="187" font-size="16" fill="red"&gt;2.00&lt;/text&gt;&lt;text x="45" y="164" font-size="16" text-anchor="end" fill="black"&gt;z&lt;/text&gt;&lt;line x1="50" y1="240" x2="90" y2="240" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="55" y="234" font-size="16" fill="green"&gt;-1.00&lt;/text&gt;&lt;text x="55" y="257" font-size="16" fill="red"&gt;0.00&lt;/text&gt;&lt;text x="45" y="234" font-size="16" text-anchor="end" fill="black"&gt;w&lt;/text&gt;&lt;line x1="170" y1="65" x2="210" y2="65" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="175" y="59" font-size="16" fill="green"&gt;-12.00&lt;/text&gt;&lt;text x="175" y="82" font-size="16" fill="red"&gt;2.00&lt;/text&gt;&lt;circle cx="130" cy="65" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="130" y="75" font-size="20" fill="black" text-anchor="middle"&gt;*&lt;/text&gt;&lt;line x1="90" y1="30" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="90" y1="100" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="150" y1="65" x2="170" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="170" y1="205" x2="210" y2="205" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="175" y="199" font-size="16" fill="green"&gt;2.00&lt;/text&gt;&lt;text x="175" y="222" font-size="16" fill="red"&gt;2.00&lt;/text&gt;&lt;circle cx="130" cy="205" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="130" y="210" font-size="20" fill="black" text-anchor="middle"&gt;max&lt;/text&gt;&lt;line x1="90" y1="170" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="90" y1="240" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="150" y1="205" x2="170" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="290" y1="135" x2="330" y2="135" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="295" y="129" font-size="16" fill="green"&gt;-10.00&lt;/text&gt;&lt;text x="295" y="152" font-size="16" fill="red"&gt;2.00&lt;/text&gt;&lt;circle cx="250" cy="135" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="250" y="140" font-size="20" fill="black" text-anchor="middle"&gt;+&lt;/text&gt;&lt;line x1="210" y1="65" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="210" y1="205" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="270" y1="135" x2="290" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="410" y1="135" x2="450" y2="135" stroke="black" stroke-width="1"&gt;&lt;/line&gt;&lt;text x="415" y="129" font-size="16" fill="green"&gt;-20.00&lt;/text&gt;&lt;text x="415" y="152" font-size="16" fill="red"&gt;1.00&lt;/text&gt;&lt;circle cx="370" cy="135" fill="white" stroke="black" stroke-width="1" r="20"&gt;&lt;/circle&gt;&lt;text x="370" y="140" font-size="20" fill="black" text-anchor="middle"&gt;*2&lt;/text&gt;&lt;line x1="330" y1="135" x2="350" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;line x1="390" y1="135" x2="410" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"&gt;&lt;/line&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;div class="figcaption"&gt;
  Пример схемы, демонстрирующей интуитивное понимание операций, которые выполняет обратное распространение ошибки во время обратного прохода для вычисления градиентов по входным данным. Операция суммирования равномерно распределяет градиенты по всем своим входам. Операция максимального значения направляет градиент на вход с наибольшим значением. Операция умножения принимает входные значения, меняет их местами и умножает на градиент.
&lt;/div&gt;
&lt;div style="clear:both;"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Рассматривая приведенную выше диаграмму в качестве примера, мы можем видеть, что:  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Сложение&lt;/strong&gt; всегда берёт градиент на выходе и распределяет его поровну между всеми входами, независимо от того, какими были их значения во время прямого прохода. Это следует из того, что локальный градиент для операции сложения равен &lt;strong&gt;+1,0&lt;/strong&gt;, поэтому градиенты на всех входах будут в точности равны градиентам на выходе, потому что они будут умножены на &lt;strong&gt;x1,0&lt;/strong&gt; (и останутся неизменными). В приведённом выше примере обратите внимание, что сумматор направил градиент&lt;strong&gt; 2,00&lt;/strong&gt; на оба входа, разделив его поровну и оставив без изменений.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Макс-сглаживатель&lt;/strong&gt; направляет градиент. В отличие от сумматора, который распределяет градиент без изменений по всем своим входам, макс-сглаживатель распределяет градиент (без изменений) только по одному из своих входов (по входу, который имел наибольшее значение во время прямого прохода). Это связано с тем, что локальный градиент для макс-сглаживателя равен &lt;strong&gt;1,0&lt;/strong&gt; для наибольшего значения и &lt;strong&gt;0,0&lt;/strong&gt; для всех остальных значений. В приведённом выше примере функция max направила градиент &lt;strong&gt;2,00&lt;/strong&gt; на переменную &lt;strong&gt;z&lt;/strong&gt;, которая имела более высокое значение, чем &lt;strong&gt;w&lt;/strong&gt;, а градиент &lt;strong&gt;w&lt;/strong&gt; остался равным нулю.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Умножитель&lt;/strong&gt; немного сложнее в интерпретации. Его локальные градиенты — это входные значения (кроме переключаемых), которые умножаются на градиент выходного значения в соответствии с правилом цепочки. В приведённом выше примере градиент &lt;strong&gt;x&lt;/strong&gt; равен &lt;strong&gt;-8,00&lt;/strong&gt;, что составляет &lt;strong&gt;-4,00 x 2,00&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Неинтуитивные эффекты и их последствия&lt;/em&gt;. Обратите внимание, что если один из входов умножителя очень мал, а другой очень велик, то умножитель сделает что-то немного неинтуитивное: он присвоит относительно большой градиент малому входу и крошечный градиент большому входу. Обратите внимание, что в линейных классификаторах, где веса умножаются на скалярное произведение, &lt;strong&gt;\(w^Tx_i\)&lt;/strong&gt; (умноженные) на входные данные, это означает, что масштаб данных влияет на величину градиента весовых коэффициентов. Например, если вы умножите все примеры входных данных &lt;strong&gt;\(x_i\)&lt;/strong&gt;. Если во время предварительной обработки умножить на 1000, то градиент по весам будет &lt;strong&gt;в 1000 раз больше&lt;/strong&gt;, и вам придётся уменьшить скорость обучения на этот коэффициент, чтобы компенсировать разницу. &lt;strong&gt;Вот почему предварительная обработка так важна, иногда даже в мелочах!&lt;/strong&gt; Интуитивное понимание того, как распределяются градиенты, может помочь вам отладить некоторые из этих случаев.  &lt;/p&gt;
&lt;h2&gt;Градиенты для векторизованных операций&lt;/h2&gt;
&lt;p&gt;В предыдущих разделах речь шла об отдельных переменных, но все концепции напрямую применимы к матричным и векторным операциям. Однако необходимо уделять больше внимания размерностям и транспонированию.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Градиент при умножении матриц&lt;/strong&gt;. Возможно, самая сложная операция — это умножение матриц (которое обобщает все операции умножения матриц на векторы и векторов на векторы):  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

&lt;span class="gh"&gt;#&lt;/span&gt; now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Совет: используйте анализ измерений!&lt;/em&gt; Обратите внимание, что вам не нужно запоминать выражения для &lt;code&gt;dW&lt;/code&gt; и &lt;code&gt;dX&lt;/code&gt;, поскольку их легко повторно вывести на основе измерений. Например, мы знаем, что градиент весов &lt;code&gt;dW&lt;/code&gt; должен быть того же размера, что и &lt;code&gt;W&lt;/code&gt; после его вычисления, и что он должен зависеть от матричного умножения &lt;code&gt;X&lt;/code&gt; и &lt;code&gt;dD&lt;/code&gt; (как в случае, когда оба &lt;code&gt;X,W&lt;/code&gt; являются одиночными числами, а не матрицами). Всегда есть только один способ достичь этого, чтобы размеры соответствовали друг другу. Например, &lt;code&gt;X&lt;/code&gt; имеет размер &lt;strong&gt;[10 x 3]&lt;/strong&gt;, а &lt;code&gt;dD&lt;/code&gt; — размер &lt;strong&gt;[5 x 3]&lt;/strong&gt;, поэтому, если мы хотим, чтобы &lt;code&gt;dW&lt;/code&gt; и &lt;code&gt;W&lt;/code&gt; имели форму &lt;strong&gt;[5 x 10]&lt;/strong&gt;, то единственный способ добиться этого — использовать &lt;code&gt;dD.dot(X.T)&lt;/code&gt;, как показано выше.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Работайте с небольшими, понятными примерами&lt;/strong&gt;. Некоторым людям поначалу может быть сложно вывести градиентные обновления для некоторых векторизованных выражений. Мы рекомендуем явно записать минимальный векторизованный пример, вывести градиент на бумаге, а затем обобщить шаблон до эффективной векторизованной формы.  &lt;/p&gt;
&lt;p&gt;Эрик Леарнед-Миллер также написал более подробный документ о вычислении матричных/векторных производных, который может оказаться вам полезным. &lt;a href="http://cs231n.stanford.edu/vecDerivs.pdf"&gt;Найдите его здесь&lt;/a&gt;.  &lt;/p&gt;
&lt;h2&gt;Краткая сводка&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Мы интуитивно понимаем, что означают градиенты, как они распространяются по цепи и как они сообщают, какую часть цепи следует увеличить или уменьшить и с какой силой, чтобы повысить конечный результат.&lt;/li&gt;
&lt;li&gt;Мы обсудили важность &lt;strong&gt;поэтапных вычислений&lt;/strong&gt; для практической реализации обратного распространения ошибки. Вы всегда хотите разбить свою функцию на модули, для которых можно легко вычислить локальные градиенты, а затем объединить их с помощью правила дифференцирования сложной функции. Важно отметить, что вы почти никогда не захотите записывать эти выражения на бумаге и дифференцировать их в полной форме, потому что вам никогда не понадобится явное математическое уравнение для градиента входных переменных. Таким образом, разбейте свои выражения на этапы так, чтобы вы могли дифференцировать каждый этап независимо (этапами будут умножение матриц, операции с максимумом, операции с суммой и т. д.), а затем выполняйте обратное распространение по переменным шаг за шагом.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В следующем разделе мы начнём определять нейронные сети, а обратное распространение ошибки позволит нам эффективно вычислять градиент функции потерь по отношению к её параметрам. Другими словами, теперь мы готовы к обучению нейронных сетей, и самая сложная с концептуальной точки зрения часть этого курса осталась позади! До ConvNets останется совсем немного.  &lt;/p&gt;
&lt;h3&gt;Ссылки&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1502.05767"&gt;Автоматическая дифференциация в машинном обучении: опрос&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><guid>https://mldl.ru/posts/backpropagation/</guid><pubDate>Fri, 07 Mar 2025 16:42:16 GMT</pubDate></item><item><title>Оптимизация</title><link>https://mldl.ru/posts/optimization/</link><dc:creator>Андрей Лабинцев</dc:creator><description>&lt;h2&gt;Оптимизация&lt;/h2&gt;
&lt;p&gt;Содержание: 
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Введение&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Визуализация функции потерь&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Оптимизация&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/optimization/"&gt;Стратегия #1: Случайный поиск&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/optimization/"&gt;Стратегия #2: Случайный локальный поиск&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/optimization/"&gt;Стратегия #3: Следование градиенту&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Вычисление градиента&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/optimization/"&gt;Численно с конечными разностями&lt;/a&gt;
    - &lt;a href="https://mldl.ru/posts/optimization/"&gt;Аналитически с помощью исчисления&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Градиентный спуск&lt;/a&gt;
- &lt;a href="https://mldl.ru/posts/optimization/"&gt;Краткая сводка&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Введение #&lt;/h2&gt;
&lt;p&gt;В предыдущем разделе мы представили два ключевых компонента в контексте задачи классификации изображений:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(Параметризованная) &lt;strong&gt;функция оценки&lt;/strong&gt;, сопоставляющая пиксели необработанного изображения с оценками класса (например, линейная функция)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Функция потерь&lt;/strong&gt;, которая измеряет качество определенного набора параметров на основе того, насколько хорошо индуцированные оценки согласуются с метками основной истины в обучающих данных. Мы увидели, что существует множество способов и версий этого (например, Softmax/SVM).  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;В частности, вспомним, что линейная функция имела вид ( f(x_i, W) = W x_i \
 и разработанная нами SVM была сформулирована следующим образом:  &lt;/p&gt;
&lt;p&gt;$$
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)&lt;em y_i&gt;j - f(x_i; W)&lt;/em&gt; + 1) \right] + \alpha R(W)
$$&lt;/p&gt;
&lt;p&gt;Мы увидели, что настройка параметров &lt;strong&gt;\(W\)&lt;/strong&gt;, которые выдавали прогнозы для примера &lt;strong&gt;\(x_i\)&lt;/strong&gt;. В соответствии с их основными истинными метками &lt;strong&gt;\(y_i\)&lt;/strong&gt; также будет иметь очень низкий убыток &lt;strong&gt;L&lt;/strong&gt;. Теперь мы представим третий и последний ключевой компонент: &lt;strong&gt;оптимизацию&lt;/strong&gt;. Оптимизация — это процесс нахождения набора параметров (W), которые минимизируют функцию потерь.&lt;/p&gt;
&lt;p&gt;Предчувствие: Как только мы поймем, как эти три основных компонента взаимодействуют, мы вернемся к первому компоненту (параметризованному отображению функций) и расширим его до функций, гораздо более сложных, чем линейное отображение: сначала целые нейронные сети, а затем сверточные нейронные сети. Функции потерь и процесс оптимизации останутся относительно неизменными.  &lt;/p&gt;
&lt;h2&gt;Визуализация функции потерь #&lt;/h2&gt;
&lt;p&gt;Функции потерь, которые мы рассмотрим в этом классе, обычно определяются в очень больших пространствах (например, в CIFAR-10 матрица весов линейного классификатора имеет размер [10 x 3073] для всего 30 730 параметров), что затрудняет их визуализацию. Тем не менее, мы все еще можем получить некоторые интуитивные представления об единице, разрезая пространство высокой размерности вдоль лучей (1 измерение) или вдоль плоскостей (2 измерения). Например, мы можем сгенерировать случайную матрицу весов &lt;strong&gt;\(W\)&lt;/strong&gt;, (которая соответствует одной точке в пространстве), затем маршировать по лучу и записывать значение функции потерь по пути. То есть мы можем сгенерировать случайное направление &lt;strong&gt;\(W\)&lt;/strong&gt; и рассчитать потери в этом направлении, оценив &lt;strong&gt;\( L(W + a W_1 + b W_2) \)&lt;/strong&gt; для различных значений &lt;strong&gt;\(a\)&lt;/strong&gt;. В результате этого процесса создается простой график со значением &lt;strong&gt;\(a\)&lt;/strong&gt; в качестве оси &lt;strong&gt;(X)&lt;/strong&gt; и значение функции потерь по оси &lt;strong&gt;\(Y\)&lt;/strong&gt;. Мы также можем провести ту же процедуру с двумя измерениями, оценив потери &lt;strong&gt;\( L(W + a W_1 + b W_2) \)&lt;/strong&gt; по мере того, как меняются значения &lt;strong&gt;\(a, b\)&lt;/strong&gt;. На графике &lt;strong&gt;\(a, b\)&lt;/strong&gt; могут соответствовать осям x и y, а значение функции потерь может быть отображено цветом:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/svm1d.png"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/svm_one.jpg"&gt;&lt;br&gt;
&lt;img alt="" src="https://cs231n.github.io/assets/svm_all.jpg"&gt;&lt;br&gt;
Ландшафт функций потерь для многоклассовой SVM (без регуляризации) для одного единственного примера (сверху, посередине) и для сотни примеров (снизу) в CIFAR-10. Сверху: одномерные потери при изменении только a. Посередине, снизу: двумерный срез потерь, &lt;strong&gt;синий = низкие потери, красный = высокие потери&lt;/strong&gt;. Обратите внимание на кусочно-линейную структуру функции потерь. Потери для нескольких примеров сочетаются со средними, поэтому форма чаши снизу является средним значением многих кусочно-линейных чаш (например, та, что посередине).  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Мы можем объяснить кусочно-линейную структуру функции потерь, изучив математические расчеты. В качестве единственного примера мы имеем:  &lt;/p&gt;
&lt;p&gt;$$
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]
$$&lt;/p&gt;
&lt;p&gt;Из уравнения ясно, что потеря данных для каждого примера равна сумме (нулевой порог из-за &lt;strong&gt;\(\max(0,-)\&lt;/strong&gt; функции) линейных функций &lt;strong&gt;\(W\)&lt;/strong&gt;. Более того, каждый ряд &lt;strong&gt;\(W\)&lt;/strong&gt; (т.е. &lt;strong&gt;\(w_j\)&lt;/strong&gt;) иногда имеет перед собой положительный знак (когда он соответствует неправильному классу для примера), а иногда отрицательный знак (когда он соответствует правильному классу для этого примера). Чтобы сделать это более явным, рассмотрим простой набор данных, содержащий три одномерные точки и три класса. Полная потеря SVM (без регуляризации) становится следующей:  &lt;/p&gt;
&lt;p&gt;$$
\begin{align}
L_0 = &amp;amp; \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\
L_1 = &amp;amp; \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\
L_2 = &amp;amp; \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\
L = &amp;amp; (L_0 + L_1 + L_2)/3
\end{align}
$$  &lt;/p&gt;
&lt;p&gt;Поскольку эти примеры являются одномерными, данные &lt;strong&gt;\(x_i\)&lt;/strong&gt; и веса &lt;strong&gt;\(w_j\)&lt;/strong&gt; -  это цифры. Глядя, например, на &lt;strong&gt;\(w_0\)&lt;/strong&gt;, некоторые из приведенных выше членов являются линейными функциями &lt;strong&gt;\(w_0\)&lt;/strong&gt;. И каждая из них зажата в точке ноль. Мы можем визуализировать это следующим образом:  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/svmbowl.png"&gt;&lt;br&gt;
&lt;strong&gt;1&lt;/strong&gt;-мерная иллюстрация потери данных:&lt;br&gt;
&lt;strong&gt;Ось x&lt;/strong&gt; - это один груз&lt;br&gt;
&lt;strong&gt;ось y&lt;/strong&gt; — потери.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;В качестве отступления, вы, возможно, догадались по ее чашеобразному виду, что функция стоимости SVM является примером &lt;a href="http://en.wikipedia.org/wiki/Convex_function"&gt;выпуклой функции&lt;/a&gt;. Существует большое количество литературы, посвященной эффективной минимизации этих типов функций, и вы также можете пройти курс Стэнфорда по этой теме (&lt;a href="http://stanford.edu/~boyd/cvxbook/"&gt;выпуклая оптимизация&lt;/a&gt;). Как только мы расширим наши функции оценки &lt;strong&gt;f&lt;/strong&gt; для нейронных сетей наши целевые функции станут невыпуклыми, и на приведенных выше визуализациях будут отображаться не чаши, а сложные, ухабистые местности.  &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Недифференцируемые функции потерь&lt;/em&gt;. В качестве технического примечания вы также можете видеть, что изломы в функции потерь (из-за максимальной операции) технически делают функцию потерь недифференцируемой, потому что при этих изломах градиент не определен. Тем не менее, субградиент все еще существует и обычно используется вместо него. В этом классе термины «субградиент» и «градиент» будут использоваться как взаимозаменяемые.  &lt;/p&gt;
&lt;p&gt;# Оптимизация  &lt;/p&gt;
&lt;p&gt;Повторимся, что функция потерь позволяет нам количественно оценить качество любого конкретного набора весов &lt;strong&gt;W&lt;/strong&gt;. Цель оптимизации — найти &lt;strong&gt;W&lt;/strong&gt;, которое минимизирует функцию потерь. Теперь мы будем мотивировать и постепенно развивать подход к оптимизации функции потерь. Для тех из вас, кто приходит на этот курс с предыдущим опытом, этот раздел может показаться странным, поскольку рабочий пример, который мы будем использовать (потери SVM), является выпуклой задачей, но имейте в виду, что наша цель состоит в том, чтобы в конечном итоге оптимизировать нейронные сети там, где мы не можем легко использовать ни один из инструментов, разработанных в литературе по выпуклой оптимизации.  &lt;/p&gt;
&lt;h3&gt;Стратегия #1: Первая очень плохая идея: Случайный поиск&lt;/h3&gt;
&lt;p&gt;Просто проверить, насколько хорош определенный набор параметров &lt;strong&gt;W&lt;/strong&gt;,что очень просто, первая (очень плохая) идея, которая может прийти в голову, — это просто попробовать множество различных случайных весов и отслеживать, что работает лучше всего. Эта процедура может выглядеть следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)&lt;/span&gt;
&lt;span class="c1"&gt;# assume Y_train are the labels (e.g. 1D array of 50,000)&lt;/span&gt;
&lt;span class="c1"&gt;# assume the function L evaluates the loss function&lt;/span&gt;

&lt;span class="n"&gt;bestloss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"inf"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Python assigns the highest possible float value&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3073&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.0001&lt;/span&gt; &lt;span class="c1"&gt;# generate random parameters&lt;/span&gt;
  &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# get the loss over the entire training set&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;bestloss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# keep track of the best solution&lt;/span&gt;
    &lt;span class="n"&gt;bestloss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
    &lt;span class="n"&gt;bestW&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;
  &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'in attempt &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; the loss was &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;, best &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bestloss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# prints:&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 0 the loss was 9.401632, best 9.401632&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 1 the loss was 8.959668, best 8.959668&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 2 the loss was 9.044034, best 8.959668&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 3 the loss was 9.278948, best 8.959668&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 4 the loss was 8.857370, best 8.857370&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 5 the loss was 8.943151, best 8.857370&lt;/span&gt;
&lt;span class="c1"&gt;# in attempt 6 the loss was 8.605604, best 8.605604&lt;/span&gt;
&lt;span class="c1"&gt;# ... (trunctated: continues for 1000 lines)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;В приведенном выше коде мы видим, что мы опробовали несколько случайных векторов &lt;strong&gt;весов W&lt;/strong&gt;, и некоторые из них работают лучше других. Мы можем взять лучшие веса &lt;strong&gt;W&lt;/strong&gt;, найденные этим поиском, и опробовать их на тестовом наборе:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# Assume X_test is [3073 x 10000], Y_test [10000 x 1]&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Wbest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xte_cols&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 10 x 10000, the class scores for all test examples&lt;/span&gt;
&lt;span class="c1"&gt;# find the index with max score in each column (the predicted class)&lt;/span&gt;
&lt;span class="n"&gt;Yte_predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# and calculate accuracy (fraction of predictions that are correct)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Yte_predict&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;Yte&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# returns 0.1555&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;При наилучшем &lt;strong&gt;W&lt;/strong&gt; это дает точность около &lt;strong&gt;15,5%&lt;/strong&gt;. Учитывая, что угадывание классов полностью случайным образом дает только 10%, это не очень плохой результат для такого примитивного решения на основе случайного поиска!  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Основная идея: итеративное уточнение&lt;/strong&gt;. Конечно, оказывается, что мы можем добиться гораздо большего. Основная идея заключается в том, что поиск наилучшего набора весов &lt;strong&gt;W&lt;/strong&gt; является очень сложной или даже невозможной задачей (особенно когда W содержит веса для целых сложных нейронных сетей), но задача уточнения конкретного набора весов &lt;strong&gt;W&lt;/strong&gt; для немного лучшего уровня значительно менее сложна. Другими словами, наш подход будет заключаться в том, чтобы начать со случайной &lt;strong&gt;W&lt;/strong&gt;, а затем итеративно уточнять ее, делая ее немного лучше с каждым разом.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Наша стратегия будет заключаться в том, чтобы начать со случайных весовых коэффициентов и итеративно уточнять их с течением времени, чтобы получить меньшие потери.   &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Аналогия с туристом с завязанными глазами&lt;/strong&gt;. Одна из аналогий, которую вы можете найти полезной в будущем - представить, что Вы идете по холмистой местности с повязкой на глазах и пытаетесь добраться до самой низины. В примере с CIFAR-10 холмы имеют размерность 30 730, так как &lt;strong&gt;размеры W&lt;/strong&gt; равны 10 x 3073. В каждой точке холма мы достигаем определенной потери (высоты над уровнем моря).  &lt;/p&gt;
&lt;h3&gt;Стратегия №2: Случайный локальный поиск&lt;/h3&gt;
&lt;p&gt;Первая стратегия, которая приходит на ум, — это попытаться вытянуть одну ногу в случайном направлении, а затем сделать шаг, только если он ведёт вниз по склону. Конкретно мы начнём со случайного &lt;strong&gt;W&lt;/strong&gt;, генерирующего случайные возмущения &lt;strong&gt;δW&lt;/strong&gt; к нему, и если потеря у возмущенного __W+δW__меньше, мы выполним обновление. Код для этой процедуры выглядит следующим образом:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3073&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="c1"&gt;# generate random starting W&lt;/span&gt;
&lt;span class="n"&gt;bestloss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"inf"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;step_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0001&lt;/span&gt;
  &lt;span class="n"&gt;Wtry&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3073&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;step_size&lt;/span&gt;
  &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr_cols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ytr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Wtry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;bestloss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Wtry&lt;/span&gt;
    &lt;span class="n"&gt;bestloss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
  &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'iter &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; loss is &lt;/span&gt;&lt;span class="si"&gt;%f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bestloss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;При использовании того же количества оценок функции потерь, что и раньше (1000), этот подход обеспечивает точность классификации тестового набора &lt;strong&gt;21,4%&lt;/strong&gt;. Это лучше, но всё равно неэффективно и требует больших вычислительных мощностей.  &lt;/p&gt;
&lt;h3&gt;Стратегия №3: Следование градиенту&lt;/h3&gt;
&lt;p&gt;В предыдущем разделе мы пытались найти направление в пространстве весов, которое улучшило бы наш вектор весов (и снизило бы потери). Оказывается, нет необходимости случайным образом искать хорошее направление: мы можем вычислить &lt;em&gt;лучшее&lt;/em&gt; направление, в котором нам следует изменить наш вектор весов, чтобы оно гарантированно было направлением наискорейшего спуска (по крайней мере, в пределе, когда размер шага стремится к нулю). Это направление будет связано с &lt;strong&gt;градиентом&lt;/strong&gt; функции потерь. В нашей аналогии с походом этот подход примерно соответствует тому, чтобы почувствовать наклон холма под ногами и идти в направлении, которое кажется наиболее крутым.  &lt;/p&gt;
&lt;p&gt;В одномерных функциях наклон — это мгновенная скорость изменения функции в любой интересующей вас точке. Градиент — это обобщение наклона для функций, которые принимают не одно число, а вектор чисел. Кроме того, градиент — это просто вектор наклонов (более известных как &lt;strong&gt;производные&lt;/strong&gt;) для каждого измерения во входном пространстве. Математическое выражение для производной одномерной функции по входным данным выглядит так:  &lt;/p&gt;
&lt;p&gt;$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$  &lt;/p&gt;
&lt;p&gt;Когда интересующие нас функции принимают вектор чисел вместо одного числа, мы называем производные &lt;strong&gt;частными производными&lt;/strong&gt;, а градиент — это просто вектор частных производных по каждому измерению. &lt;/p&gt;
&lt;h2&gt;Вычисление градиента&lt;/h2&gt;
&lt;p&gt;Существует два способа вычисления градиента: медленный, приблизительный, но простой (&lt;strong&gt;численный градиент&lt;/strong&gt;) и быстрый, точный, но более подверженный ошибкам способ, требующий математических вычислений (&lt;strong&gt;аналитический градиент&lt;/strong&gt;). Сейчас мы рассмотрим оба способа.  &lt;/p&gt;
&lt;h3&gt;Вычисление градиента численно с конечными разностями&lt;/h3&gt;
&lt;p&gt;Приведённая выше формула позволяет вычислить градиент численно. Вот универсальная функция, которая принимает градиент &lt;code&gt;f&lt;/code&gt; и вектор &lt;code&gt;x&lt;/code&gt; для вычисления функции и возвращает градиент &lt;code&gt;f&lt;/code&gt; в точке &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;eval_numerical_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;  a naive implementation of numerical gradient of f at x&lt;/span&gt;
&lt;span class="sd"&gt;  - f should be a function that takes a single argument&lt;/span&gt;
&lt;span class="sd"&gt;  - x is the point (numpy array) to evaluate the gradient at&lt;/span&gt;
&lt;span class="sd"&gt;  """&lt;/span&gt;

  &lt;span class="n"&gt;fx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# evaluate function value at original point&lt;/span&gt;
  &lt;span class="n"&gt;grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.00001&lt;/span&gt;

  &lt;span class="c1"&gt;# iterate over all indexes in x&lt;/span&gt;
  &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nditer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'multi_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;op_flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'readwrite'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finished&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="c1"&gt;# evaluate function at x+h&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multi_index&lt;/span&gt;
    &lt;span class="n"&gt;old_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;old_value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="c1"&gt;# increment by h&lt;/span&gt;
    &lt;span class="n"&gt;fxh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# evalute f(x + h)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;old_value&lt;/span&gt; &lt;span class="c1"&gt;# restore to previous value (very important!)&lt;/span&gt;

    &lt;span class="c1"&gt;# compute the partial derivative&lt;/span&gt;
    &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fxh&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;fx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="c1"&gt;# the slope&lt;/span&gt;
    &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iternext&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# step to next dimension&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;В соответствии с формулой градиента, которую мы привели выше, приведённый код перебирает все параметры один за другим, вносит небольшое изменение &lt;code&gt;h&lt;/code&gt; в этом параметре и вычисляет частную производную функции потерь по этому параметру, определяя, насколько изменилась функция. Переменная &lt;code&gt;grad&lt;/code&gt; в итоге содержит полный градиент.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Практические соображения&lt;/strong&gt;. Обратите внимание, что в математической формулировке градиент определяется в пределе, когда &lt;strong&gt;h&lt;/strong&gt; стремится к нулю, но на практике часто достаточно использовать очень маленькое значение (например, &lt;strong&gt;1e-5&lt;/strong&gt;, как показано в примере). В идеале нужно использовать наименьший размер шага, который не приводит к численным проблемам. Кроме того, на практике часто лучше вычислять численный градиент с помощью &lt;strong&gt;формулы центрированной разности: [f(x+h)−f(x−h)]/2h&lt;/strong&gt;. Смотрите &lt;a href="http://en.wikipedia.org/wiki/Numerical_differentiation"&gt;wiki&lt;/a&gt; для получения подробной информации.  &lt;/p&gt;
&lt;p&gt;Мы можем использовать приведённую выше функцию для вычисления градиента в любой точке и для любой функции. Давайте вычислим градиент функции потерь CIFAR-10 в некоторой случайной точке в пространстве весов:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; to use the generic code above we want a function that takes a single argument
&lt;span class="gh"&gt;#&lt;/span&gt; (the weights in our case) so we close over X_train and Y_train
def CIFAR10_loss_fun(W):
  return L(X_train, Y_train, W)

W = np.random.rand(10, 3073) * 0.001 # random weight vector
df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Градиент показывает наклон функции потерь по каждому измерению, и мы можем использовать его для обновления:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;loss_original = CIFAR10_loss_fun(W) # the original loss
print 'original loss: %f' % (loss_original, )

&lt;span class="gh"&gt;#&lt;/span&gt; lets see the effect of multiple step sizes
for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
  step_size = 10 ** step_size_log
  W_new = W - step_size * df # new position in the weight space
  loss_new = CIFAR10_loss_fun(W_new)
  print 'for step size %f new loss: %f' % (step_size, loss_new)

&lt;span class="gh"&gt;#&lt;/span&gt; prints:
&lt;span class="gh"&gt;#&lt;/span&gt; original loss: 2.200718
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-10 new loss: 2.200652
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-09 new loss: 2.200057
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-08 new loss: 2.194116
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-07 new loss: 2.135493
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-06 new loss: 1.647802
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-05 new loss: 2.844355
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-04 new loss: 25.558142
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-03 new loss: 254.086573
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-02 new loss: 2539.370888
&lt;span class="gh"&gt;#&lt;/span&gt; for step size 1.000000e-01 new loss: 25392.214036
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Обновление в направлении отрицательного градиента&lt;/strong&gt;. В приведенном выше коде обратите внимание, что для вычисления &lt;code&gt;W_new&lt;/code&gt; мы выполняем обновление в направлении отрицательного градиента &lt;code&gt;df&lt;/code&gt;, поскольку хотим, чтобы наша функция потерь уменьшалась, а не увеличивалась.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Влияние размера шага&lt;/strong&gt;. Градиент показывает нам направление, в котором функция возрастает наиболее быстро, но не говорит нам, насколько далеко в этом направлении мы должны продвинуться. Как мы увидим далее в курсе, выбор размера шага (также называемого &lt;em&gt;скоростью обучения&lt;/em&gt;) станет одним из самых важных (и самых сложных) параметров при обучении нейронной сети. В нашей аналогии со спуском с холма вслепую мы чувствуем, что склон под нашими ногами наклонён в каком-то направлении, но длина шага, который мы должны сделать, неизвестна. Если мы будем осторожно переставлять ноги, то сможем рассчитывать на последовательный, но очень медленный прогресс (это соответствует небольшому размеру шага). И наоборот, мы можем сделать большой уверенный шаг, чтобы спуститься быстрее, но это может не окупиться. Как вы можете видеть в примере кода выше, в какой-то момент более длинный шаг приведёт к большим потерям, так как мы «перешагнём».  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/stepsize.jpg"&gt;&lt;br&gt;
Визуализация влияния размера шага. Мы начинаем с какой-то конкретной точки &lt;strong&gt;W&lt;/strong&gt; и вычисляем градиент (или, скорее, его отрицательную величину — белую стрелку), который указывает направление наиболее резкого снижения функции потерь. Маленькие шаги, скорее всего, приведут к стабильному, но медленному прогрессу. Большие шаги могут привести к более быстрому прогрессу, но они более рискованны. Обратите внимание, что в конечном итоге при большом размере шага мы совершим ошибку и увеличим потери. Размер шага (или, как мы позже назовём его, &lt;strong&gt;скорость обучения&lt;/strong&gt;) станет одним из важнейших гиперпараметров, которые нам придётся тщательно настраивать.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Проблема эффективности&lt;/strong&gt;. Возможно, вы заметили, что вычисление численного градиента имеет сложность, линейную по отношению к количеству параметров. В нашем примере у нас было 30 730 параметров, и поэтому для вычисления градиента и обновления только одного параметра нам пришлось выполнить 30 731 вычисление функции потерь. Эта проблема усугубляется тем, что современные нейронные сети могут легко содержать десятки миллионов параметров. Очевидно, что эта стратегия не масштабируется, и нам нужно что-то получше.  &lt;/p&gt;
&lt;h3&gt;Вычисление градиента аналитически с помощью математического анализа&lt;/h3&gt;
&lt;p&gt;Численный градиент очень просто вычислить с помощью конечно-разностного приближения, но его недостатком является то, что он является приблизительным (поскольку нам нужно выбрать небольшое значение &lt;em&gt;h&lt;/em&gt;, в то время как истинный градиент определяется как предел, когда &lt;em&gt;h&lt;/em&gt; стремится к нулю), а также то, что его вычисление требует больших вычислительных мощностей. Второй способ вычисления градиента — аналитический, с использованием математического анализа, который позволяет вывести прямую формулу для градиента (без приближений), которая также очень быстро вычисляется. Однако, в отличие от численного градиента, его реализация может быть более подвержена ошибкам, поэтому на практике очень часто вычисляют аналитический градиент и сравнивают его с численным градиентом, чтобы проверить правильность реализации. Это называется &lt;strong&gt;проверкой градиента&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;Давайте рассмотрим пример функции потерь SVM для одной точки данных:  &lt;/p&gt;
&lt;p&gt;$$
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]
$$  &lt;/p&gt;
&lt;p&gt;Мы можем дифференцировать функцию по весовым коэффициентам. Например, взяв градиент по &lt;strong&gt;\(w_{y_i}\)&lt;/strong&gt; мы получаем:  &lt;/p&gt;
&lt;p&gt;$$
\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &amp;gt; 0) \right) x_i
$$  &lt;/p&gt;
&lt;p&gt;где &lt;strong&gt;\(\mathbb{1}\)&lt;/strong&gt;- это индикаторная функция, которая принимает значение 1, если условие внутри истинно, и 0 в противном случае. Хотя это выражение может показаться пугающим, когда вы записываете его, при реализации в коде вы просто подсчитываете количество классов, которые не соответствуют желаемой погрешности (и, следовательно, влияют на функцию потерь), а затем вектор данных &lt;strong&gt;\(x_i\)&lt;/strong&gt;, умноженное на это число — это градиент. Обратите внимание, что это градиент только по отношению к строке &lt;strong&gt;W&lt;/strong&gt;, а это соответствует правильному классу. Для других строк, где &lt;strong&gt;j≠\(y_i\)&lt;/strong&gt; градиент равен:  &lt;/p&gt;
&lt;p&gt;$$
\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &amp;gt; 0) x_i
$$  &lt;/p&gt;
&lt;p&gt;Как только вы получите выражение для градиента, будет несложно реализовать эти выражения и использовать их для обновления градиента.  &lt;/p&gt;
&lt;h2&gt;Градиентный спуск&lt;/h2&gt;
&lt;p&gt;Теперь, когда мы можем вычислить градиент функции потерь, процедура многократного вычисления градиента, а затем обновления параметров, называется градиентным спуском. Его простая версия выглядит следующим образом: &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; Vanilla Gradient Descent

while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Этот простой цикл лежит в основе всех библиотек нейронных сетей. Существуют и другие способы оптимизации (например, LBFGS), но градиентный спуск в настоящее время является наиболее распространённым и устоявшимся способом оптимизации функций потерь нейронных сетей. В ходе курса мы рассмотрим некоторые детали этого цикла (например, точное уравнение обновления), но основная идея следования за градиентом до тех пор, пока нас не удовлетворят результаты, останется прежней.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Мини-пакетный градиентный спуск&lt;/strong&gt;. В крупномасштабных приложениях (таких как ILSVRC) обучающие данные могут насчитывать миллионы примеров. Следовательно, вычисление полной функции потерь по всему обучающему набору данных для выполнения только одного обновления параметров кажется нецелесообразным. Очень распространённым подходом к решению этой проблемы является вычисление градиента по &lt;strong&gt;пакетам&lt;/strong&gt; обучающих данных. Например, в современных сверточных нейронных сетях типичная партия содержит 256 примеров из всего обучающего набора, состоящего 1,2 миллиона примеров. Затем эта партия используется для обновления параметров:  &lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="gh"&gt;#&lt;/span&gt; Vanilla Minibatch Gradient Descent

while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Причина, по которой это работает, заключается в том, что примеры в обучающих данных взаимосвязаны. Чтобы понять это, рассмотрим крайний случай, когда все 1,2 миллиона изображений в ILSVRC на самом деле являются точными дубликатами всего 1000 уникальных изображений (по одному для каждого класса, или, другими словами, 1200 идентичных копий каждого изображения). Тогда очевидно, что градиенты, которые мы вычислили бы для всех 1200 идентичных копий, были бы одинаковыми, и если бы мы усреднили потерю данных по всем 1,2 миллионам изображений, то получили бы точно такую же потерю, как если бы мы оценивали только небольшое подмножество из 1000 изображений. На практике, конечно, набор данных не содержит дубликатов изображений, и градиент от мини-пакета является хорошим приближением к градиенту полной задачи. Таким образом, на практике можно добиться гораздо более быстрой сходимости, оценивая градиенты мини-пакетов для более частого обновления параметров.&lt;/p&gt;
&lt;p&gt;Крайним случаем этого является ситуация, когда мини-пакет содержит только один пример. Этот процесс называется &lt;strong&gt;стохастическим градиентным спуском (SGD)&lt;/strong&gt; (или иногда &lt;strong&gt;онлайн&lt;/strong&gt;-градиентным спуском). Это относительно редкое явление, потому что на практике из-за оптимизации кода с помощью векторизации гораздо эффективнее вычислять градиент для 100 примеров, чем градиент для одного примера 100 раз. Несмотря на то, что SGD технически подразумевает использование одного примера для оценки градиента, вы услышите, как люди используют термин SGD даже при упоминании градиентного спуска с мини-пакетами (т. е. редко можно встретить упоминания MGD для «градиентного спуска с мини-пакетами» или BGD для «пакетного градиентного спуска»), где обычно предполагается использование мини-пакетов. Размер мини-пакета является гиперпараметром, но его нечасто проверяют на перекрёстной проверке. Обычно это зависит от ограничений памяти (если они есть) или устанавливается на какое-то значение, например 32, 64 или 128. На практике мы используем степени двойки, потому что многие реализации векторизованных операций работают быстрее, если размер входных данных равен степени двойки.  &lt;/p&gt;
&lt;h2&gt;Краткая сводка&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt="" src="https://cs231n.github.io/assets/dataflow.jpeg"&gt;&lt;br&gt;
Краткое описание информационного потока. Набор данных, состоящий из пар &lt;strong&gt;(x,y)&lt;/strong&gt;, задан и неизменен. Веса изначально являются случайными числами и могут меняться. Во время прямого прохода функция оценки вычисляет оценки классов, которые сохраняются в векторе &lt;strong&gt;f&lt;/strong&gt;. Функция потерь содержит два компонента: Функция потерь данных вычисляет соответствие между оценками &lt;strong&gt;f&lt;/strong&gt; и метками &lt;strong&gt;y&lt;/strong&gt;. Функция потерь регуляризации зависит только от весов. Во время градиентного спуска мы вычисляем градиент по весовым коэффициентам (и, при желании, по данным) и используем его для обновления параметров во время градиентного спуска.  &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;В этом разделе: 
- Мы представили функцию потерь как &lt;strong&gt;многомерный ландшафт оптимизации&lt;/strong&gt;, в котором мы пытаемся достичь дна. Рабочая аналогия, которую мы разработали, — это турист с завязанными глазами, который хочет добраться до дна. В частности, мы увидели, что функция стоимости SVM является кусочно-линейной и имеет форму чаши.
- Мы обосновали идею оптимизации функции потерь с помощью &lt;strong&gt;итеративного уточнения&lt;/strong&gt;, при котором мы начинаем со случайного набора весовых коэффициентов и шаг за шагом уточняем их, пока потери не будут минимизированы.
- Мы увидели, что &lt;strong&gt;градиент&lt;/strong&gt; функции указывает направление наискорейшего подъёма, и обсудили простой, но неэффективный способ его численного вычисления с помощью конечно-разностной аппроксимации (конечно-разностная аппроксимация — это значение &lt;em&gt;h&lt;/em&gt;, используемое при вычислении численного градиента).
- Мы увидели, что для обновления параметров требуется сложная настройка &lt;strong&gt;размера шага&lt;/strong&gt; (или &lt;strong&gt;скорости обучения&lt;/strong&gt;), который должен быть установлен правильно: если он слишком мал, прогресс будет стабильным, но медленным. Если он слишком велик, прогресс может быть быстрее, но более рискованным. Мы рассмотрим этот компромисс более подробно в следующих разделах.
- Мы обсудили компромиссы между вычислением &lt;strong&gt;численного&lt;/strong&gt; и &lt;strong&gt;аналитического&lt;/strong&gt; градиента. Численный градиент прост, но он приблизителен и требует больших вычислительных затрат. Аналитический градиент точен, быстро вычисляется, но более подвержен ошибкам, поскольку требует вычисления градиента с помощью математики. Поэтому на практике мы всегда используем аналитический градиент, а затем выполняем &lt;strong&gt;проверку градиента&lt;/strong&gt;, в ходе которой его реализация сравнивается с численным градиентом.
- Мы представили алгоритм &lt;strong&gt;градиентного спуска&lt;/strong&gt;, который итеративно вычисляет градиент и выполняет обновление параметров в цикле.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Далее&lt;/strong&gt;: основной вывод из этого раздела заключается в том, что способность вычислять градиент функции потерь по отношению к её весовым коэффициентам (и иметь некоторое интуитивное представление об этом) — самый важный навык, необходимый для проектирования, обучения и понимания нейронных сетей. В следующем разделе мы научимся вычислять градиент аналитически с помощью правила дифференцирования сложной функции, также известного как &lt;strong&gt;обратное распространение ошибки&lt;/strong&gt;. Это позволит нам эффективно оптимизировать относительно произвольные функции потерь, которые используются во всех видах нейронных сетей, включая свёрточные нейронные сети.&lt;/p&gt;</description><guid>https://mldl.ru/posts/optimization/</guid><pubDate>Thu, 06 Mar 2025 16:42:16 GMT</pubDate></item></channel></rss>